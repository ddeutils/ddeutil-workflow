{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Workflow","text":"<p>The Lightweight Workflow Orchestration with fewer dependencies the was created for easy to make a simple metadata driven data workflow. It can use for data operator by a <code>.yaml</code> template.</p> <p>Warning</p> <p>This package provide only orchestration workload task. That mean you should not use the workflow stage to process any large volume data which use lot of compute resource. </p> <p> Rules of This Workflow engine:</p> <ol> <li>The Minimum frequency unit of built-in scheduling is 1 Minute \ud83d\udd58</li> <li>Can not re-run only failed stage and its pending downstream \u21a9\ufe0f</li> <li>All parallel tasks inside workflow core engine use Multi-Threading pool    (Python 3.13 unlock GIL \ud83d\udc0d\ud83d\udd13)</li> <li>Recommend to pass a Secret Value with environment variable in YAML template \ud83d\udd10</li> <li>Any datatime value convert to UTC Timezone \ud83c\udf10</li> </ol> <p> Workflow Diagrams:</p> <p>This diagram show where is this application run on the production infrastructure. You will see that this application do only running code with stress-less which mean you should to set the data layer separate this core program before run this application.</p> <pre><code>flowchart LR\n    A((fa:fa-user User))\n\n    subgraph Docker Container\n        direction TB\n        G@{ shape: rounded, label: \"\ud83d\udce1Observe&lt;br&gt;Application\" }\n    end\n\n    subgraph Docker Container\n        direction TB\n        B@{ shape: rounded, label: \"\ud83c\udfc3Workflow&lt;br&gt;Application\" }\n    end\n\n    A &lt;--&gt;|action &amp;&lt;br&gt;response| B\n    B -...-&gt; |response| G\n    G -...-&gt; |request| B\n\n    subgraph Data Context\n        D@{ shape: processes, label: \"Logs\" }\n        E@{ shape: lin-cyl, label: \"Audit&lt;br&gt;Logs\" }\n    end\n\n    subgraph Config Context\n        F@{ shape: tag-rect, label: \"YAML&lt;br&gt;files\" }\n    end\n\n    A ---&gt; |push| H(Repo)\n    H -.-&gt; |pull| F\n\n    B &lt;--&gt;|disable &amp;&lt;br&gt;read| F\n\n    B &lt;--&gt;|read &amp;&lt;br&gt;write| E\n\n    B --&gt;|write| D\n\n    D -.-&gt;|read| G\n    E -.-&gt;|read| G</code></pre> <p>Note</p> <p>Disclaimer: I inspire the dynamic YAML statement from the GitHub Action, and my experience of data framework configs pattern. </p> <p>Other workflow orchestration services that I interest and pick them to be this project inspiration:</p> <ul> <li>Google Workflows</li> <li>AWS Step Functions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>This project need <code>ddeutil</code> and <code>ddeutil-io</code> extension namespace packages to be the base deps. If you want to install this package with application add-ons, you should add <code>app</code> in installation;</p> Use-case Install Optional Support Python <code>ddeutil-workflow</code> \u2705 FastAPI Server <code>ddeutil-workflow[all]</code> \u2705"},{"location":"#usage","title":"\ud83c\udfaf Usage","text":"<p>This is examples that use workflow file for running common Data Engineering use-case.</p> <p>Important</p> <p>I recommend you to use the <code>call</code> stage for all actions that you want to do with workflow activity that you want to orchestrate. Because it able to dynamic an input argument with the same call function that make you use less time to maintenance your data workflows.</p> <pre><code>run-py-local:\n\n   # Validate model that use to parsing exists for template file\n   type: Workflow\n   on:\n      # If workflow deploy to schedule, it will run every 5 minutes\n      # with Asia/Bangkok timezone.\n      - cronjob: '*/5 * * * *'\n        timezone: \"Asia/Bangkok\"\n   params:\n      # Incoming execution parameters will validate with this type. It allows\n      # to set default value or templating.\n      source-extract: str\n      run-date: datetime\n   jobs:\n      getting-api-data:\n         runs-on:\n            type: local\n         stages:\n            - name: \"Retrieve API Data\"\n              id: retrieve-api\n              uses: tasks/get-api-with-oauth-to-s3@requests\n              with:\n                 # Arguments of source data that want to retrieve.\n                 method: post\n                 url: https://finances/open-data/currency-pairs/\n                 body:\n                    resource: ${{ params.source-extract }}\n\n                    # You can use filtering like Jinja template but this\n                    # package does not use it.\n                    filter: ${{ params.run-date | fmt(fmt='%Y%m%d') }}\n                 auth:\n                    type: bearer\n                    keys: ${API_ACCESS_REFRESH_TOKEN}\n\n                 # Arguments of target data that want to land.\n                 writing_mode: flatten\n                 aws:\n                    path: my-data/open-data/${{ params.source-extract }}\n\n                    # This Authentication code should implement with your custom call\n                    # function. The template allow you to use environment variable.\n                    access_client_id: ${AWS_ACCESS_CLIENT_ID}\n                    access_client_secret: ${AWS_ACCESS_CLIENT_SECRET}\n</code></pre> <p>Before execute this workflow, you should implement caller function first.</p> <pre><code>registry-caller/\n  \u2570\u2500 tasks.py\n</code></pre> <p>This function will store as module that will import from <code>WORKFLOW_CORE_REGISTRY_CALLER</code> value (This config can override by extra parameters with <code>registry_caller</code> key).</p> <pre><code>from ddeutil.workflow import Result, tag\nfrom ddeutil.workflow.errors import StageError\nfrom pydantic import BaseModel, SecretStr\n\n\nclass AwsCredential(BaseModel):\n    path: str\n    access_client_id: str\n    access_client_secret: SecretStr\n\n\nclass RestAuth(BaseModel):\n    type: str\n    keys: SecretStr\n\n\n@tag(\"requests\", alias=\"get-api-with-oauth-to-s3\")\ndef get_api_with_oauth_to_s3(\n     method: str,\n     url: str,\n     body: dict[str, str],\n     auth: RestAuth,\n     writing_node: str,\n     aws: AwsCredential,\n     result: Result,\n) -&gt; dict[str, int]:\n    \"\"\"Get the data from RestAPI via Authenticate with OAuth and then store to\n    AWS S3 service.\n    \"\"\"\n    result.trace.info(\"[CALLER]: Start get data via RestAPI to S3.\")\n    result.trace.info(f\"... {method}: {url}\")\n    if method != \"post\":\n        raise StageError(f\"RestAPI does not support for {method} action.\")\n    # NOTE: If you want to use secret, you can use `auth.keys.get_secret_value()`.\n    return {\"records\": 1000}\n</code></pre> <p>The above workflow template is main executor pipeline that you want to do. If you want to schedule this workflow, you want to dynamic its parameters change base on execution time such as <code>run-date</code> should change base on that workflow running date.</p> <pre><code>from ddeutil.workflow import Workflow, Result\n\nworkflow: Workflow = Workflow.from_conf('run-py-local')\nresult: Result = workflow.execute(\n   params={\"source-extract\": \"USD-THB\", \"run-date\": \"2024-01-01\"}\n)\n</code></pre>"},{"location":"build/","title":"Build","text":"<p>Note for building the Docker image.</p>"},{"location":"build/#docker-image","title":"Docker Image","text":"<pre><code>docker build -t ddeutil-workflow:latest -f ./.container/Dockerfile .\n</code></pre>"},{"location":"build/#docker-container","title":"Docker Container","text":"<pre><code>docker run --name ddeutil-worker -t --rm ddeutil-workflow:latest version\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>Important</p> <p>The config value that you will set on the environment should combine with prefix, component, and name which is <code>WORKFLOW_{component}_{name}</code> with upper case.</p>"},{"location":"configuration/#environment-variable","title":"Environment Variable","text":""},{"location":"configuration/#core","title":"Core","text":"<p>The main configuration that use to dynamic changing with your objective of this application. If any configuration values do not set yet, it will use default value and do not raise any error to you.</p> Name Component Default Description REGISTRY_CALLER CORE <code>.</code> List of importable string for the call stage. REGISTRY_FILTER CORE <code>ddeutil.workflow.reusables</code> List of importable string for the filter template. CONF_PATH CORE <code>./conf</code> The config path that keep all template <code>.yaml</code> files. STAGE_DEFAULT_ID CORE <code>false</code> A flag that enable default stage ID that use for catch an execution output. GENERATE_ID_SIMPLE_MODE CORE <code>true</code> A flog that enable generating ID with <code>md5</code> algorithm. DEBUG_MODE LOG <code>true</code> A flag that enable logging with debug level mode. TIMEZONE LOG <code>Asia/Bangkok</code> A Timezone string value that will pass to <code>ZoneInfo</code> object. TRACE_HANDLERS LOG <code>[{\"type\": \"console\"}]</code> A Json string of list of trace handler config data that use to emit log message. AUDIT_CONF LOG <code>{\"type\": \"file\", \"path\": \"./audits\"}</code> A Json string of audit config data that use to write audit metrix. AUDIT_ENABLE_WRITE LOG <code>true</code> A flag that enable writing audit log after end execution in the workflow release step. ## Execution Override <p>Some config can override by an extra parameters. For the below example, I override the <code>conf_path</code> and <code>stage_default_id</code> config values at the execution time from <code>WORKFLOW_CORE_CONF_PATH</code> and <code>WORKFLOW_CORE_STAGE_DEFAULT_ID</code> environment variables.</p> <p>That is mean, it does not impact to running workflow that do not override and use the current environment config values.</p> <pre><code>from pathlib import Path\nfrom ddeutil.workflow import Workflow\n\nworkflow = Workflow.from_conf(\n    \"wf-tester\",\n    extras={\n        \"conf_path\": Path(\"./new-path/conf\"),\n        \"stage_default_id\": True,\n    }\n)\n</code></pre>"},{"location":"context/","title":"Context","text":"<p>This content will explain a context data that passing in and out to execution process for each workflow model objects such as Workflow, Job, and Stage.</p> <pre><code>Input       --&gt; {params: {}}\n|\nWorkflow    --&gt; {params: {}, jobs: {&lt;job-id&gt;: {}}}\n|\nJob         --&gt; {\n|                   status: SUCCESS,\n|                   params: {},\n|                   jobs: {\n|                       &lt;job-id&gt;: {\n|                           status: SUCCESS,\n|                           strategies: {\n|                               &lt;strategy-id&gt;: {metrix: {}, stages: {}}\n|                           },\n|                           errors: {\n|                               &lt;strategy-id&gt;: {}\n|                           }\n|                       }\n|                   },\n|                   metrix: {},\n|                   stages: {}\n|           --&gt; {\n|                   status: SUCCESS,\n|                   params: {},\n|                   jobs: {\n|                       &lt;job-id&gt;: {stages: {}, errors: {}, status: SUCCESS}\n|                   },\n|                   metrix: {},\n|                   stages: {}\n|               }\n|\nStage       --&gt; {\n                    params: {},\n                    jobs: {},\n                    metrix: {},\n                    stages: {&lt;stage-id&gt;: {outputs: {}, errors: {}, status: SUCCESS}}\n                }\n</code></pre>"},{"location":"context/#stage","title":"Stage","text":"<p>A stage context is the minimum standard context for this package. I will explain context execution that return from <code>execute</code> and <code>process</code> methods can be any custom output with its stage.</p>"},{"location":"context/#input-template-params","title":"Input Template Params","text":"<p>The template parameter that want to use on stage will can be</p> <ul> <li><code>${{ stages.&lt;stage-id&gt;.outputs.&lt;result&gt; }}</code></li> <li><code>${{ stages.&lt;stage-id&gt;.errors?.name }}</code></li> </ul> <p>Job reference if it has any job running finish before.</p> <ul> <li><code>${{ jobs.&lt;job-id&gt;.stages.&lt;stage-id&gt;.outputs.&lt;result&gt; }}</code></li> <li><code>${{ jobs.&lt;job-id&gt;.strategies.&lt;strategy-id&gt;.stages.&lt;stage-id&gt;.errors?.name }}</code></li> </ul>"},{"location":"context/#execution-output","title":"Execution Output","text":"<p>The result from execution method should be.</p> SUCCESSFAILED/CANCEL <pre><code>{\n  \"result\": 100\n}\n</code></pre> <pre><code>{\n  \"errors\": {\n    \"name\": \"error-class-name\",\n    \"message\": \"error-message\"\n  }\n}\n</code></pre> <p>For nested stage, it can return skipped output with <code>SUCCESS</code> status, but it will keep in nested-stage ID instead parent output.</p> <pre><code>{\n    \"stages\": {\n        \"&lt;stage-ID&gt;\": {\n            \"status\": SKIP,\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>This step can raise any error by custom excution. It does not raise only <code>StageExcepton</code>.</p>"},{"location":"context/#set-output","title":"Set Output","text":"<p>A context that return from <code>set_outputs</code> method.</p> <p>if a <code>to</code> argument that pass to this method be;</p> <pre><code>{\n  \"params\": {\"key\":  \"value\"}\n}\n</code></pre> <p>it will return result be;</p> <pre><code>{\n  \"params\": {\"key\": \"value\"},\n  \"stages\": {\n    \"&lt;stage-id&gt;\": {\n      \"status\": \"0\",\n      \"outputs\": {\"result\": \"100\"},\n      \"errors\": {\n        \"name\": \"class-name\",\n        \"message\": \"error-message\"\n      }\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>The main key from stage setting output method are <code>outputs</code> and <code>errors</code>.</p>"},{"location":"context/#workflow","title":"Workflow","text":"<p>A workflow execution context that return from the <code>execute</code> method.</p>"},{"location":"context/#execution","title":"Execution","text":"<p>For the fist context values that passing to the workflow execution method:</p> <pre><code>{\n  \"params\": {\"key\": \"value\"},\n  \"jobs\": {\n    \"&lt;job-name&gt;\": {},\n    \"&lt;job-name-02&gt;\": {}\n  },\n  \"errors\": {\n    \"name\": \"\",\n    \"message\": \"\"\n  }\n}\n</code></pre> <p>The <code>params</code> is the values from the parameterize method that already validated typing.</p>"},{"location":"context/#job","title":"Job","text":"<p>A job execution context that return from the <code>execute</code> method.</p>"},{"location":"context/#execution_1","title":"Execution","text":"<pre><code>{\n  \"params\": {},\n  \"jobs\": {\n    \"&lt;job-name&gt;\": {\n      \"strategies\": {\n        \"&lt;strategy-id&gt;\": {\n          \"matrix\": {},\n          \"stages\": {\n            \"&lt;stage-id&gt;\": {}\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>If the job does not set strategy matrix;</p> <pre><code>{\n  \"params\": {},\n  \"jobs\": {\n    \"&lt;job-name&gt;\": {\n      \"stages\": {\n        \"&lt;stage-id&gt;\": {}\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This quick start use-case is getting data from external API and then prepare these data and finally, aggregate them into the silver data zone by daily basis.</p>"},{"location":"getting-started/#prerequisite","title":"Prerequisite","text":"<p>I will use only the core package of the workflow and run with manual action.</p> <pre><code>$ pip install uv\n$ uv pip install -U ddeutil-workflow\n</code></pre> <p>Project structure:</p> <pre><code>project/\n \u251c\u2500 conf/\n \u2502   \u2570\u2500 manual-workflow.yml\n \u251c\u2500 logs/\n \u251c\u2500 src/\n \u2502   \u2570\u2500 tasks/\n \u2502       \u251c\u2500 __init__.py\n \u2502       \u2570\u2500 https_call.py\n \u251c\u2500 main.py\n \u2570\u2500 .env\n</code></pre>"},{"location":"getting-started/#getting-started_1","title":"Getting Started","text":"<p>Create initial config path at <code>.env</code>:</p> <pre><code>WORKFLOW_CORE_REGISTRY_CALLER=src\nWORKFLOW_LOG_TIMEZONE=Asia/Bangkok\nWORKFLOW_LOG_AUDIT_ENABLE_WRITE=true\nWORKFLOW_LOG_TRACE_ENABLE_WRITE=true\n</code></pre> <p>Create the first pipeline template at <code>./conf/manual-workflow.yml</code>:</p> ./conf/manual-workflow.yml<pre><code>wf-run-manual:\n  type: Workflow\n  params:\n    run_date: datetime\n  jobs:\n    stage-to-curated:\n      stages:\n        - name: \"Extract data from external API\"\n          uses: tasks/https-external@httpx\n          with:\n            url: \"https://some-endpoint/api/v1/extract\"\n            auth: \"http_conn_id\"\n            incremental: {{ params.run_date }}\n</code></pre> <p>Create the called function that use on your stage.</p> <ol> <li>Create <code>__init__.py</code> file inside <code>./src/tasks</code> folder for import your called    function from your module.</li> </ol> ./src/__init__.py<pre><code>from .https_call import *\n</code></pre> <ol> <li>Create <code>https_call.py</code> file inside <code>./src/tasks</code> folder to be task module.</li> </ol> ./src/https_call.py<pre><code>from ddeutil.workflow import Result, tag\n\n\n@tag(\"httpx\", alias=\"https-external\")\ndef demo_http_to_external_task(url: str, auth: str, result: Result) -&gt; dict[str, int]:\n    result.trace.info(f\"Start POST: {url} with auth: {auth}\")\n    return {\"counts\": 0}\n</code></pre> <p>The name of called function can be free text with snake case. The stage will    use <code>alias</code> name to search this function instead its function name.</p>"},{"location":"getting-started/#run-workflow","title":"Run Workflow","text":"<p>The workflow allow you to run with 2 modes, <code>execute</code> and <code>release</code> modes.</p>"},{"location":"getting-started/#execute","title":"Execute","text":"<p>At the <code>main.py</code> file:</p> ./main.py<pre><code>from ddeutil.workflow import Workflow, Result\n\n\ndef call_execute():\n    result: Result = (\n        Workflow\n        .from_conf('wf-run-manual')\n        .execute(params={\"run_date\": \"2024-08-01\"})\n    )\n    print(result)\n\n\nif __name__ == '__main__':\n    call_execute()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"getting-started/#release","title":"Release","text":"<p>At the <code>main.py</code> file:</p> ./main.py<pre><code>from datetime import datetime\nfrom ddeutil.workflow import Workflow, Result, config\n\n\ndef call_release():\n    result: Result = (\n        Workflow\n        .from_conf('wf-run-manual')\n        .release(\n            datetime.now(tz=config.tz),\n            params={\"run_date\": \"2024-08-01\"},\n        )\n    )\n    print(result)\n\n\nif __name__ == '__main__':\n    call_release()\n</code></pre> <pre><code>\n</code></pre> <p>The audit file that keep from this release execution:</p> <pre><code>project/\n \u2570\u2500 audits/\n     \u2570\u2500 workflow=wf-run-manual/\n         \u2570\u2500 release=20240101001011/\n             \u2570\u2500 820626787820250106163236493894.log\n</code></pre>"},{"location":"getting-started/#conclusion","title":"Conclusion","text":"<p>You can run any workflow that you want by config a YAML file. If it raises any error from your caller stage, you can observe the error log in log path with the release running ID from <code>run_id</code> attribute of the return Result object.</p> <pre><code>project/\n \u2570\u2500 logs/\n     \u2570\u2500 run_id=820626787820250106163236493894/\n         \u251c\u2500 metadata.json\n         \u251c\u2500 stderr.txt\n         \u2570\u2500 stdout.txt\n</code></pre>"},{"location":"logs/","title":"Logs","text":"<p>For logging of this package will split to 2 parts:</p> <ol> <li>Trace log</li> <li>Audit log</li> </ol>"},{"location":"logs/#trace-log","title":"Trace Log","text":"<p>This part will show all process logs that return with the trace model.</p>"},{"location":"logs/#audit-log","title":"Audit Log","text":"<p>This part will use to tracking workflow release log.</p>"},{"location":"api/audits/","title":"Audits","text":"<p>The Audits module provides comprehensive audit capabilities for workflow execution tracking and monitoring. It supports multiple audit backends for capturing execution metadata, status information, and detailed logging.</p>"},{"location":"api/audits/#overview","title":"Overview","text":"<p>The audit system provides:</p> <ul> <li>Execution tracking: Complete workflow execution metadata</li> <li>Release logging: Detailed workflow release information</li> <li>Parameter capture: Input parameters and context data</li> <li>Result persistence: Execution outcomes and status tracking</li> <li>Query capabilities: Search and retrieve audit logs</li> <li>Multiple backends: File-based JSON storage and SQLite database storage</li> </ul> <p>Single Backend Limitation</p> <p>You can set only one audit backend setting for the current run-time because it will conflict audit data if more than one audit backend pointer is set.</p>"},{"location":"api/audits/#core-components","title":"Core Components","text":""},{"location":"api/audits/#baseaudit","title":"<code>BaseAudit</code>","text":"<p>Abstract base class for all audit implementations providing core audit functionality.</p> <p>Key Features</p> <ul> <li>Workflow execution metadata capture</li> <li>Release timestamp tracking</li> <li>Context and parameter logging</li> <li>Status and result persistence</li> <li>Automatic configuration validation</li> </ul>"},{"location":"api/audits/#auditdata","title":"<code>AuditData</code>","text":"<p>Data model for audit information containing workflow execution details.</p> <p>AuditData Structure</p> <pre><code>from ddeutil.workflow.audits import AuditData\nfrom datetime import datetime\n\naudit_data = AuditData(\n    name=\"data-pipeline\",\n    release=datetime(2024, 1, 15, 10, 30),\n    type=\"scheduled\",\n    context={\n        \"params\": {\"source_table\": \"users\", \"target_env\": \"prod\"},\n        \"jobs\": {\"extract\": {\"status\": \"SUCCESS\"}}\n    },\n    run_id=\"workflow-123\",\n    parent_run_id=None,\n    runs_metadata={\"execution_time\": \"300s\", \"memory_usage\": \"512MB\"}\n)\n</code></pre>"},{"location":"api/audits/#file-based-audits","title":"File-based Audits","text":""},{"location":"api/audits/#fileaudit","title":"<code>FileAudit</code>","text":"<p>File-based audit implementation that persists audit logs to the local filesystem in JSON format.</p> <p>File Audit Usage</p> <pre><code>from ddeutil.workflow.audits import FileAudit, AuditData\nfrom datetime import datetime\n\n# Create file audit instance\naudit = FileAudit(\n    type=\"file\",\n    path=\"./audits\",\n    extras={\"enable_write_audit\": True}\n)\n\n# Create audit data\ndata = AuditData(\n    name=\"data-pipeline\",\n    release=datetime(2024, 1, 15, 10, 30),\n    type=\"scheduled\",\n    context={\n        \"params\": {\"source_table\": \"users\", \"target_env\": \"prod\"},\n        \"jobs\": {\"extract\": {\"status\": \"SUCCESS\"}}\n    },\n    run_id=\"workflow-123\",\n    parent_run_id=None,\n    runs_metadata={\"execution_time\": \"300s\"}\n)\n\n# Save audit log\naudit.save(data)\n\n# Log is saved to:\n# ./audits/workflow=data-pipeline/release=20240115103000/workflow-123.log\n</code></pre>"},{"location":"api/audits/#audit-file-structure","title":"Audit File Structure","text":"<pre><code>audits/\n\u251c\u2500\u2500 workflow=data-pipeline/\n\u2502   \u251c\u2500\u2500 release=20240115103000/\n\u2502   \u2502   \u251c\u2500\u2500 workflow-123.log\n\u2502   \u2502   \u2514\u2500\u2500 workflow-124.log\n\u2502   \u2514\u2500\u2500 release=20240116080000/\n\u2502       \u2514\u2500\u2500 workflow-125.log\n\u2514\u2500\u2500 workflow=etl-process/\n    \u2514\u2500\u2500 release=20240115120000/\n        \u2514\u2500\u2500 workflow-126.log\n</code></pre>"},{"location":"api/audits/#finding-audits","title":"Finding Audits","text":"<p>The <code>FileAudit</code> class provides utilities to search and retrieve audit logs.</p> <p>Audit Discovery</p> <pre><code>from ddeutil.workflow.audits import FileAudit\n\naudit = FileAudit(type=\"file\", path=\"./audits\")\n\n# Find all audits for a workflow\nfor audit_data in audit.find_audits(\"data-pipeline\"):\n    print(f\"Release: {audit_data.release}\")\n    print(f\"Run ID: {audit_data.run_id}\")\n    print(f\"Type: {audit_data.type}\")\n    print(f\"Context: {audit_data.context}\")\n\n# Find specific audit by release\naudit_data = audit.find_audit_with_release(\n    name=\"data-pipeline\",\n    release=datetime(2024, 1, 15, 10, 30)\n)\n\n# Check if audit exists for specific release\nexists = audit.is_pointed(\n    data=audit_data\n)\n</code></pre>"},{"location":"api/audits/#audit-log-format","title":"Audit Log Format","text":"<p>Each audit log is stored as JSON with the following structure:</p> <p>Audit Log Content</p> <pre><code>{\n  \"name\": \"data-pipeline\",\n  \"release\": \"2024-01-15T10:30:00\",\n  \"type\": \"scheduled\",\n  \"context\": {\n    \"params\": {\n      \"source_table\": \"users\",\n      \"target_env\": \"prod\"\n    },\n    \"jobs\": {\n      \"extract\": {\n        \"status\": \"SUCCESS\",\n        \"start_time\": \"2024-01-15T10:30:05\",\n        \"end_time\": \"2024-01-15T10:35:12\"\n      },\n      \"transform\": {\n        \"status\": \"SUCCESS\",\n        \"start_time\": \"2024-01-15T10:35:15\",\n        \"end_time\": \"2024-01-15T10:40:22\"\n      }\n    }\n  },\n  \"parent_run_id\": null,\n  \"run_id\": \"workflow-123\",\n  \"runs_metadata\": {\n    \"execution_time\": \"300s\",\n    \"memory_usage\": \"512MB\",\n    \"cpu_usage\": \"45%\"\n  }\n}\n</code></pre>"},{"location":"api/audits/#cleanup-functionality","title":"Cleanup Functionality","text":"<p>The <code>FileAudit</code> class provides cleanup functionality for old audit files:</p> <p>Audit Cleanup</p> <pre><code>from ddeutil.workflow.audits import FileAudit\n\naudit = FileAudit(type=\"file\", path=\"./audits\")\n\n# Clean up audit files older than 180 days\ncleaned_count = audit.cleanup(max_age_days=180)\nprint(f\"Cleaned up {cleaned_count} old audit files\")\n</code></pre>"},{"location":"api/audits/#database-audits","title":"Database Audits","text":""},{"location":"api/audits/#sqliteaudit","title":"<code>SQLiteAudit</code>","text":"<p>SQLite-based audit implementation for scalable logging with compression support.</p> <p>SQLite Audit Usage</p> <pre><code>from ddeutil.workflow.audits import SQLiteAudit, AuditData\nfrom datetime import datetime\n\n# Create SQLite audit instance\naudit = SQLiteAudit(\n    type=\"sqlite\",\n    path=\"./audits/workflow_audits.db\",\n    extras={\"enable_write_audit\": True}\n)\n\n# Create audit data\ndata = AuditData(\n    name=\"data-pipeline\",\n    release=datetime(2024, 1, 15, 10, 30),\n    type=\"scheduled\",\n    context={\n        \"params\": {\"source_table\": \"users\"},\n        \"jobs\": {\"extract\": {\"status\": \"SUCCESS\"}}\n    },\n    run_id=\"workflow-123\",\n    runs_metadata={\"execution_time\": \"300s\"}\n)\n\n# Save audit log\naudit.save(data)\n\n# Traces are stored in SQLite with compression for efficiency\n</code></pre>"},{"location":"api/audits/#sqlite-schema","title":"SQLite Schema","text":"<p>The SQLite audit creates a comprehensive table structure:</p> <pre><code>CREATE TABLE IF NOT EXISTS audits (\n    workflow        TEXT NOT NULL,\n    release         TEXT NOT NULL,\n    type            TEXT NOT NULL,\n    context         BLOB NOT NULL,\n    parent_run_id   TEXT,\n    run_id          TEXT NOT NULL,\n    metadata        BLOB NOT NULL,\n    created_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (workflow, release)\n)\n</code></pre>"},{"location":"api/audits/#finding-sqlite-audits","title":"Finding SQLite Audits","text":"<p>The <code>SQLiteAudit</code> class provides utilities to search and retrieve audit logs:</p> <p>SQLite Audit Discovery</p> <pre><code>from ddeutil.workflow.audits import SQLiteAudit\n\naudit = SQLiteAudit(type=\"sqlite\", path=\"./audits/workflow_audits.db\")\n\n# Find all audits for a workflow\nfor audit_data in audit.find_audits(\"data-pipeline\"):\n    print(f\"Release: {audit_data.release}\")\n    print(f\"Run ID: {audit_data.run_id}\")\n    print(f\"Context: {audit_data.context}\")\n\n# Find specific audit by release\naudit_data = audit.find_audit_with_release(\n    name=\"data-pipeline\",\n    release=datetime(2024, 1, 15, 10, 30)\n)\n\n# Find latest audit for a workflow\nlatest_audit = audit.find_audit_with_release(name=\"data-pipeline\")\n</code></pre>"},{"location":"api/audits/#compression-support","title":"Compression Support","text":"<p>SQLite audit uses compression for efficient storage:</p> <p>Compression Features</p> <pre><code>from ddeutil.workflow.audits import SQLiteAudit\n\n# Data is automatically compressed using zlib\n# Context and metadata are stored as compressed BLOB\n# This significantly reduces storage requirements for large audit logs\n</code></pre>"},{"location":"api/audits/#sqlite-cleanup","title":"SQLite Cleanup","text":"<p>The <code>SQLiteAudit</code> class provides cleanup functionality for old audit records:</p> <p>SQLite Audit Cleanup</p> <pre><code>from ddeutil.workflow.audits import SQLiteAudit\n\naudit = SQLiteAudit(type=\"sqlite\", path=\"./audits/workflow_audits.db\")\n\n# Clean up audit records older than 180 days\ncleaned_count = audit.cleanup(max_age_days=180)\nprint(f\"Cleaned up {cleaned_count} old audit records\")\n</code></pre>"},{"location":"api/audits/#audit-data-model","title":"Audit Data Model","text":""},{"location":"api/audits/#field-specifications","title":"Field Specifications","text":"Field Type Required Description <code>name</code> str Yes Workflow name <code>release</code> datetime Yes Workflow release timestamp <code>type</code> str Yes Execution type (scheduled, manual, event, rerun) <code>context</code> DictData No Execution context including params and job results <code>parent_run_id</code> str | None No Parent workflow run ID for nested executions <code>run_id</code> str Yes Unique execution identifier <code>runs_metadata</code> DictData No Additional metadata for tracking audit logs"},{"location":"api/audits/#context-structure","title":"Context Structure","text":"<p>The <code>context</code> field contains comprehensive execution information:</p> <p>Context Structure</p> <pre><code>context = {\n    \"params\": {\n        # Input parameters passed to workflow\n        \"source_table\": \"users\",\n        \"batch_date\": \"2024-01-15\",\n        \"environment\": \"production\"\n    },\n    \"jobs\": {\n        # Results from each job execution\n        \"job_name\": {\n            \"status\": \"SUCCESS|FAILED|SKIP|CANCEL\",\n            \"start_time\": \"2024-01-15T10:30:00\",\n            \"end_time\": \"2024-01-15T10:35:00\",\n            \"stages\": {\n                # Stage-level execution details\n                \"stage_name\": {\n                    \"status\": \"SUCCESS\",\n                    \"output\": {\"key\": \"value\"}\n                }\n            }\n        }\n    },\n    \"status\": \"SUCCESS\",  # Overall workflow status\n    \"errors\": {           # Error information if failed\n        \"error_type\": \"WorkflowError\",\n        \"message\": \"Error description\"\n    }\n}\n</code></pre>"},{"location":"api/audits/#factory-function","title":"Factory Function","text":""},{"location":"api/audits/#get_audit","title":"<code>get_audit</code>","text":"<p>Factory function that returns the appropriate audit implementation based on configuration.</p> <p>Dynamic Audit Creation</p> <pre><code>from ddeutil.workflow.audits import get_audit\n\n# Automatically selects appropriate audit implementation\naudit = get_audit(extras={\"custom_config\": \"value\"})\n\n# Configuration determines audit type:\n# - If audit_url points to file: FileAudit\n# - If audit_url points to database: SQLiteAudit\n</code></pre>"},{"location":"api/audits/#configuration","title":"Configuration","text":"<p>Audit behavior is controlled by configuration settings:</p> Setting Description <code>audit_conf</code> Audit configuration including type and path <code>enable_write_audit</code> Enable/disable audit logging <code>audit_url</code> URL/path for audit storage <p>Configuration</p> <pre><code># Enable audit logging\nextras = {\"enable_write_audit\": True}\n\n# File-based audit\naudit_conf = {\n    \"type\": \"file\",\n    \"path\": \"./audits\"\n}\n\n# SQLite-based audit\naudit_conf = {\n    \"type\": \"sqlite\",\n    \"path\": \"./audits/workflow_audits.db\"\n}\n</code></pre>"},{"location":"api/audits/#integration-with-workflows","title":"Integration with Workflows","text":"<p>Audits are automatically created and managed during workflow execution:</p> <p>Workflow Integration</p> <pre><code>from ddeutil.workflow import Workflow\nfrom ddeutil.workflow.audits import get_audit\n\n# Load workflow\nworkflow = Workflow.from_conf(\"data-pipeline\")\n\n# Get audit instance\naudit = get_audit(extras={\"enable_write_audit\": True})\n\n# Execute with audit logging\nresult = workflow.release(\n    release=datetime.now(),\n    params={\"source\": \"users\", \"target\": \"warehouse\"}\n)\n\n# Audit log is automatically created with:\n# - Workflow execution metadata\n# - Input parameters\n# - Job execution results\n# - Final status and timing\n</code></pre>"},{"location":"api/audits/#use-cases","title":"Use Cases","text":""},{"location":"api/audits/#compliance-monitoring","title":"Compliance Monitoring","text":"<p>Compliance Tracking</p> <pre><code>from ddeutil.workflow.audits import FileAudit\n\naudit = FileAudit(type=\"file\", path=\"./audits\")\n\n# Query audits for compliance reporting\nfor audit_data in audit.find_audits(\"financial-etl\"):\n    if audit_data.release.date() == target_date:\n        print(f\"Execution: {audit_data.run_id}\")\n        print(f\"Status: {audit_data.context.get('status')}\")\n        print(f\"Parameters: {audit_data.context.get('params')}\")\n</code></pre>"},{"location":"api/audits/#failure-analysis","title":"Failure Analysis","text":"<p>Error Investigation</p> <pre><code>from ddeutil.workflow.audits import SQLiteAudit\n\naudit = SQLiteAudit(type=\"sqlite\", path=\"./audits/workflow_audits.db\")\n\n# Find failed workflow executions\nfor audit_data in audit.find_audits(\"data-pipeline\"):\n    if audit_data.context.get(\"status\") == \"FAILED\":\n        print(f\"Failed run: {audit_data.run_id}\")\n        print(f\"Error: {audit_data.context.get('errors')}\")\n        print(f\"Failed jobs: {[j for j, data in audit_data.context['jobs'].items()\n                              if data['status'] == 'FAILED']}\")\n</code></pre>"},{"location":"api/audits/#performance-monitoring","title":"Performance Monitoring","text":"<p>Performance Analysis</p> <pre><code>from ddeutil.workflow.audits import FileAudit\nfrom datetime import datetime\n\naudit = FileAudit(type=\"file\", path=\"./audits\")\n\n# Analyze workflow performance trends\nexecution_times = []\nfor audit_data in audit.find_audits(\"etl-workflow\"):\n    start = audit_data.release\n    # Calculate duration from metadata or context\n    duration = audit_data.runs_metadata.get(\"execution_time\", \"0s\")\n    execution_times.append(duration)\n\nprint(f\"Total executions: {len(execution_times)}\")\n</code></pre> <p>Best Practices</p> <ul> <li>Enable auditing in production for compliance and monitoring</li> <li>Configure appropriate retention policies for audit log cleanup</li> <li>Use SQLite for high-volume deployments with compression benefits</li> <li>Use file-based audit for simple deployments with easy file access</li> <li>Regular audit log analysis helps identify patterns and optimization opportunities</li> <li>Monitor audit storage usage and implement cleanup schedules</li> </ul>"},{"location":"api/errors/","title":"Exceptions","text":"<p>The Exceptions module provides a comprehensive error handling system for the workflow package. All exception classes inherit from <code>BaseError</code>, providing a unified error handling interface.</p>"},{"location":"api/errors/#overview","title":"Overview","text":"<p>The exception system provides:</p> <ul> <li>Hierarchical error types: Specific exceptions for different modules</li> <li>Context preservation: Rich error information with context</li> <li>Unified handling: Common base class for all workflow errors</li> <li>Debugging support: Detailed error messages and stack traces</li> </ul>"},{"location":"api/errors/#base-exception","title":"Base Exception","text":""},{"location":"api/errors/#baseerror","title":"BaseError","text":"<p>Base exception class for all workflow-related errors.</p> <p>Base Error Usage</p> <pre><code>from ddeutil.workflow.errors import BaseError\n\ntry:\n    # Workflow operations\n    workflow.execute(params)\nexcept BaseError as e:\n    print(f\"Workflow error: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n</code></pre>"},{"location":"api/errors/#module-specific-exceptions","title":"Module-Specific Exceptions","text":""},{"location":"api/errors/#utilerror","title":"UtilError","text":"<p>Utility exception class raised from the <code>utils</code> and <code>reusables</code> modules.</p> <p>Utility Error</p> <pre><code>from ddeutil.workflow.errors import UtilError\n\ntry:\n    # Template processing\n    result = str2template(\"${{ invalid.template }}\", params)\nexcept UtilError as e:\n    print(f\"Template error: {e}\")\n</code></pre> <p>Common Causes: - Invalid template syntax - Missing template variables - Filter function errors - Parameter validation failures</p>"},{"location":"api/errors/#resulterror","title":"ResultError","text":"<p>Result exception class raised from the <code>results</code> module.</p> <p>Result Error</p> <pre><code>from ddeutil.workflow.errors import ResultError\n\ntry:\n    # Result operations\n    result.validate()\nexcept ResultError as e:\n    print(f\"Result validation error: {e}\")\n</code></pre> <p>Common Causes: - Invalid result status - Missing required result fields - Result serialization errors</p>"},{"location":"api/errors/#stageerror","title":"StageError","text":"<p>Stage execution exception class raised during stage processing.</p> <p>Stage Error</p> <pre><code>from ddeutil.workflow.errors import StageError\n\ntry:\n    # Stage execution\n    stage.execute(params)\nexcept StageError as e:\n    print(f\"Stage execution failed: {e}\")\n    print(f\"Stage: {e.stage_name}\")\n    print(f\"Error details: {e.details}\")\n</code></pre> <p>Common Causes: - Stage execution failures - Invalid stage configuration - Stage timeout errors - Resource allocation failures</p>"},{"location":"api/errors/#joberror","title":"JobError","text":"<p>Job execution exception class raised during job processing.</p> <p>Job Error</p> <pre><code>from ddeutil.workflow.errors import JobError\n\ntry:\n    # Job execution\n    job.execute(params)\nexcept JobError as e:\n    print(f\"Job execution failed: {e}\")\n    print(f\"Job ID: {e.job_id}\")\n    print(f\"Failed stages: {e.failed_stages}\")\n</code></pre> <p>Common Causes: - Job dependency failures - Matrix strategy errors - Job timeout errors - Stage orchestration failures</p>"},{"location":"api/errors/#workflowerror","title":"WorkflowError","text":"<p>Workflow execution exception class raised during workflow processing.</p> <p>Workflow Error</p> <pre><code>from ddeutil.workflow.errors import WorkflowError\n\ntry:\n    # Workflow execution\n    workflow.execute(params)\nexcept WorkflowError as e:\n    print(f\"Workflow execution failed: {e}\")\n    print(f\"Workflow: {e.workflow_name}\")\n    print(f\"Failed jobs: {e.failed_jobs}\")\n</code></pre> <p>Common Causes: - Workflow configuration errors - Job orchestration failures - Parameter validation errors - Workflow timeout errors</p>"},{"location":"api/errors/#paramerror","title":"ParamError","text":"<p>Parameter validation exception class raised during parameter processing.</p> <p>Parameter Error</p> <pre><code>from ddeutil.workflow.errors import ParamError\n\ntry:\n    # Parameter validation\n    params.validate()\nexcept ParamError as e:\n    print(f\"Parameter validation failed: {e}\")\n    print(f\"Invalid parameter: {e.param_name}\")\n    print(f\"Validation error: {e.validation_error}\")\n</code></pre> <p>Common Causes: - Missing required parameters - Invalid parameter types - Parameter constraint violations - Template resolution errors</p>"},{"location":"api/errors/#scheduleexception","title":"ScheduleException","text":"<p>Schedule-related exception class raised during scheduling operations.</p> <p>Schedule Error</p> <pre><code>from ddeutil.workflow.errors import ScheduleException\n\ntry:\n    # Schedule operations\n    schedule.generate_next()\nexcept ScheduleException as e:\n    print(f\"Schedule error: {e}\")\n    print(f\"Schedule: {e.schedule_name}\")\n    print(f\"Error type: {e.error_type}\")\n</code></pre> <p>Common Causes: - Invalid cron expressions - Schedule parsing errors - Timezone conversion errors - Schedule conflict errors</p>"},{"location":"api/errors/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/errors/#generic-error-handling","title":"Generic Error Handling","text":"<pre><code>from ddeutil.workflow.errors import BaseError\n\ndef safe_workflow_execution(workflow, params):\n    \"\"\"Execute workflow with comprehensive error handling.\"\"\"\n    try:\n        result = workflow.execute(params)\n        return result\n    except BaseError as e:\n        # Log the error\n        logger.error(f\"Workflow execution failed: {e}\")\n\n        # Handle specific error types\n        if isinstance(e, ParamError):\n            logger.error(f\"Parameter error: {e.param_name}\")\n            # Retry with default parameters\n            return retry_with_defaults(workflow)\n        elif isinstance(e, StageError):\n            logger.error(f\"Stage error: {e.stage_name}\")\n            # Skip failed stage\n            return skip_failed_stage(workflow, e.stage_name)\n        elif isinstance(e, WorkflowError):\n            logger.error(f\"Workflow error: {e.workflow_name}\")\n            # Abort workflow\n            return abort_workflow(workflow)\n\n        # Re-raise unknown errors\n        raise\n</code></pre>"},{"location":"api/errors/#specific-error-handling","title":"Specific Error Handling","text":"<pre><code>from ddeutil.workflow.errors import (\n    UtilError, ResultError, StageError, JobError,\n    WorkflowError, ParamError, ScheduleException\n)\n\ndef handle_workflow_errors(func):\n    \"\"\"Decorator for handling workflow-specific errors.\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ParamError as e:\n            logger.error(f\"Parameter validation failed: {e}\")\n            # Return default result\n            return create_default_result()\n        except StageError as e:\n            logger.error(f\"Stage execution failed: {e}\")\n            # Mark stage as failed and continue\n            return mark_stage_failed(e.stage_name)\n        except JobError as e:\n            logger.error(f\"Job execution failed: {e}\")\n            # Retry job with exponential backoff\n            return retry_job_with_backoff(e.job_id)\n        except WorkflowError as e:\n            logger.error(f\"Workflow execution failed: {e}\")\n            # Abort entire workflow\n            return abort_workflow_execution()\n        except ScheduleException as e:\n            logger.error(f\"Schedule error: {e}\")\n            # Use fallback schedule\n            return use_fallback_schedule()\n        except (UtilError, ResultError) as e:\n            logger.error(f\"System error: {e}\")\n            # System-level error, re-raise\n            raise\n    return wrapper\n</code></pre>"},{"location":"api/errors/#error-recovery-strategies","title":"Error Recovery Strategies","text":"<pre><code>from ddeutil.workflow.errors import BaseError\n\nclass WorkflowErrorHandler:\n    \"\"\"Error handler with recovery strategies.\"\"\"\n\n    def handle_error(self, error: BaseError, context: dict):\n        \"\"\"Handle workflow errors with appropriate recovery strategies.\"\"\"\n\n        if isinstance(error, ParamError):\n            return self.handle_param_error(error, context)\n        elif isinstance(error, StageError):\n            return self.handle_stage_error(error, context)\n        elif isinstance(error, JobError):\n            return self.handle_job_error(error, context)\n        elif isinstance(error, WorkflowError):\n            return self.handle_workflow_error(error, context)\n        else:\n            return self.handle_unknown_error(error, context)\n\n    def handle_param_error(self, error: ParamError, context: dict):\n        \"\"\"Handle parameter errors with default values.\"\"\"\n        logger.warning(f\"Using default values for parameter: {error.param_name}\")\n        return context.get('default_params', {})\n\n    def handle_stage_error(self, error: StageError, context: dict):\n        \"\"\"Handle stage errors with retry logic.\"\"\"\n        retry_count = context.get('retry_count', 0)\n        if retry_count &lt; 3:\n            logger.info(f\"Retrying stage {error.stage_name}\")\n            return {'retry': True, 'stage': error.stage_name}\n        else:\n            logger.error(f\"Stage {error.stage_name} failed after 3 retries\")\n            return {'skip': True, 'stage': error.stage_name}\n\n    def handle_job_error(self, error: JobError, context: dict):\n        \"\"\"Handle job errors with dependency resolution.\"\"\"\n        logger.error(f\"Job {error.job_id} failed\")\n        return {'abort_job': True, 'job_id': error.job_id}\n\n    def handle_workflow_error(self, error: WorkflowError, context: dict):\n        \"\"\"Handle workflow errors with graceful shutdown.\"\"\"\n        logger.error(f\"Workflow {error.workflow_name} failed\")\n        return {'abort_workflow': True, 'workflow': error.workflow_name}\n\n    def handle_unknown_error(self, error: BaseError, context: dict):\n        \"\"\"Handle unknown errors with logging.\"\"\"\n        logger.error(f\"Unknown error: {error}\")\n        return {'unknown_error': True, 'error': str(error)}\n</code></pre>"},{"location":"api/errors/#error-context-and-debugging","title":"Error Context and Debugging","text":""},{"location":"api/errors/#error-information","title":"Error Information","text":"<p>All workflow exceptions provide rich context information:</p> <pre><code>try:\n    workflow.execute(params)\nexcept BaseError as e:\n    print(f\"Error message: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Error context: {getattr(e, 'context', {})}\")\n    print(f\"Stack trace: {e.__traceback__}\")\n</code></pre>"},{"location":"api/errors/#debugging-support","title":"Debugging Support","text":"<pre><code>import logging\nfrom ddeutil.workflow.errors import BaseError\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\ndef debug_workflow_execution(workflow, params):\n    \"\"\"Execute workflow with debug information.\"\"\"\n    try:\n        result = workflow.execute(params)\n        return result\n    except BaseError as e:\n        # Log detailed error information\n        logger.debug(f\"Error details: {e}\")\n        logger.debug(f\"Error context: {getattr(e, 'context', {})}\")\n        logger.debug(f\"Error attributes: {dir(e)}\")\n\n        # Re-raise for further handling\n        raise\n</code></pre>"},{"location":"api/errors/#best-practices","title":"Best Practices","text":""},{"location":"api/errors/#1-error-handling-strategy","title":"1. Error Handling Strategy","text":"<ul> <li>Always catch <code>BaseError</code> for workflow operations</li> <li>Use specific exception types for targeted handling</li> <li>Implement appropriate recovery strategies</li> <li>Log errors with sufficient context</li> </ul>"},{"location":"api/errors/#2-error-recovery","title":"2. Error Recovery","text":"<ul> <li>Implement retry logic for transient failures</li> <li>Use fallback values for parameter errors</li> <li>Skip failed stages when appropriate</li> <li>Gracefully handle workflow aborts</li> </ul>"},{"location":"api/errors/#3-error-logging","title":"3. Error Logging","text":"<ul> <li>Log errors with appropriate levels</li> <li>Include context information in log messages</li> <li>Use structured logging for error analysis</li> <li>Preserve stack traces for debugging</li> </ul>"},{"location":"api/errors/#4-error-propagation","title":"4. Error Propagation","text":"<ul> <li>Re-raise system-level errors</li> <li>Transform errors when appropriate</li> <li>Provide meaningful error messages</li> <li>Maintain error context through layers</li> </ul>"},{"location":"api/errors/#5-testing-error-scenarios","title":"5. Testing Error Scenarios","text":"<ul> <li>Test error handling paths</li> <li>Verify recovery strategies</li> <li>Validate error messages</li> <li>Test error propagation</li> </ul>"},{"location":"api/event/","title":"Event API Reference","text":"<p>The Event module provides cron-based scheduling and event-driven triggers for workflow orchestration, enabling time-based execution and release-based triggering.</p>"},{"location":"api/event/#overview","title":"Overview","text":"<p>The Event module implements a cron-based scheduling system that provides:</p> <ul> <li>Cron scheduling: Traditional cron expressions for time-based triggers</li> <li>Interval scheduling: Simple interval-based scheduling (daily, weekly, monthly)</li> <li>Year-aware scheduling: Extended cron with year support for tools like AWS Glue</li> <li>Release events: Workflow-to-workflow triggering based on completion events</li> <li>Timezone support: Full timezone handling for global deployments</li> <li>Validation: Comprehensive validation of cron expressions and schedules</li> </ul>"},{"location":"api/event/#quick-start","title":"Quick Start","text":"<pre><code>from ddeutil.workflow.event import Crontab, CrontabValue, Event\nfrom datetime import datetime\n\n# Create a daily schedule at 9:30 AM\ndaily_schedule = CrontabValue(\n    interval=\"daily\",\n    time=\"09:30\",\n    timezone=\"America/New_York\"\n)\n\n# Create a traditional cron schedule\ncron_schedule = Crontab(\n    cronjob=\"0 9 * * 1-5\",  # 9 AM on weekdays\n    timezone=\"UTC\"\n)\n\n# Create an event with multiple schedules\nevent = Event(\n    schedule=[daily_schedule, cron_schedule],\n    release=[\"upstream-workflow\"]\n)\n\n# Generate next execution time\nrunner = cron_schedule.generate(datetime.now())\nnext_run = runner.next\nprint(f\"Next execution: {next_run}\")\n</code></pre>"},{"location":"api/event/#classes","title":"Classes","text":""},{"location":"api/event/#basecrontab","title":"BaseCrontab","text":"<p>Base class for crontab-based scheduling models.</p>"},{"location":"api/event/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>extras</code> <code>DictData</code> <code>{}</code> Additional parameters to pass to the CronJob field <code>tz</code> <code>TimeZoneName</code> <code>\"UTC\"</code> Timezone string value (alias: timezone)"},{"location":"api/event/#crontabvalue","title":"CrontabValue","text":"<p>Crontab model using interval-based specification for simplified scheduling.</p>"},{"location":"api/event/#attributes_1","title":"Attributes","text":"Attribute Type Default Description <code>interval</code> <code>Interval</code> - Scheduling interval string ('daily', 'weekly', 'monthly') <code>day</code> <code>str</code> <code>None</code> Day specification for weekly/monthly schedules <code>time</code> <code>str</code> <code>\"00:00\"</code> Time of day in 'HH:MM' format <code>tz</code> <code>TimeZoneName</code> <code>\"UTC\"</code> Timezone string value"},{"location":"api/event/#methods","title":"Methods","text":""},{"location":"api/event/#cronjob-property","title":"<code>cronjob</code> (property)","text":"<p>Get CronJob object built from interval format.</p> <p>Returns: - <code>CronJob</code>: CronJob instance configured with interval-based schedule</p>"},{"location":"api/event/#generatestart","title":"<code>generate(start)</code>","text":"<p>Generate CronRunner from initial datetime.</p> <p>Parameters: - <code>start</code> (Union[str, datetime]): Starting datetime for schedule generation</p> <p>Returns: - <code>CronRunner</code>: CronRunner instance for schedule generation</p>"},{"location":"api/event/#nextstart","title":"<code>next(start)</code>","text":"<p>Get next scheduled datetime after given start time.</p> <p>Parameters: - <code>start</code> (Union[str, datetime]): Starting datetime for schedule generation</p> <p>Returns: - <code>CronRunner</code>: CronRunner instance positioned at next scheduled time</p>"},{"location":"api/event/#crontab","title":"Crontab","text":"<p>Cron event model wrapping CronJob functionality for traditional cron expressions.</p>"},{"location":"api/event/#attributes_2","title":"Attributes","text":"Attribute Type Default Description <code>cronjob</code> <code>CronJob</code> - CronJob instance for schedule validation and generation <code>tz</code> <code>TimeZoneName</code> <code>\"UTC\"</code> Timezone string value"},{"location":"api/event/#methods_1","title":"Methods","text":""},{"location":"api/event/#generatestart_1","title":"<code>generate(start)</code>","text":"<p>Generate schedule runner from start time.</p> <p>Parameters: - <code>start</code> (Union[str, datetime]): Starting datetime for schedule generation</p> <p>Returns: - <code>CronRunner</code>: CronRunner instance for schedule generation</p>"},{"location":"api/event/#nextstart_1","title":"<code>next(start)</code>","text":"<p>Get runner positioned at next scheduled time.</p> <p>Parameters: - <code>start</code> (Union[str, datetime]): Starting datetime for schedule generation</p> <p>Returns: - <code>CronRunner</code>: CronRunner instance positioned at next scheduled time</p>"},{"location":"api/event/#crontabyear","title":"CrontabYear","text":"<p>Cron event model with enhanced year-based scheduling, particularly useful for tools like AWS Glue.</p>"},{"location":"api/event/#attributes_3","title":"Attributes","text":"Attribute Type Default Description <code>cronjob</code> <code>CronJobYear</code> - CronJobYear instance for year-aware schedule validation <code>tz</code> <code>TimeZoneName</code> <code>\"UTC\"</code> Timezone string value"},{"location":"api/event/#event","title":"Event","text":"<p>Event model for defining workflow triggers combining scheduled and release-based events.</p>"},{"location":"api/event/#attributes_4","title":"Attributes","text":"Attribute Type Default Description <code>schedule</code> <code>list[Cron]</code> <code>[]</code> List of cron schedules for time-based triggers <code>release</code> <code>list[str]</code> <code>[]</code> List of workflow names for release-based triggers"},{"location":"api/event/#functions","title":"Functions","text":""},{"location":"api/event/#interval2crontabinterval-daynone-time0000","title":"interval2crontab(interval, *, day=None, time=\"00:00\")","text":"<p>Convert interval specification to cron expression.</p> <p>Parameters: - <code>interval</code> (Interval): Scheduling interval ('daily', 'weekly', or 'monthly') - <code>day</code> (str, optional): Day of week for weekly intervals or monthly schedules - <code>time</code> (str): Time of day in 'HH:MM' format</p> <p>Returns: - <code>str</code>: Generated crontab expression string</p>"},{"location":"api/event/#usage-examples","title":"Usage Examples","text":""},{"location":"api/event/#basic-cron-scheduling","title":"Basic Cron Scheduling","text":"<pre><code>from ddeutil.workflow.event import Crontab\nfrom datetime import datetime\n\n# Create a cron schedule for every weekday at 9 AM\nschedule = Crontab(\n    cronjob=\"0 9 * * 1-5\",\n    timezone=\"America/New_York\"\n)\n\n# Generate next execution times\nrunner = schedule.generate(datetime.now())\nprint(f\"Next execution: {runner.next}\")\nprint(f\"Following execution: {runner.next}\")\n</code></pre>"},{"location":"api/event/#interval-based-scheduling","title":"Interval-Based Scheduling","text":"<pre><code>from ddeutil.workflow.event import CrontabValue\n\n# Daily schedule at 2:30 PM\ndaily_schedule = CrontabValue(\n    interval=\"daily\",\n    time=\"14:30\",\n    timezone=\"UTC\"\n)\n\n# Weekly schedule on Friday at 6 PM\nweekly_schedule = CrontabValue(\n    interval=\"weekly\",\n    day=\"friday\",\n    time=\"18:00\",\n    timezone=\"Europe/London\"\n)\n\n# Monthly schedule on the 1st at midnight\nmonthly_schedule = CrontabValue(\n    interval=\"monthly\",\n    time=\"00:00\",\n    timezone=\"Asia/Tokyo\"\n)\n\n# Generate next execution\nrunner = weekly_schedule.generate(datetime.now())\nnext_run = runner.next\nprint(f\"Next Friday 6 PM: {next_run}\")\n</code></pre>"},{"location":"api/event/#year-aware-scheduling","title":"Year-Aware Scheduling","text":"<pre><code>from ddeutil.workflow.event import CrontabYear\n\n# AWS Glue compatible schedule with year\nglue_schedule = CrontabYear(\n    cronjob=\"0 12 1 * ? 2024\",  # First day of every month at noon in 2024\n    timezone=\"UTC\"\n)\n\n# Generate schedule for specific year\nrunner = glue_schedule.generate(datetime(2024, 1, 1))\nexecutions = []\nfor _ in range(12):  # Get all monthly executions\n    executions.append(runner.next)\n\nprint(\"Monthly executions in 2024:\")\nfor execution in executions:\n    print(f\"  {execution}\")\n</code></pre>"},{"location":"api/event/#complex-event-configuration","title":"Complex Event Configuration","text":"<pre><code>from ddeutil.workflow.event import Event, Crontab, CrontabValue\n\n# Create multiple schedules\nmorning_schedule = CrontabValue(\n    interval=\"daily\",\n    time=\"09:00\",\n    timezone=\"UTC\"\n)\n\nevening_schedule = Crontab(\n    cronjob=\"0 18 * * 1-5\",  # 6 PM on weekdays\n    timezone=\"UTC\"\n)\n\nweekend_schedule = CrontabValue(\n    interval=\"weekly\",\n    day=\"saturday\",\n    time=\"10:00\",\n    timezone=\"UTC\"\n)\n\n# Create event with multiple triggers\nevent = Event(\n    schedule=[morning_schedule, evening_schedule, weekend_schedule],\n    release=[\"data-ingestion-workflow\", \"validation-workflow\"]\n)\n\n# This workflow will trigger:\n# 1. Daily at 9 AM\n# 2. Weekdays at 6 PM\n# 3. Saturdays at 10 AM\n# 4. When data-ingestion-workflow completes\n# 5. When validation-workflow completes\n</code></pre>"},{"location":"api/event/#timezone-handling","title":"Timezone Handling","text":"<pre><code>from ddeutil.workflow.event import Crontab\nfrom datetime import datetime\n\n# Create schedules in different timezones\nny_schedule = Crontab(\n    cronjob=\"0 9 * * *\",  # 9 AM Eastern\n    timezone=\"America/New_York\"\n)\n\nlondon_schedule = Crontab(\n    cronjob=\"0 9 * * *\",  # 9 AM GMT\n    timezone=\"Europe/London\"\n)\n\ntokyo_schedule = Crontab(\n    cronjob=\"0 9 * * *\",  # 9 AM JST\n    timezone=\"Asia/Tokyo\"\n)\n\n# Generate next execution times\nnow = datetime.now()\nschedules = [\n    (\"New York\", ny_schedule),\n    (\"London\", london_schedule),\n    (\"Tokyo\", tokyo_schedule)\n]\n\nfor name, schedule in schedules:\n    runner = schedule.generate(now)\n    next_run = runner.next\n    print(f\"{name}: {next_run}\")\n</code></pre>"},{"location":"api/event/#interval-conversion","title":"Interval Conversion","text":"<pre><code>from ddeutil.workflow.event import interval2crontab\n\n# Convert various intervals to cron expressions\nexamples = [\n    (\"daily\", None, \"01:30\"),\n    (\"weekly\", \"friday\", \"18:30\"),\n    (\"monthly\", None, \"00:00\"),\n    (\"monthly\", \"tuesday\", \"12:00\"),\n]\n\nfor interval, day, time in examples:\n    cron_expr = interval2crontab(interval, day=day, time=time)\n    print(f\"{interval} {day or ''} at {time}: {cron_expr}\")\n\n# Output:\n# daily  at 01:30: 1 30 * * *\n# weekly friday at 18:30: 18 30 * * 5\n# monthly  at 00:00: 0 0 1 * *\n# monthly tuesday at 12:00: 12 0 1 * 2\n</code></pre>"},{"location":"api/event/#release-based-triggering","title":"Release-Based Triggering","text":"<pre><code>from ddeutil.workflow.event import Event\n\n# Create event that triggers on workflow completion\ndownstream_event = Event(\n    release=[\n        \"etl-pipeline\",\n        \"data-validation\",\n        \"quality-check\"\n    ]\n)\n\n# This event will trigger when any of the specified workflows complete\n# Useful for creating dependent workflow chains\n</code></pre>"},{"location":"api/event/#advanced-scheduling-patterns","title":"Advanced Scheduling Patterns","text":"<pre><code>from ddeutil.workflow.event import Crontab, CrontabValue, Event\n\n# Business hours schedule (9 AM - 5 PM, weekdays)\nbusiness_hours_schedules = [\n    Crontab(cronjob=f\"0 {hour} * * 1-5\", timezone=\"UTC\")\n    for hour in range(9, 18)\n]\n\n# Quarter-end schedule (last day of quarter)\nquarter_end_schedule = Crontab(\n    cronjob=\"0 23 31 3,6,9,12 *\",  # Last day of quarters at 11 PM\n    timezone=\"UTC\"\n)\n\n# Monthly reporting schedule (first Monday of each month)\nmonthly_reporting = Crontab(\n    cronjob=\"0 8 1-7 * 1\",  # 8 AM on first Monday\n    timezone=\"UTC\"\n)\n\n# Combine all schedules\ncomprehensive_event = Event(\n    schedule=business_hours_schedules + [\n        quarter_end_schedule,\n        monthly_reporting\n    ],\n    release=[\"upstream-data-pipeline\"]\n)\n</code></pre>"},{"location":"api/event/#schedule-validation-and-debugging","title":"Schedule Validation and Debugging","text":"<pre><code>from ddeutil.workflow.event import Crontab, CrontabValue\nfrom datetime import datetime, timedelta\n\ndef validate_schedule(schedule, hours_ahead=24):\n    \"\"\"Validate and debug schedule generation.\"\"\"\n    now = datetime.now()\n    future = now + timedelta(hours=hours_ahead)\n\n    print(f\"Schedule: {schedule}\")\n    print(f\"Timezone: {schedule.tz}\")\n\n    runner = schedule.generate(now)\n    executions = []\n\n    current = runner.next\n    while current &lt;= future:\n        executions.append(current)\n        current = runner.next\n\n    print(f\"Next {len(executions)} executions:\")\n    for i, execution in enumerate(executions):\n        print(f\"  {i+1}. {execution}\")\n\n    return executions\n\n# Test different schedules\nschedules = [\n    Crontab(cronjob=\"0 */4 * * *\", timezone=\"UTC\"),  # Every 4 hours\n    CrontabValue(interval=\"daily\", time=\"12:00\", timezone=\"UTC\"),\n    Crontab(cronjob=\"0 9 * * 1-5\", timezone=\"America/New_York\"),\n]\n\nfor schedule in schedules:\n    validate_schedule(schedule)\n    print(\"-\" * 50)\n</code></pre>"},{"location":"api/event/#best-practices","title":"Best Practices","text":""},{"location":"api/event/#1-schedule-design","title":"1. Schedule Design","text":"<ul> <li>Clear expressions: Use readable cron expressions or interval formats</li> <li>Timezone awareness: Always specify appropriate timezones</li> <li>Validation: Test schedule generation before deployment</li> <li>Documentation: Document complex cron expressions</li> </ul>"},{"location":"api/event/#2-performance-considerations","title":"2. Performance Considerations","text":"<ul> <li>Limit schedules: Keep the number of schedules per event reasonable (\u226410)</li> <li>Efficient expressions: Use efficient cron expressions to minimize CPU usage</li> <li>Timezone caching: Timezone objects are cached for performance</li> <li>Memory usage: Large numbers of schedules can impact memory</li> </ul>"},{"location":"api/event/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Invalid expressions: Handle cron expression validation errors</li> <li>Timezone errors: Validate timezone strings</li> <li>Schedule conflicts: Avoid overlapping schedules that might cause issues</li> <li>Resource limits: Monitor resource usage with frequent schedules</li> </ul>"},{"location":"api/event/#4-debugging","title":"4. Debugging","text":"<ul> <li>Schedule testing: Test schedules with different start times</li> <li>Timezone verification: Verify timezone behavior with daylight saving time</li> <li>Expression validation: Validate cron expressions before deployment</li> <li>Execution tracking: Monitor actual execution times vs. scheduled times</li> <li>Use logging: Enable debug logging for schedule processing</li> </ul>"},{"location":"api/event/#validation-rules","title":"Validation Rules","text":""},{"location":"api/event/#schedule-validation","title":"Schedule Validation","text":"<p>The Event model enforces several validation rules:</p> <ol> <li>No duplicate schedules: Each cron expression must be unique</li> <li>Timezone consistency: All schedules in an event must use the same timezone</li> <li>Schedule limit: Maximum of 10 schedules per event</li> <li>Valid expressions: All cron expressions must be syntactically valid</li> </ol>"},{"location":"api/event/#interval-validation","title":"Interval Validation","text":"<ul> <li>Time format: Time must be in 'HH:MM' format</li> <li>Day validation: Day names must be valid (Monday, Tuesday, etc.)</li> <li>Interval types: Only 'daily', 'weekly', 'monthly' are supported</li> </ul>"},{"location":"api/event/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/event/#supported-cron-formats","title":"Supported Cron Formats","text":"Format Example Description Standard <code>0 9 * * 1-5</code> Minute Hour Day Month DayOfWeek Year-aware <code>0 9 * * ? 2024</code> Minute Hour Day Month DayOfWeek Year"},{"location":"api/event/#timezone-support","title":"Timezone Support","text":"<p>The module uses <code>TimeZoneName</code> validation from <code>pydantic_extra_types</code>, supporting: - IANA timezone names (e.g., 'America/New_York') - UTC and GMT - Common timezone abbreviations</p>"},{"location":"api/event/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>WORKFLOW_EVENT_DEFAULT_TIMEZONE</code> <code>UTC</code> Default timezone for schedules <code>WORKFLOW_EVENT_MAX_SCHEDULES</code> <code>10</code> Maximum schedules per event"},{"location":"api/event/#common-patterns","title":"Common Patterns","text":""},{"location":"api/event/#data-pipeline-scheduling","title":"Data Pipeline Scheduling","text":"<pre><code># ETL pipeline with multiple triggers\netl_event = Event(\n    schedule=[\n        CrontabValue(interval=\"daily\", time=\"02:00\", timezone=\"UTC\"),  # Daily at 2 AM\n        Crontab(cronjob=\"0 */6 * * *\", timezone=\"UTC\"),  # Every 6 hours\n    ],\n    release=[\"data-source-updated\"]\n)\n</code></pre>"},{"location":"api/event/#reporting-schedules","title":"Reporting Schedules","text":"<pre><code># Business reporting schedule\nreporting_event = Event(\n    schedule=[\n        CrontabValue(interval=\"weekly\", day=\"monday\", time=\"08:00\"),  # Weekly reports\n        Crontab(cronjob=\"0 9 1 * *\", timezone=\"UTC\"),  # Monthly reports\n        Crontab(cronjob=\"0 10 1 1,4,7,10 *\", timezone=\"UTC\"),  # Quarterly reports\n    ]\n)\n</code></pre>"},{"location":"api/event/#maintenance-windows","title":"Maintenance Windows","text":"<pre><code># Maintenance and cleanup schedules\nmaintenance_event = Event(\n    schedule=[\n        Crontab(cronjob=\"0 3 * * 0\", timezone=\"UTC\"),  # Weekly maintenance (Sunday 3 AM)\n        Crontab(cronjob=\"0 2 1 * *\", timezone=\"UTC\"),  # Monthly cleanup\n    ]\n)\n</code></pre>"},{"location":"api/event/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/event/#common-issues","title":"Common Issues","text":""},{"location":"api/event/#invalid-cron-expression","title":"Invalid Cron Expression","text":"<pre><code># Problem: Invalid cron expression\ntry:\n    schedule = Crontab(cronjob=\"invalid expression\")\nexcept ValueError as e:\n    print(f\"Invalid cron expression: {e}\")\n\n# Solution: Use valid cron format\nschedule = Crontab(cronjob=\"0 9 * * 1-5\")  # 9 AM weekdays\n</code></pre>"},{"location":"api/event/#timezone-issues","title":"Timezone Issues","text":"<pre><code># Problem: Incorrect timezone\ntry:\n    schedule = Crontab(cronjob=\"0 9 * * *\", timezone=\"Invalid/Timezone\")\nexcept ValueError as e:\n    print(f\"Invalid timezone: {e}\")\n\n# Solution: Use valid IANA timezone\nschedule = Crontab(cronjob=\"0 9 * * *\", timezone=\"America/New_York\")\n</code></pre>"},{"location":"api/event/#schedule-conflicts","title":"Schedule Conflicts","text":"<pre><code># Problem: Duplicate schedules\ntry:\n    event = Event(schedule=[\n        Crontab(cronjob=\"0 9 * * *\"),\n        Crontab(cronjob=\"0 9 * * *\"),  # Duplicate\n    ])\nexcept ValueError as e:\n    print(f\"Duplicate schedule: {e}\")\n\n# Solution: Use unique schedules\nevent = Event(schedule=[\n    Crontab(cronjob=\"0 9 * * *\"),\n    Crontab(cronjob=\"0 18 * * *\"),  # Different time\n])\n</code></pre>"},{"location":"api/event/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Test expressions: Use online cron expression testers</li> <li>Validate timezones: Verify timezone strings with <code>zoneinfo</code></li> <li>Check generation: Test schedule generation with different start times</li> <li>Monitor execution: Track actual vs. scheduled execution times</li> <li>Use logging: Enable debug logging for schedule processing</li> </ol>"},{"location":"api/job/","title":"Job API Reference","text":"<p>The Job module provides execution containers for stage orchestration within workflows. Jobs manage stage lifecycle, dependency resolution, matrix strategies, and multi-environment execution.</p>"},{"location":"api/job/#overview","title":"Overview","text":"<p>Jobs are the primary execution units within workflows, serving as containers for multiple stages that execute sequentially. They provide comprehensive support for:</p> <ul> <li>Stage execution orchestration and lifecycle management</li> <li>Matrix strategies for parameterized parallel execution</li> <li>Multi-environment deployment (local, self-hosted, Docker, Azure Batch, AWS Batch, Cloud Batch)</li> <li>Dependency management through job needs</li> <li>Conditional execution based on dynamic expressions</li> <li>Output coordination between stages and jobs</li> </ul>"},{"location":"api/job/#job-execution-flow","title":"Job Execution Flow","text":"<pre><code>flowchart TD\n    A[\"Job.execute()\"] --&gt; B[\"Generate run_id\"]\n    B --&gt; C[\"Initialize trace and context\"]\n    C --&gt; D[\"Check job condition\"]\n    D --&gt;|Skip| E[\"Return SKIP Result\"]\n    D --&gt;|Execute| F[\"Route to execution environment\"]\n\n    F --&gt; G{\"Environment Type\"}\n    G --&gt;|LOCAL| H[\"local_execute()\"]\n    G --&gt;|DOCKER| I[\"docker_execution()\"]\n    G --&gt;|SELF_HOSTED| J[\"self_hosted_execute()\"]\n    G --&gt;|AZ_BATCH| K[\"azure_batch_execute()\"]\n    G --&gt;|AWS_BATCH| L[\"aws_batch_execute()\"]\n    G --&gt;|CLOUD_BATCH| M[\"cloud_batch_execute()\"]\n\n    H --&gt; N[\"Generate matrix strategies\"]\n    N --&gt; O[\"Create ThreadPoolExecutor\"]\n    O --&gt; P[\"Submit strategy executions\"]\n    P --&gt; Q[\"Execute strategies in parallel\"]\n    Q --&gt; R[\"Collect results\"]\n    R --&gt; S[\"Validate final status\"]\n    S --&gt; T[\"Return Result\"]\n\n    I --&gt; T\n    J --&gt; T\n    K --&gt; T\n    L --&gt; T\n    M --&gt; T\n    E --&gt; T\n\n    style A fill:#e1f5fe\n    style T fill:#c8e6c9\n    style E fill:#ffcdd2</code></pre>"},{"location":"api/job/#strategy-execution-flow","title":"Strategy Execution Flow","text":"<pre><code>flowchart TD\n    A[\"local_execute_strategy()\"] --&gt; B[\"Generate strategy_id\"]\n    B --&gt; C[\"Initialize context with matrix\"]\n    C --&gt; D[\"Execute stages sequentially\"]\n    D --&gt; E{\"Stage Result\"}\n    E --&gt;|SUCCESS| F[\"Continue to next stage\"]\n    E --&gt;|FAILED| G[\"Raise JobError\"]\n    E --&gt;|SKIP| H[\"Mark stage as skipped\"]\n    E --&gt;|CANCEL| I[\"Raise JobCancelError\"]\n\n    F --&gt; J{\"More stages?\"}\n    J --&gt;|Yes| D\n    J --&gt;|No| K[\"Determine final status\"]\n\n    H --&gt; J\n    G --&gt; L[\"Return FAILED\"]\n    I --&gt; M[\"Return CANCEL\"]\n    K --&gt; N[\"Return SUCCESS/SKIP\"]\n\n    style A fill:#e1f5fe\n    style N fill:#c8e6c9\n    style L fill:#ffcdd2\n    style M fill:#ffcdd2</code></pre>"},{"location":"api/job/#classes","title":"Classes","text":""},{"location":"api/job/#job","title":"Job","text":"<p>Job execution container for stage orchestration.</p> <p>The Job model represents a logical unit of work containing multiple stages that execute sequentially. Jobs support matrix strategies for parameterized execution, dependency management, conditional execution, and multi-environment deployment.</p>"},{"location":"api/job/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>id</code> <code>str \\| None</code> <code>None</code> Unique job identifier within workflow <code>desc</code> <code>str \\| None</code> <code>None</code> Job description in markdown format <code>runs_on</code> <code>RunsOnModel</code> <code>OnLocal()</code> Execution environment configuration <code>condition</code> <code>str \\| None</code> <code>None</code> Conditional execution expression <code>stages</code> <code>list[Stage]</code> <code>[]</code> Ordered list of stages to execute <code>trigger_rule</code> <code>Rule</code> <code>ALL_SUCCESS</code> Rule for handling job dependencies <code>needs</code> <code>list[str]</code> <code>[]</code> List of prerequisite job IDs <code>strategy</code> <code>Strategy</code> <code>Strategy()</code> Matrix strategy for parameterized execution <code>extras</code> <code>dict</code> <code>{}</code> Additional configuration parameters"},{"location":"api/job/#methods","title":"Methods","text":""},{"location":"api/job/#executeparams-run_idnone-parent_run_idnone-eventnone","title":"<code>execute(params, *, run_id=None, parent_run_id=None, event=None)</code>","text":"<p>Execute the job with all its stages.</p> <p>Parameters: - <code>params</code> (dict): Parameter values for job execution - <code>run_id</code> (str, optional): Unique identifier for this execution run - <code>parent_run_id</code> (str, optional): Parent workflow run identifier - <code>event</code> (Event, optional): Threading event for cancellation control</p> <p>Returns: - <code>Result</code>: Job execution result with status and output data</p>"},{"location":"api/job/#check_needsjobs","title":"<code>check_needs(jobs)</code>","text":"<p>Check if job dependencies are satisfied.</p> <p>Parameters: - <code>jobs</code> (dict): Dictionary of job results by job ID</p> <p>Returns: - <code>Status</code>: Dependency check result (SUCCESS, SKIP, or WAIT)</p>"},{"location":"api/job/#is_skippedparams","title":"<code>is_skipped(params)</code>","text":"<p>Check if job should be skipped based on condition.</p> <p>Parameters: - <code>params</code> (dict): Current parameter context</p> <p>Returns: - <code>bool</code>: True if job should be skipped</p>"},{"location":"api/job/#strategy","title":"Strategy","text":"<p>Matrix strategy model for parameterized job execution.</p> <p>The Strategy model generates combinations of matrix values to enable parallel execution of jobs with different parameter sets. It supports cross-product generation, inclusion of specific combinations, and exclusion of unwanted combinations.</p>"},{"location":"api/job/#attributes_1","title":"Attributes","text":"Attribute Type Default Description <code>fail_fast</code> <code>bool</code> <code>False</code> Cancel remaining executions on first failure <code>max_parallel</code> <code>int</code> <code>1</code> Maximum concurrent executions (1-9) <code>matrix</code> <code>dict</code> <code>{}</code> Base matrix values for cross-product generation <code>include</code> <code>list[dict]</code> <code>[]</code> Additional specific combinations to include <code>exclude</code> <code>list[dict]</code> <code>[]</code> Specific combinations to exclude from results"},{"location":"api/job/#methods_1","title":"Methods","text":""},{"location":"api/job/#make","title":"<code>make()</code>","text":"<p>Generate list of parameter combinations from matrix.</p> <p>Returns: - <code>list[dict]</code>: List of parameter dictionaries for execution</p>"},{"location":"api/job/#is_set","title":"<code>is_set()</code>","text":"<p>Check if strategy matrix is configured.</p> <p>Returns: - <code>bool</code>: True if matrix has been defined</p>"},{"location":"api/job/#rule","title":"Rule","text":"<p>Trigger rules for job dependency evaluation.</p>"},{"location":"api/job/#values","title":"Values","text":"<ul> <li><code>ALL_SUCCESS</code>: All prerequisite jobs must succeed</li> <li><code>ALL_FAILED</code>: All prerequisite jobs must fail</li> <li><code>ALL_DONE</code>: All prerequisite jobs must complete (success or failure)</li> <li><code>ONE_SUCCESS</code>: At least one prerequisite job must succeed</li> <li><code>ONE_FAILED</code>: At least one prerequisite job must fail</li> <li><code>NONE_FAILED</code>: No prerequisite jobs should fail</li> <li><code>NONE_SKIPPED</code>: No prerequisite jobs should be skipped</li> </ul>"},{"location":"api/job/#runson","title":"RunsOn","text":"<p>Execution environment enumeration.</p>"},{"location":"api/job/#values_1","title":"Values","text":"<ul> <li><code>LOCAL</code>: Execute on local machine</li> <li><code>SELF_HOSTED</code>: Execute on remote self-hosted runner</li> <li><code>DOCKER</code>: Execute in Docker container</li> <li><code>AZ_BATCH</code>: Execute on Azure Batch</li> <li><code>AWS_BATCH</code>: Execute on AWS Batch</li> <li><code>CLOUD_BATCH</code>: Execute on Cloud Batch</li> </ul>"},{"location":"api/job/#execution-environment-models","title":"Execution Environment Models","text":""},{"location":"api/job/#onlocal","title":"OnLocal","text":"<p>Local execution environment.</p> <pre><code>runs_on = OnLocal()  # Default local execution\n</code></pre>"},{"location":"api/job/#onselfhosted","title":"OnSelfHosted","text":"<p>Self-hosted remote execution environment.</p> <p>Configuration: <pre><code>runs_on = OnSelfHosted(\n    args=SelfHostedArgs(\n        host=\"https://runner.example.com\",\n        token=\"your-api-token\"\n    )\n)\n</code></pre></p>"},{"location":"api/job/#ondocker","title":"OnDocker","text":"<p>Docker container execution environment.</p> <p>Configuration: <pre><code>runs_on = OnDocker(\n    args=DockerArgs(\n        image=\"python:3.11-slim\",\n        env={\"ENV\": \"production\"},\n        volume={\"/local/path\": \"/container/path\"}\n    )\n)\n</code></pre></p>"},{"location":"api/job/#onazbatch","title":"OnAzBatch","text":"<p>Azure Batch execution environment.</p> <p>Configuration: <pre><code>runs_on = OnAzBatch(\n    args=AzBatchArgs(\n        batch_account_name=\"mybatch\",\n        batch_account_key=\"key\",\n        batch_account_url=\"https://mybatch.region.batch.azure.com\",\n        storage_account_name=\"mystorage\",\n        storage_account_key=\"storagekey\"\n    )\n)\n</code></pre></p>"},{"location":"api/job/#usage-examples","title":"Usage Examples","text":""},{"location":"api/job/#basic-job-configuration","title":"Basic Job Configuration","text":"<pre><code>from ddeutil.workflow import Job, EmptyStage, PyStage\n\njob = Job(\n    id=\"data-processing\",\n    desc=\"Process daily data files\",\n    stages=[\n        EmptyStage(name=\"Start\", echo=\"Processing started\"),\n        PyStage(\n            name=\"Process\",\n            run=\"result = process_data(input_file)\",\n            vars={\"input_file\": \"/data/input.csv\"}\n        ),\n        EmptyStage(name=\"Complete\", echo=\"Processing finished\")\n    ]\n)\n</code></pre>"},{"location":"api/job/#job-with-matrix-strategy","title":"Job with Matrix Strategy","text":"<pre><code>from ddeutil.workflow import Job, Strategy, BashStage\n\njob = Job(\n    id=\"multi-env-deploy\",\n    strategy=Strategy(\n        matrix={\n            'environment': ['dev', 'staging', 'prod'],\n            'region': ['us-east', 'eu-west']\n        },\n        max_parallel=3,\n        fail_fast=True,\n        exclude=[{'environment': 'dev', 'region': 'eu-west'}]\n    ),\n    stages=[\n        BashStage(\n            name=\"Deploy to ${{ matrix.environment }}-${{ matrix.region }}\",\n            bash=\"kubectl apply -f deploy.yaml\",\n            env={\n                'ENVIRONMENT': '${{ matrix.environment }}',\n                'REGION': '${{ matrix.region }}'\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#job-with-dependencies-and-conditional-execution","title":"Job with Dependencies and Conditional Execution","text":"<pre><code>from ddeutil.workflow import Job, Rule\n\n# Setup job\nsetup_job = Job(\n    id=\"setup\",\n    stages=[\n        BashStage(name=\"Create directories\", bash=\"mkdir -p /tmp/workspace\")\n    ]\n)\n\n# Processing job that depends on setup\nprocessing_job = Job(\n    id=\"process\",\n    needs=[\"setup\"],\n    trigger_rule=Rule.ALL_SUCCESS,\n    condition=\"${{ params.enable_processing == true }}\",\n    stages=[\n        PyStage(\n            name=\"Process Data\",\n            run=\"process_data()\"\n        )\n    ]\n)\n\n# Cleanup job that runs regardless of previous job status\ncleanup_job = Job(\n    id=\"cleanup\",\n    needs=[\"setup\", \"process\"],\n    trigger_rule=Rule.ALL_DONE,\n    stages=[\n        BashStage(name=\"Cleanup\", bash=\"rm -rf /tmp/workspace\")\n    ]\n)\n</code></pre>"},{"location":"api/job/#advanced-matrix-strategies","title":"Advanced Matrix Strategies","text":""},{"location":"api/job/#complex-matrix-with-exclusions","title":"Complex Matrix with Exclusions","text":"<pre><code>from ddeutil.workflow import Job, Strategy\n\njob = Job(\n    id=\"comprehensive-testing\",\n    strategy=Strategy(\n        matrix={\n            'python_version': ['3.9', '3.10', '3.11'],\n            'platform': ['linux', 'windows', 'macos'],\n            'database': ['postgresql', 'mysql', 'sqlite']\n        },\n        include=[\n            # Additional specific combinations\n            {\n                'python_version': '3.12',\n                'platform': 'linux',\n                'database': 'postgresql'\n            }\n        ],\n        exclude=[\n            # Exclude incompatible combinations\n            {'platform': 'windows', 'database': 'sqlite'},\n            {'platform': 'macos', 'python_version': '3.9'}\n        ],\n        max_parallel=5,\n        fail_fast=False\n    ),\n    stages=[\n        PyStage(\n            name=\"Test ${{ matrix.python_version }} on ${{ matrix.platform }} with ${{ matrix.database }}\",\n            run=\"\"\"\nimport sys\nprint(f\"Python: {sys.version}\")\nprint(f\"Platform: {sys.platform}\")\nprint(f\"Database: {params.database}\")\nrun_tests()\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#dynamic-matrix-generation","title":"Dynamic Matrix Generation","text":"<pre><code>from ddeutil.workflow import Job, Strategy, PyStage\n\njob = Job(\n    id=\"dynamic-matrix\",\n    stages=[\n        PyStage(\n            name=\"Generate Matrix\",\n            run=\"\"\"\n# Generate matrix based on available resources\navailable_regions = get_available_regions()\navailable_instances = get_available_instances()\n\nmatrix_data = {\n    'region': available_regions,\n    'instance_type': available_instances\n}\n\nresult.outputs = {\"matrix\": matrix_data}\n\"\"\"\n        )\n    ],\n    strategy=Strategy(\n        matrix=\"${{ fromJson(needs.generate-matrix.outputs.matrix) }}\",\n        max_parallel=3\n    )\n)\n</code></pre>"},{"location":"api/job/#job-orchestration-patterns","title":"Job Orchestration Patterns","text":""},{"location":"api/job/#fan-outfan-in-pattern","title":"Fan-Out/Fan-In Pattern","text":"<pre><code>from ddeutil.workflow import Job, Rule\n\n# Split job\nsplit_job = Job(\n    id=\"split-data\",\n    stages=[\n        PyStage(\n            name=\"Split Data\",\n            run=\"\"\"\npartitions = split_large_dataset()\nresult.outputs = {\"partitions\": partitions}\n\"\"\"\n        )\n    ]\n)\n\n# Process partitions in parallel\nprocess_job = Job(\n    id=\"process-partitions\",\n    needs=[\"split-data\"],\n    strategy=Strategy(\n        matrix={\n            'partition': \"${{ fromJson(needs.split-data.outputs.partitions) }}\"\n        },\n        max_parallel=4\n    ),\n    stages=[\n        PyStage(\n            name=\"Process Partition ${{ matrix.partition }}\",\n            run=\"process_partition(${{ matrix.partition }})\"\n        )\n    ]\n)\n\n# Merge results\nmerge_job = Job(\n    id=\"merge-results\",\n    needs=[\"process-partitions\"],\n    trigger_rule=Rule.ALL_SUCCESS,\n    stages=[\n        PyStage(\n            name=\"Merge Results\",\n            run=\"merge_all_partition_results()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>from ddeutil.workflow import Job, Rule\n\n# Health check job\nhealth_job = Job(\n    id=\"health-check\",\n    stages=[\n        PyStage(\n            name=\"Check System Health\",\n            run=\"\"\"\nhealth_status = check_system_health()\nif not health_status.is_healthy:\n    raise Exception(\"System unhealthy\")\n\"\"\"\n        )\n    ]\n)\n\n# Main processing job\nmain_job = Job(\n    id=\"main-process\",\n    needs=[\"health-check\"],\n    trigger_rule=Rule.ALL_SUCCESS,\n    stages=[\n        PyStage(\n            name=\"Main Processing\",\n            run=\"process_data()\"\n        )\n    ]\n)\n\n# Fallback job\nfallback_job = Job(\n    id=\"fallback\",\n    needs=[\"health-check\"],\n    trigger_rule=Rule.ALL_FAILED,\n    stages=[\n        PyStage(\n            name=\"Fallback Processing\",\n            run=\"fallback_processing()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#retry-with-exponential-backoff","title":"Retry with Exponential Backoff","text":"<pre><code>from ddeutil.workflow import Job, PyStage\n\njob = Job(\n    id=\"retry-with-backoff\",\n    stages=[\n        PyStage(\n            name=\"Retry Operation\",\n            retry=5,\n            run=\"\"\"\nimport time\n\nattempt = context.get('attempt', 1)\ndelay = 2 ** (attempt - 1)  # Exponential backoff: 1, 2, 4, 8, 16 seconds\ntime.sleep(delay)\n\n# Attempt the operation\nresult = risky_operation()\nif not result.success:\n    raise Exception(f\"Operation failed on attempt {attempt}\")\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#multi-environment-deployment","title":"Multi-Environment Deployment","text":""},{"location":"api/job/#environment-specific-jobs","title":"Environment-Specific Jobs","text":"<pre><code>from ddeutil.workflow import Job, Strategy\n\ndeploy_job = Job(\n    id=\"deploy\",\n    strategy=Strategy(\n        matrix={\n            'environment': ['dev', 'staging', 'prod']\n        }\n    ),\n    stages=[\n        PyStage(\n            name=\"Deploy to ${{ matrix.environment }}\",\n            run=\"\"\"\nenv = matrix['environment']\nconfig = get_environment_config(env)\n\nif env == 'prod':\n    # Additional safety checks for production\n    validate_production_deployment()\n    notify_team(\"Production deployment starting\")\n\ndeploy_application(config)\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#docker-based-execution","title":"Docker-Based Execution","text":"<pre><code>from ddeutil.workflow import Job, OnDocker, DockerArgs\n\ndocker_job = Job(\n    id=\"docker-process\",\n    runs_on=OnDocker(\n        args=DockerArgs(\n            image=\"python:3.11-slim\",\n            env={\n                \"PYTHONPATH\": \"/app\",\n                \"DATABASE_URL\": \"${{ params.database_url }}\"\n            },\n            volume={\n                \"/local/data\": \"/app/data\",\n                \"/local/output\": \"/app/output\"\n            }\n        )\n    ),\n    stages=[\n        PyStage(\n            name=\"Docker Processing\",\n            run=\"\"\"\nimport os\nprint(f\"Running in container: {os.uname()}\")\nprint(f\"Data directory: {os.listdir('/app/data')}\")\n\n# Process data\nprocess_data('/app/data', '/app/output')\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#conditional-job-execution","title":"Conditional Job Execution","text":""},{"location":"api/job/#parameter-based-conditions","title":"Parameter-Based Conditions","text":"<pre><code>from ddeutil.workflow import Job\n\njob = Job(\n    id=\"conditional-job\",\n    condition=\"${{ params.environment == 'production' &amp;&amp; params.enable_feature == true }}\",\n    stages=[\n        PyStage(\n            name=\"Production Feature\",\n            run=\"enable_production_feature()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#time-based-conditions","title":"Time-Based Conditions","text":"<pre><code>from ddeutil.workflow import Job\n\njob = Job(\n    id=\"time-based-job\",\n    condition=\"${{ datetime.now().hour &gt;= 9 &amp;&amp; datetime.now().hour &lt;= 17 }}\",\n    stages=[\n        PyStage(\n            name=\"Business Hours Task\",\n            run=\"business_hours_task()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#dependency-based-conditions","title":"Dependency-Based Conditions","text":"<pre><code>from ddeutil.workflow import Job\n\njob = Job(\n    id=\"dependent-job\",\n    needs=[\"predecessor-job\"],\n    condition=\"${{ needs.predecessor-job.result == 'success' }}\",\n    stages=[\n        PyStage(\n            name=\"Dependent Task\",\n            run=\"dependent_task()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#job-lifecycle-management","title":"Job Lifecycle Management","text":""},{"location":"api/job/#job-state-transitions","title":"Job State Transitions","text":"<pre><code>from ddeutil.workflow import Job, SUCCESS, FAILED, SKIP, WAIT\n\ndef monitor_job_lifecycle(job: Job, params: dict):\n    \"\"\"Monitor job execution lifecycle.\"\"\"\n\n    # Check if job should be skipped\n    if job.is_skipped(params):\n        print(f\"Job {job.id} will be skipped\")\n        return SKIP\n\n    # Check dependencies\n    dependency_status = job.check_needs(previous_job_results)\n    if dependency_status == WAIT:\n        print(f\"Job {job.id} waiting for dependencies\")\n        return WAIT\n\n    # Execute job\n    try:\n        result = job.execute(params)\n        print(f\"Job {job.id} completed with status: {result.status}\")\n        return result.status\n    except Exception as e:\n        print(f\"Job {job.id} failed: {e}\")\n        return FAILED\n</code></pre>"},{"location":"api/job/#job-result-aggregation","title":"Job Result Aggregation","text":"<pre><code>from ddeutil.workflow import Job, Result\n\ndef aggregate_job_results(jobs: dict[str, Job], results: dict[str, Result]) -&gt; dict:\n    \"\"\"Aggregate results from multiple jobs.\"\"\"\n\n    aggregated = {\n        'total_jobs': len(jobs),\n        'successful_jobs': 0,\n        'failed_jobs': 0,\n        'skipped_jobs': 0,\n        'job_details': {}\n    }\n\n    for job_id, result in results.items():\n        aggregated['job_details'][job_id] = {\n            'status': result.status,\n            'execution_time': result.context.get('execution_time'),\n            'outputs': result.context.get('outputs', {})\n        }\n\n        if result.status == SUCCESS:\n            aggregated['successful_jobs'] += 1\n        elif result.status == FAILED:\n            aggregated['failed_jobs'] += 1\n        elif result.status == SKIP:\n            aggregated['skipped_jobs'] += 1\n\n    return aggregated\n</code></pre>"},{"location":"api/job/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/job/#parallel-execution-strategies","title":"Parallel Execution Strategies","text":"<pre><code>from ddeutil.workflow import Job, Strategy\n\n# Optimize for CPU-bound tasks\ncpu_intensive_job = Job(\n    id=\"cpu-intensive\",\n    strategy=Strategy(\n        matrix={'task_id': range(100)},\n        max_parallel=os.cpu_count()  # Use all CPU cores\n    )\n)\n\n# Optimize for I/O-bound tasks\nio_intensive_job = Job(\n    id=\"io-intensive\",\n    strategy=Strategy(\n        matrix={'file_id': range(50)},\n        max_parallel=20  # Higher parallelism for I/O\n    )\n)\n\n# Resource-aware execution\nresource_aware_job = Job(\n    id=\"resource-aware\",\n    strategy=Strategy(\n        matrix={'region': ['us-east', 'us-west', 'eu-west']},\n        max_parallel=min(3, available_resources)  # Limit based on resources\n    )\n)\n</code></pre>"},{"location":"api/job/#caching-and-optimization","title":"Caching and Optimization","text":"<pre><code>from ddeutil.workflow import Job, PyStage\n\ncached_job = Job(\n    id=\"cached-computation\",\n    stages=[\n        PyStage(\n            name=\"Check Cache\",\n            run=\"\"\"\ncache_key = generate_cache_key(params)\nif cache_exists(cache_key):\n    result = load_from_cache(cache_key)\n    print(\"Using cached result\")\nelse:\n    result = expensive_computation()\n    save_to_cache(cache_key, result)\n    print(\"Computed and cached result\")\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"api/job/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>from ddeutil.workflow import Job, PyStage, Rule\n\nrobust_job = Job(\n    id=\"robust-processing\",\n    stages=[\n        PyStage(\n            name=\"Primary Processing\",\n            retry=3,\n            run=\"\"\"\ntry:\n    result = primary_processing()\nexcept Exception as e:\n    logger.error(f\"Primary processing failed: {e}\")\n    raise\n\"\"\"\n        )\n    ]\n)\n\n# Fallback job\nfallback_job = Job(\n    id=\"fallback-processing\",\n    needs=[\"robust-processing\"],\n    trigger_rule=Rule.ALL_FAILED,\n    stages=[\n        PyStage(\n            name=\"Fallback Processing\",\n            run=\"fallback_processing()\"\n        )\n    ]\n)\n\n# Cleanup job\ncleanup_job = Job(\n    id=\"cleanup\",\n    needs=[\"robust-processing\", \"fallback-processing\"],\n    trigger_rule=Rule.ALL_DONE,\n    stages=[\n        PyStage(\n            name=\"Cleanup\",\n            run=\"cleanup_resources()\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from ddeutil.workflow import Job, PyStage\n\ndegraded_job = Job(\n    id=\"graceful-degradation\",\n    stages=[\n        PyStage(\n            name=\"Check System Resources\",\n            run=\"\"\"\nresources = check_system_resources()\nif resources.memory &lt; 1024:  # Less than 1GB\n    print(\"Low memory detected, using degraded mode\")\n    params['degraded_mode'] = True\n\"\"\"\n        ),\n        PyStage(\n            name=\"Adaptive Processing\",\n            run=\"\"\"\nif params.get('degraded_mode'):\n    process_with_limited_resources()\nelse:\n    process_with_full_resources()\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"api/job/#job-metrics-collection","title":"Job Metrics Collection","text":"<pre><code>from ddeutil.workflow import Job, PyStage\nimport time\n\nmonitored_job = Job(\n    id=\"monitored-job\",\n    stages=[\n        PyStage(\n            name=\"Collect Metrics\",\n            run=\"\"\"\nimport time\nimport psutil\n\nstart_time = time.time()\nstart_memory = psutil.virtual_memory().used\n\n# Your processing logic here\nprocess_data()\n\nend_time = time.time()\nend_memory = psutil.virtual_memory().used\n\nmetrics = {\n    'execution_time': end_time - start_time,\n    'memory_usage': end_memory - start_memory,\n    'cpu_usage': psutil.cpu_percent()\n}\n\nresult.outputs = {\"metrics\": metrics}\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#job-health-checks","title":"Job Health Checks","text":"<pre><code>from ddeutil.workflow import Job, PyStage\n\nhealth_check_job = Job(\n    id=\"health-check\",\n    stages=[\n        PyStage(\n            name=\"System Health Check\",\n            run=\"\"\"\nhealth_status = {\n    'database': check_database_health(),\n    'api': check_api_health(),\n    'storage': check_storage_health(),\n    'network': check_network_health()\n}\n\nall_healthy = all(health_status.values())\nif not all_healthy:\n    unhealthy_services = [k for k, v in health_status.items() if not v]\n    raise Exception(f\"Unhealthy services: {unhealthy_services}\")\n\nresult.outputs = {\"health_status\": health_status}\n\"\"\"\n        )\n    ]\n)\n</code></pre>"},{"location":"api/job/#best-practices","title":"Best Practices","text":""},{"location":"api/job/#1-job-design","title":"1. Job Design","text":"<ul> <li>Single responsibility: Each job should have a clear, focused purpose</li> <li>Idempotency: Jobs should be safe to retry without side effects</li> <li>Modularity: Break complex jobs into smaller, manageable stages</li> <li>Reusability: Design jobs to be reusable across different workflows</li> </ul>"},{"location":"api/job/#2-matrix-strategies","title":"2. Matrix Strategies","text":"<ul> <li>Resource awareness: Set <code>max_parallel</code> based on available resources</li> <li>Failure handling: Use <code>fail_fast</code> appropriately for your use case</li> <li>Exclusion logic: Carefully design exclusion rules to avoid conflicts</li> <li>Performance: Balance parallelism with resource constraints</li> </ul>"},{"location":"api/job/#3-dependencies","title":"3. Dependencies","text":"<ul> <li>Clear dependencies: Explicitly define job dependencies</li> <li>Trigger rules: Choose appropriate trigger rules for your use case</li> <li>Circular dependencies: Avoid circular dependency patterns</li> <li>Failure propagation: Understand how failures propagate through dependencies</li> </ul>"},{"location":"api/job/#4-error-handling","title":"4. Error Handling","text":"<ul> <li>Retry logic: Implement appropriate retry strategies</li> <li>Fallback mechanisms: Provide fallback options for critical jobs</li> <li>Graceful degradation: Handle resource constraints gracefully</li> <li>Monitoring: Monitor job execution and failure patterns</li> </ul>"},{"location":"api/job/#5-performance","title":"5. Performance","text":"<ul> <li>Parallelization: Use matrix strategies for parallel execution</li> <li>Resource optimization: Optimize resource usage based on job type</li> <li>Caching: Implement caching for expensive operations</li> <li>Monitoring: Track performance metrics and optimize accordingly</li> </ul>"},{"location":"api/job/#6-security","title":"6. Security","text":"<ul> <li>Input validation: Validate all inputs to jobs</li> <li>Access control: Implement proper access controls</li> <li>Secret management: Handle secrets securely</li> <li>Audit logging: Enable audit logging for compliance</li> </ul>"},{"location":"api/job/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/job/#common-issues","title":"Common Issues","text":""},{"location":"api/job/#job-dependencies-not-met","title":"Job Dependencies Not Met","text":"<pre><code># Problem: Job waiting indefinitely for dependencies\njob_results = {\n    'job-a': Result(status=SUCCESS),\n    'job-b': Result(status=FAILED)\n}\n\n# Check dependency status\nfor job_id, job in workflow.jobs.items():\n    if job.needs:\n        status = job.check_needs(job_results)\n        print(f\"Job {job_id} dependency status: {status}\")\n</code></pre>"},{"location":"api/job/#matrix-strategy-issues","title":"Matrix Strategy Issues","text":"<pre><code># Problem: Matrix combinations not generating as expected\nstrategy = Strategy(\n    matrix={\n        'env': ['dev', 'prod'],\n        'region': ['us-east', 'eu-west']\n    },\n    exclude=[{'env': 'dev', 'region': 'eu-west'}]\n)\n\n# Generate and inspect combinations\ncombinations = strategy.make()\nprint(f\"Generated {len(combinations)} combinations:\")\nfor combo in combinations:\n    print(f\"  {combo}\")\n</code></pre>"},{"location":"api/job/#resource-exhaustion","title":"Resource Exhaustion","text":"<pre><code># Problem: Too many parallel executions causing resource issues\n# Solution: Monitor and adjust max_parallel\nimport psutil\n\navailable_memory = psutil.virtual_memory().available\navailable_cpu = psutil.cpu_count()\n\n# Adjust max_parallel based on available resources\nmax_parallel = min(\n    available_cpu,\n    available_memory // (1024 * 1024 * 512),  # 512MB per job\n    10  # Maximum limit\n)\n\njob = Job(\n    id=\"resource-aware\",\n    strategy=Strategy(\n        matrix={'task_id': range(100)},\n        max_parallel=max_parallel\n    )\n)\n</code></pre>"},{"location":"api/job/#conditional-execution-issues","title":"Conditional Execution Issues","text":"<pre><code># Problem: Job not executing when expected\n# Solution: Debug condition evaluation\njob = Job(\n    id=\"conditional-job\",\n    condition=\"${{ params.environment == 'production' }}\"\n)\n\n# Test condition evaluation\ntest_params = {'environment': 'production'}\nis_skipped = job.is_skipped(test_params)\nprint(f\"Job would be skipped: {is_skipped}\")\n\n# Check parameter values\nprint(f\"Environment parameter: {test_params.get('environment')}\")\n</code></pre>"},{"location":"api/job/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Enable verbose logging: Set log level to DEBUG for detailed execution information</li> <li>Check job dependencies: Verify that all required jobs have completed successfully</li> <li>Validate matrix combinations: Inspect generated matrix combinations for correctness</li> <li>Monitor resource usage: Track CPU, memory, and I/O usage during execution</li> <li>Test incrementally: Test individual stages before running full jobs</li> <li>Use conditional execution: Add debug stages that only run in development</li> </ol>"},{"location":"api/job/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/job/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>WORKFLOW_CORE_JOB_TIMEOUT</code> <code>3600</code> Default job timeout in seconds <code>WORKFLOW_CORE_MAX_PARALLEL</code> <code>2</code> Default max parallel executions <code>WORKFLOW_CORE_RETRY_DELAY</code> <code>5</code> Default retry delay in seconds <code>WORKFLOW_CORE_RETRY_ATTEMPTS</code> <code>3</code> Default retry attempts"},{"location":"api/job/#job-configuration-schema","title":"Job Configuration Schema","text":"<pre><code>job-name:\n  id: \"unique-job-id\"\n  desc: \"Job description\"\n  runs-on:\n    type: \"local\" | \"self-hosted\" | \"docker\" | \"az-batch\" | \"aws-batch\" | \"cloud-batch\"\n    # Additional environment-specific configuration\n  condition: \"${{ expression }}\"\n  needs: [\"job1\", \"job2\"]\n  trigger-rule: \"all_success\" | \"all_failed\" | \"all_done\" | \"one_success\" | \"one_failed\" | \"none_failed\" | \"none_skipped\"\n  strategy:\n    matrix:\n      key1: [value1, value2]\n      key2: [value3, value4]\n    include:\n      - key1: value5\n        key2: value6\n    exclude:\n      - key1: value1\n        key2: value3\n    max-parallel: 3\n    fail-fast: false\n  stages:\n    - name: \"Stage Name\"\n      # Stage-specific configuration\n</code></pre>"},{"location":"api/params/","title":"Parameters","text":"<p>The Parameters module provides a comprehensive type system for workflow parameter definitions, validation, and processing. All parameter types inherit from a base <code>Param</code> type and support various data types with validation rules.</p>"},{"location":"api/params/#overview","title":"Overview","text":"<p>The parameter system provides:</p> <ul> <li>Type validation: Strong typing with Pydantic validation</li> <li>Default values: Optional default values for parameters</li> <li>Description support: Human-readable parameter descriptions</li> <li>Nested structures: Support for complex data types</li> <li>Template integration: Seamless integration with workflow templating</li> </ul>"},{"location":"api/params/#base-parameter-type","title":"Base Parameter Type","text":"<p>The <code>Param</code> type is constructed as:</p> <pre><code>Param = Annotated[\n    Union[\n        MapParam,\n        ArrayParam,\n        ChoiceParam,\n        DatetimeParam,\n        DateParam,\n        IntParam,\n        StrParam,\n    ],\n    Field(discriminator=\"type\"),\n]\n</code></pre>"},{"location":"api/params/#parameter-types","title":"Parameter Types","text":""},{"location":"api/params/#strparam","title":"StrParam","text":"<p>String parameter type for text values.</p> <p>String Parameter</p> Basic StringString with DefaultRequired String <pre><code>params:\n    name:\n        type: str\n        description: \"User's full name\"\n</code></pre> <pre><code>params:\n    environment:\n        type: str\n        default: \"development\"\n        description: \"Target environment\"\n</code></pre> <pre><code>params:\n    api_key:\n        type: str\n        description: \"API authentication key\"\n        required: true\n</code></pre>"},{"location":"api/params/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"str\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> str | None <code>None</code> Default value if not provided <code>required</code> bool <code>False</code> Whether parameter is required"},{"location":"api/params/#intparam","title":"IntParam","text":"<p>Integer parameter type for numeric values.</p> <p>Integer Parameter</p> Basic IntegerInteger with DefaultInteger with Constraints <pre><code>params:\n    batch_size:\n        type: int\n        description: \"Number of records to process\"\n</code></pre> <pre><code>params:\n    timeout:\n        type: int\n        default: 300\n        description: \"Operation timeout in seconds\"\n</code></pre> <pre><code>params:\n    port:\n        type: int\n        default: 8080\n        description: \"Server port number\"\n        ge: 1024\n        le: 65535\n</code></pre>"},{"location":"api/params/#attributes_1","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"int\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> int | None <code>None</code> Default value if not provided <code>ge</code> int | None <code>None</code> Greater than or equal constraint <code>le</code> int | None <code>None</code> Less than or equal constraint <code>gt</code> int | None <code>None</code> Greater than constraint <code>lt</code> int | None <code>None</code> Less than constraint"},{"location":"api/params/#datetimeparam","title":"DatetimeParam","text":"<p>Datetime parameter type for date and time values.</p> <p>Datetime Parameter</p> Basic DatetimeDatetime with DefaultDatetime with Format <pre><code>params:\n    start_time:\n        type: datetime\n        description: \"Workflow start time\"\n</code></pre> <pre><code>params:\n    scheduled_time:\n        type: datetime\n        default: \"2024-01-01T00:00:00\"\n        description: \"Scheduled execution time\"\n</code></pre> <pre><code>params:\n    event_time:\n        type: datetime\n        description: \"Event timestamp\"\n        format: \"%Y-%m-%d %H:%M:%S\"\n</code></pre>"},{"location":"api/params/#attributes_2","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"datetime\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> str | None <code>None</code> Default value in ISO format <code>format</code> str | None <code>None</code> Custom datetime format string"},{"location":"api/params/#dateparam","title":"DateParam","text":"<p>Date parameter type for date-only values.</p> <p>Date Parameter</p> Basic DateDate with Default <pre><code>params:\n    processing_date:\n        type: date\n        description: \"Data processing date\"\n</code></pre> <pre><code>params:\n    report_date:\n        type: date\n        default: \"2024-01-01\"\n        description: \"Report generation date\"\n</code></pre>"},{"location":"api/params/#attributes_3","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"date\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> str | None <code>None</code> Default value in YYYY-MM-DD format"},{"location":"api/params/#choiceparam","title":"ChoiceParam","text":"<p>Choice parameter type for enumerated values.</p> <p>Choice Parameter</p> Basic ChoiceChoice with Default <pre><code>params:\n    environment:\n        type: choice\n        choices: [\"development\", \"staging\", \"production\"]\n        description: \"Target environment\"\n</code></pre> <pre><code>params:\n    mode:\n        type: choice\n        choices: [\"batch\", \"stream\", \"interactive\"]\n        default: \"batch\"\n        description: \"Processing mode\"\n</code></pre>"},{"location":"api/params/#attributes_4","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"choice\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>choices</code> list[str] <code>[]</code> Available choice options <code>default</code> str | None <code>None</code> Default selected choice"},{"location":"api/params/#mapparam","title":"MapParam","text":"<p>Map parameter type for key-value structures.</p> <p>Map Parameter</p> Basic MapMap with Default <pre><code>params:\n    config:\n        type: map\n        description: \"Configuration settings\"\n</code></pre> <pre><code>params:\n    headers:\n        type: map\n        default:\n            \"Content-Type\": \"application/json\"\n            \"User-Agent\": \"Workflow/1.0\"\n        description: \"HTTP request headers\"\n</code></pre>"},{"location":"api/params/#attributes_5","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"map\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> dict <code>{}</code> Default key-value pairs"},{"location":"api/params/#arrayparam","title":"ArrayParam","text":"<p>Array parameter type for list values.</p> <p>Array Parameter</p> Basic ArrayArray with DefaultTyped Array <pre><code>params:\n    files:\n        type: array\n        description: \"List of files to process\"\n</code></pre> <pre><code>params:\n    regions:\n        type: array\n        default: [\"us-east\", \"us-west\"]\n        description: \"Target regions\"\n</code></pre> <pre><code>params:\n    numbers:\n        type: array\n        items_type: int\n        description: \"List of numeric values\"\n</code></pre>"},{"location":"api/params/#attributes_6","title":"Attributes","text":"Attribute Type Default Description <code>type</code> str <code>\"array\"</code> Parameter type identifier <code>description</code> str | None <code>None</code> Human-readable description <code>default</code> list <code>[]</code> Default array values <code>items_type</code> str | None <code>None</code> Type of array items"},{"location":"api/params/#usage-examples","title":"Usage Examples","text":""},{"location":"api/params/#workflow-parameter-definition","title":"Workflow Parameter Definition","text":"<pre><code>workflow:\n  type: Workflow\n  params:\n    # String parameters\n    data_source:\n      type: str\n      description: \"Source data location\"\n      required: true\n\n    environment:\n      type: str\n      default: \"development\"\n      description: \"Target environment\"\n\n    # Numeric parameters\n    batch_size:\n      type: int\n      default: 1000\n      description: \"Processing batch size\"\n      ge: 1\n      le: 10000\n\n    # Date/time parameters\n    start_date:\n      type: date\n      description: \"Processing start date\"\n\n    scheduled_time:\n      type: datetime\n      default: \"2024-01-01T09:00:00\"\n      description: \"Scheduled execution time\"\n\n    # Choice parameters\n    mode:\n      type: choice\n      choices: [\"full\", \"incremental\", \"test\"]\n      default: \"incremental\"\n      description: \"Processing mode\"\n\n    # Complex parameters\n    config:\n      type: map\n      default:\n        timeout: 300\n        retries: 3\n      description: \"Processing configuration\"\n\n    file_list:\n      type: array\n      description: \"Files to process\"\n</code></pre>"},{"location":"api/params/#parameter-validation","title":"Parameter Validation","text":"<pre><code>from ddeutil.workflow.params import StrParam, IntParam, ChoiceParam\n\n# String parameter with validation\nname_param = StrParam(\n    description=\"User name\",\n    required=True\n)\n\n# Integer parameter with constraints\nage_param = IntParam(\n    description=\"User age\",\n    default=18,\n    ge=0,\n    le=120\n)\n\n# Choice parameter\nstatus_param = ChoiceParam(\n    description=\"User status\",\n    choices=[\"active\", \"inactive\", \"pending\"],\n    default=\"pending\"\n)\n</code></pre>"},{"location":"api/params/#template-integration","title":"Template Integration","text":"<p>Parameters integrate seamlessly with workflow templating:</p> <pre><code>stages:\n  - name: \"Process data for ${{ params.environment }}\"\n    run: |\n      process_data(\n          source=\"${{ params.data_source }}\",\n          batch_size=${{ params.batch_size }},\n          mode=\"${{ params.mode }}\"\n      )\n\n  - name: \"Generate report\"\n    if: \"${{ params.mode }} == 'full'\"\n    run: |\n      generate_report(\n          date=\"${{ params.start_date }}\",\n          config=${{ params.config }}\n      )\n</code></pre>"},{"location":"api/params/#best-practices","title":"Best Practices","text":""},{"location":"api/params/#1-parameter-naming","title":"1. Parameter Naming","text":"<ul> <li>Use descriptive, lowercase names with underscores</li> <li>Avoid reserved words and special characters</li> <li>Be consistent with naming conventions</li> </ul>"},{"location":"api/params/#2-validation-rules","title":"2. Validation Rules","text":"<ul> <li>Set appropriate constraints for numeric parameters</li> <li>Use choice parameters for enumerated values</li> <li>Provide meaningful default values when possible</li> </ul>"},{"location":"api/params/#3-documentation","title":"3. Documentation","text":"<ul> <li>Always include descriptions for parameters</li> <li>Use clear, concise language</li> <li>Provide examples for complex parameters</li> </ul>"},{"location":"api/params/#4-type-safety","title":"4. Type Safety","text":"<ul> <li>Choose the most specific parameter type</li> <li>Use typed arrays when possible</li> <li>Leverage validation constraints</li> </ul>"},{"location":"api/params/#5-default-values","title":"5. Default Values","text":"<ul> <li>Provide sensible defaults for optional parameters</li> <li>Use environment-specific defaults</li> <li>Document the rationale for default choices</li> </ul>"},{"location":"api/result/","title":"Results","text":"<p>The Results module provides the core data structures for passing and receiving execution context throughout the workflow system. It defines status enumerations and result containers for stage, job, and workflow execution.</p>"},{"location":"api/result/#overview","title":"Overview","text":"<p>The result system provides:</p> <ul> <li>Status management: Comprehensive status enumeration for execution states</li> <li>Context containers: Structured data containers for execution results</li> <li>Hierarchical results: Support for nested execution contexts</li> <li>Error handling: Integrated error information and status mapping</li> </ul>"},{"location":"api/result/#status-enumeration","title":"Status Enumeration","text":""},{"location":"api/result/#status","title":"Status","text":"<p>Enumeration of possible execution statuses.</p>"},{"location":"api/result/#values","title":"Values","text":"Status Value Description <code>WAIT</code> <code>0</code> Execution is waiting to start <code>SUCCESS</code> <code>1</code> Execution completed successfully <code>FAILED</code> <code>2</code> Execution failed with errors <code>SKIP</code> <code>3</code> Execution was skipped <code>CANCEL</code> <code>4</code> Execution was cancelled <p>Status Usage</p> <pre><code>from ddeutil.workflow.result import Status, SUCCESS, FAILED\n\n# Using enum values\nif result.status == Status.SUCCESS:\n    print(\"Execution successful\")\n\n# Using constants\nif result.status == SUCCESS:\n    print(\"Execution successful\")\n\n# Status comparison\nif result.status &gt;= SUCCESS:\n    print(\"Execution completed\")\n</code></pre>"},{"location":"api/result/#result-model","title":"Result Model","text":""},{"location":"api/result/#result","title":"Result","text":"<p>Pydantic model for passing and receiving data context from any module execution process like stage execution, job execution, or workflow execution.</p> <p>For comparison property, this result will use <code>status</code>, <code>context</code>, and <code>run_id</code> fields to compare with other result instances.</p> <p>Result Creation</p> <pre><code>from ddeutil.workflow.result import Result, SUCCESS\n\n# Create successful result\nresult = Result(\n    status=SUCCESS,\n    context={\"output\": \"data processed\"},\n    run_id=\"workflow-123\"\n)\n\n# Create failed result\nfailed_result = Result(\n    status=FAILED,\n    context={\"error\": \"connection timeout\"},\n    run_id=\"workflow-124\"\n)\n</code></pre>"},{"location":"api/result/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>status</code> Status <code>WAIT</code> Execution status <code>context</code> DictData <code>{}</code> Execution context data <code>run_id</code> str | None <code>None</code> Unique execution identifier <code>parent_run_id</code> str | None <code>None</code> Parent execution identifier"},{"location":"api/result/#result-context-structure","title":"Result Context Structure","text":"<p>The <code>context</code> field contains structured data that varies based on the execution level:</p> <p>Context Structures</p> Workflow ContextJob ContextStage Context <pre><code>{\n  \"params\": {\n    \"input_file\": \"/data/input.csv\",\n    \"output_dir\": \"/data/output\"\n  },\n  \"jobs\": {\n    \"extract\": {\n      \"matrix\": {},\n      \"stages\": {\n        \"download\": {\"outputs\": {\"file_count\": 100}},\n        \"validate\": {\"outputs\": {\"valid_count\": 95}}\n      }\n    },\n    \"transform\": {\n      \"matrix\": {},\n      \"stages\": {\n        \"process\": {\"outputs\": {\"processed_count\": 95}}\n      }\n    }\n  },\n  \"error\": \"WorkflowError\",\n  \"error_message\": \"Job transform failed\"\n}\n</code></pre> <pre><code>{\n  \"matrix-001\": {\n    \"matrix\": {\"env\": \"prod\", \"region\": \"us-east\"},\n    \"stages\": {\n      \"setup\": {\"outputs\": {\"workspace\": \"/tmp/work\"}},\n      \"process\": {\"outputs\": {\"result\": \"success\"}}\n    }\n  },\n  \"matrix-002\": {\n    \"matrix\": {\"env\": \"prod\", \"region\": \"us-west\"},\n    \"stages\": {\n      \"setup\": {\"outputs\": {\"workspace\": \"/tmp/work\"}},\n      \"process\": {\"outputs\": {\"result\": \"success\"}}\n    }\n  }\n}\n</code></pre> <pre><code>{\n  \"stages\": {\n    \"data-processing\": {\n      \"outputs\": {\n        \"processed_rows\": 1000,\n        \"error_count\": 5\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"api/result/#utility-functions","title":"Utility Functions","text":""},{"location":"api/result/#get_status_from_error","title":"<code>get_status_from_error</code>","text":"<p>Convert exception to appropriate status value.</p> <p>Error to Status</p> <pre><code>from ddeutil.workflow.result import get_status_from_error, FAILED, CANCEL\n\ntry:\n    # Some operation that might fail\n    process_data()\nexcept Exception as e:\n    status = get_status_from_error(e)\n    # Returns FAILED for most exceptions\n    # Returns CANCEL for cancellation exceptions\n</code></pre>"},{"location":"api/result/#usage-examples","title":"Usage Examples","text":""},{"location":"api/result/#creating-results","title":"Creating Results","text":"<pre><code>from ddeutil.workflow.result import Result, SUCCESS, FAILED, SKIP\n\n# Successful execution\nsuccess_result = Result(\n    status=SUCCESS,\n    context={\n        \"outputs\": {\n            \"processed_files\": 10,\n            \"total_size\": \"1.5MB\"\n        }\n    },\n    run_id=\"job-123\"\n)\n\n# Failed execution\nfailed_result = Result(\n    status=FAILED,\n    context={\n        \"error\": \"Connection timeout\",\n        \"error_type\": \"NetworkError\",\n        \"retry_count\": 3\n    },\n    run_id=\"job-124\"\n)\n\n# Skipped execution\nskipped_result = Result(\n    status=SKIP,\n    context={\n        \"reason\": \"Condition not met\",\n        \"condition\": \"params.enable_processing == true\"\n    },\n    run_id=\"job-125\"\n)\n</code></pre>"},{"location":"api/result/#result-comparison","title":"Result Comparison","text":"<pre><code>from ddeutil.workflow.result import Result, SUCCESS, FAILED\n\ndef compare_results(result1: Result, result2: Result) -&gt; bool:\n    \"\"\"Compare two results for equality.\"\"\"\n    return (\n        result1.status == result2.status and\n        result1.context == result2.context and\n        result1.run_id == result2.run_id\n    )\n\n# Usage\nresult1 = Result(status=SUCCESS, context={\"data\": \"value\"})\nresult2 = Result(status=SUCCESS, context={\"data\": \"value\"})\n\nare_equal = compare_results(result1, result2)  # True\n</code></pre>"},{"location":"api/result/#error-handling","title":"Error Handling","text":"<pre><code>from ddeutil.workflow.result import Result, get_status_from_error\n\ndef safe_execution(func, *args, **kwargs):\n    \"\"\"Execute function with error handling.\"\"\"\n    try:\n        output = func(*args, **kwargs)\n        return Result(\n            status=SUCCESS,\n            context={\"output\": output}\n        )\n    except Exception as e:\n        return Result(\n            status=get_status_from_error(e),\n            context={\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            }\n        )\n</code></pre>"},{"location":"api/result/#workflow-integration","title":"Workflow Integration","text":"<pre><code>from ddeutil.workflow.result import Result, SUCCESS, FAILED\n\nclass WorkflowExecutor:\n    def execute_stage(self, stage, params):\n        \"\"\"Execute a stage and return result.\"\"\"\n        try:\n            output = stage.execute(params)\n            return Result(\n                status=SUCCESS,\n                context={\"outputs\": output},\n                run_id=stage.run_id\n            )\n        except Exception as e:\n            return Result(\n                status=FAILED,\n                context={\n                    \"error\": str(e),\n                    \"stage_name\": stage.name\n                },\n                run_id=stage.run_id\n            )\n\n    def aggregate_results(self, results):\n        \"\"\"Aggregate multiple stage results.\"\"\"\n        if all(r.status == SUCCESS for r in results):\n            return Result(\n                status=SUCCESS,\n                context={\"stages\": {r.run_id: r.context for r in results}}\n            )\n        else:\n            failed_results = [r for r in results if r.status == FAILED]\n            return Result(\n                status=FAILED,\n                context={\n                    \"failed_stages\": [r.context for r in failed_results]\n                }\n            )\n</code></pre>"},{"location":"api/result/#best-practices","title":"Best Practices","text":""},{"location":"api/result/#1-status-management","title":"1. Status Management","text":"<ul> <li>Use appropriate status values for different execution states</li> <li>Handle all possible status values in your code</li> <li>Use status comparison for conditional logic</li> </ul>"},{"location":"api/result/#2-context-structure","title":"2. Context Structure","text":"<ul> <li>Maintain consistent context structure within your workflows</li> <li>Include relevant metadata in context</li> <li>Use descriptive keys for context data</li> </ul>"},{"location":"api/result/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Always include error information in failed results</li> <li>Use <code>get_status_from_error()</code> for consistent error mapping</li> <li>Preserve error context for debugging</li> </ul>"},{"location":"api/result/#4-result-comparison","title":"4. Result Comparison","text":"<ul> <li>Use the built-in comparison mechanism for result equality</li> <li>Consider run_id when comparing results</li> <li>Handle None values appropriately</li> </ul>"},{"location":"api/result/#5-performance","title":"5. Performance","text":"<ul> <li>Keep context data minimal for large-scale executions</li> <li>Avoid storing large objects in context</li> <li>Use appropriate data structures for context</li> </ul>"},{"location":"api/reusables/","title":"Reusables","text":"<p>The Reusables module contains template functions, filters, and decorators for workflow parameter templating and function registration.</p>"},{"location":"api/reusables/#overview","title":"Overview","text":"<p>This module provides:</p> <ul> <li>Template rendering: Dynamic parameter substitution with <code>${{ }}</code> syntax</li> <li>Filter functions: Data transformation functions for template values</li> <li>Function registration: Decorator-based system for workflow callable functions</li> <li>Argument parsing: Utilities for parsing and validating function arguments</li> </ul>"},{"location":"api/reusables/#template-functions","title":"Template Functions","text":""},{"location":"api/reusables/#str2template","title":"<code>str2template</code>","text":"<p>Converts a string with template syntax to its resolved value.</p> <p>Template Syntax</p> <pre><code>from ddeutil.workflow.reusables import str2template\n\nparams = {\n    \"name\": \"John\",\n    \"date\": datetime(2024, 1, 1)\n}\n\n# Basic templating\nresult = str2template(\"Hello ${{ params.name }}\", params)\n# Output: \"Hello John\"\n\n# With filters\nresult = str2template(\"Date: ${{ params.date | fmt('%Y-%m-%d') }}\", params)\n# Output: \"Date: 2024-01-01\"\n</code></pre>"},{"location":"api/reusables/#param2template","title":"<code>param2template</code>","text":"<p>Recursively processes nested data structures for template resolution.</p> <p>Nested Templates</p> <pre><code>from ddeutil.workflow.reusables import param2template\n\nparams = {\"env\": \"prod\", \"version\": \"1.0\"}\ndata = {\n    \"image\": \"app:${{ params.version }}\",\n    \"config\": {\n        \"environment\": \"${{ params.env }}\"\n    }\n}\n\nresult = param2template(data, params)\n# Output: {\"image\": \"app:1.0\", \"config\": {\"environment\": \"prod\"}}\n</code></pre>"},{"location":"api/reusables/#filter-functions","title":"Filter Functions","text":""},{"location":"api/reusables/#built-in-filters","title":"Built-in Filters","text":"Filter Description Example <code>abs</code> Absolute value <code>${{ -5 \\| abs }}</code> \u2192 <code>5</code> <code>str</code> Convert to string <code>${{ 123 \\| str }}</code> \u2192 <code>\"123\"</code> <code>int</code> Convert to integer <code>${{ \"123\" \\| int }}</code> \u2192 <code>123</code> <code>upper</code> Uppercase string <code>${{ \"hello\" \\| upper }}</code> \u2192 <code>\"HELLO\"</code> <code>lower</code> Lowercase string <code>${{ \"HELLO\" \\| lower }}</code> \u2192 <code>\"hello\"</code> <code>title</code> Title case <code>${{ \"hello world\" \\| title }}</code> \u2192 <code>\"Hello World\"</code>"},{"location":"api/reusables/#custom-filters","title":"Custom Filters","text":""},{"location":"api/reusables/#custom_filter","title":"<code>@custom_filter</code>","text":"<p>Decorator for creating custom filter functions.</p> <p>Custom Filter</p> <pre><code>from ddeutil.workflow.reusables import custom_filter\n\n@custom_filter(\"multiply\")\ndef multiply_by(value: int, factor: int = 2) -&gt; int:\n    return value * factor\n\n# Usage in template: ${{ 5 | multiply(3) }}\n# Result: 15\n</code></pre>"},{"location":"api/reusables/#built-in-custom-filters","title":"Built-in Custom Filters","text":""},{"location":"api/reusables/#fmt","title":"<code>fmt</code>","text":"<p>Formats datetime objects using strftime patterns.</p> <p>Date Formatting</p> <pre><code># Template: ${{ params.date | fmt('%Y-%m-%d') }}\n# Input: datetime(2024, 1, 15, 10, 30)\n# Output: \"2024-01-15\"\n</code></pre>"},{"location":"api/reusables/#coalesce","title":"<code>coalesce</code>","text":"<p>Returns the first non-None value or a default.</p> <p>Coalesce</p> <pre><code># Template: ${{ params.optional_value | coalesce('default') }}\n# Input: None\n# Output: \"default\"\n</code></pre>"},{"location":"api/reusables/#getitem","title":"<code>getitem</code>","text":"<p>Gets item from dictionary with optional default.</p> <p>Get Item</p> <pre><code># Template: ${{ params.config | getitem('timeout', 30) }}\n# Input: {\"host\": \"localhost\"}\n# Output: 30\n</code></pre>"},{"location":"api/reusables/#getindex","title":"<code>getindex</code>","text":"<p>Gets item from list by index.</p> <p>Get Index</p> <pre><code># Template: ${{ params.servers | getindex(0) }}\n# Input: [\"server1\", \"server2\"]\n# Output: \"server1\"\n</code></pre>"},{"location":"api/reusables/#function-registration","title":"Function Registration","text":""},{"location":"api/reusables/#tag","title":"<code>@tag</code>","text":"<p>Decorator for registering workflow callable functions.</p> <p>Function Registration</p> <pre><code>from ddeutil.workflow import tag, Result\n\n@tag(\"database\", alias=\"connect-postgres\")\ndef connect_to_postgres(\n    host: str,\n    port: int = 5432,\n    database: str = \"mydb\",\n    result: Result = None\n) -&gt; dict:\n    # Database connection logic\n    return {\"status\": \"connected\", \"host\": host}\n</code></pre> <p>Usage in YAML</p> <pre><code>stages:\n  - name: \"Connect to Database\"\n    uses: database/connect_to_postgres@connect-postgres\n    with:\n      host: ${{ params.db_host }}\n      database: ${{ params.db_name }}\n</code></pre>"},{"location":"api/reusables/#registry-functions","title":"Registry Functions","text":""},{"location":"api/reusables/#make_registry","title":"<code>make_registry</code>","text":"<p>Creates a registry of tagged functions from specified modules.</p> <p>Registry Creation</p> <pre><code>from ddeutil.workflow.reusables import make_registry\n\n# Load functions from tasks module\nregistry = make_registry(\"tasks\", registries=[\"my_project.tasks\"])\n\n# Access registered function\nfunc = registry[\"my_function\"]\n</code></pre>"},{"location":"api/reusables/#extract_call","title":"<code>extract_call</code>","text":"<p>Extracts and validates function calls from workflow strings.</p> <p>Call Extraction</p> <pre><code>from ddeutil.workflow.reusables import extract_call\n\n# From workflow: \"tasks/process_data@etl\"\nfunc = extract_call(\"tasks/process_data@etl\")\nresult = func()  # Execute the function\n</code></pre>"},{"location":"api/reusables/#utility-functions","title":"Utility Functions","text":""},{"location":"api/reusables/#has_template","title":"<code>has_template</code>","text":"<p>Checks if a value contains template syntax.</p> <p>Template Detection</p> <pre><code>from ddeutil.workflow.reusables import has_template\n\nhas_template(\"Hello ${{ params.name }}\")  # True\nhas_template(\"Hello World\")               # False\n</code></pre>"},{"location":"api/reusables/#get_args_const","title":"<code>get_args_const</code>","text":"<p>Parses function call expressions to extract arguments.</p> <p>Argument Parsing</p> <pre><code>from ddeutil.workflow.reusables import get_args_const\n\nname, args, kwargs = get_args_const(\"func(1, 2, key='value')\")\n# name: \"func\"\n# args: [1, 2]\n# kwargs: {\"key\": \"value\"}\n</code></pre>"},{"location":"api/reusables/#configuration","title":"Configuration","text":"<p>Reusables behavior can be configured through environment variables:</p> Variable Default Description <code>WORKFLOW_CORE_REGISTRY_FILTER</code> <code>[]</code> List of modules to search for filter functions <code>WORKFLOW_CORE_REGISTRY_CALLER</code> <code>[]</code> List of modules to search for callable functions <p>Performance</p> <p>Filter and tag registries are cached at module level for optimal performance. Use <code>make_filter_registry()</code> and <code>make_registry()</code> to rebuild registries when needed.</p>"},{"location":"api/stages/","title":"Stages","text":"<p>The Stages module provides the core execution layer for workflow tasks. Stages are the smallest executable units that run within a single thread and handle specific operations like script execution, function calls, data processing, and workflow orchestration.</p>"},{"location":"api/stages/#overview","title":"Overview","text":"<p>Stages are the fundamental building blocks of workflows, providing:</p> <ul> <li>Atomic execution: Each stage runs as a single unit with clear inputs/outputs</li> <li>Error isolation: Stage failures are contained and don't affect other stages</li> <li>Flexible execution: Support for various execution environments and languages</li> <li>Conditional logic: Dynamic execution based on parameters and context</li> <li>Retry mechanisms: Automatic retry on failures with configurable policies</li> </ul>"},{"location":"api/stages/#stage-execution-flow","title":"Stage Execution Flow","text":"<pre><code>flowchart TD\n    A[\"Stage.execute()\"] --&gt; B[\"Generate run_id\"]\n    B --&gt; C[\"Initialize context\"]\n    C --&gt; D[\"Log stage start\"]\n    D --&gt; E{\"Check condition\"}\n    E --&gt;|Skip| F[\"Raise StageSkipError\"]\n    E --&gt;|Execute| G[\"Call _execute()\"]\n    G --&gt; H[\"Set context status WAIT\"]\n    H --&gt; I[\"Call stage.process()\"]\n    I --&gt; J{\"Process result\"}\n    J --&gt;|Success| K[\"Return SUCCESS Result\"]\n    J --&gt;|Failure| L[\"Raise StageError\"]\n    J --&gt;|Cancel| M[\"Raise StageCancelError\"]\n    J --&gt;|Skip| N[\"Raise StageSkipError\"]\n\n    F --&gt; O[\"Return SKIP Result\"]\n    L --&gt; P[\"Return FAILED Result\"]\n    M --&gt; Q[\"Return CANCEL Result\"]\n    N --&gt; O\n\n    K --&gt; R[\"Add execution time\"]\n    O --&gt; R\n    P --&gt; R\n    Q --&gt; R\n\n    R --&gt; S[\"Final Result\"]\n\n    style A fill:#e1f5fe\n    style S fill:#c8e6c9\n    style F fill:#ffcdd2\n    style L fill:#ffcdd2\n    style M fill:#ffcdd2\n    style N fill:#ffcdd2</code></pre>"},{"location":"api/stages/#quick-start","title":"Quick Start","text":"<pre><code>from ddeutil.workflow.stages import EmptyStage, BashStage, PyStage\n\n# Simple echo stage\nstage = EmptyStage(\n    name=\"Hello World\",\n    echo=\"Starting workflow execution\"\n)\n\n# Bash script execution\nbash_stage = BashStage(\n    name=\"Process Data\",\n    bash=\"echo 'Processing...' &amp;&amp; ls -la /data\"\n)\n\n# Python code execution\npy_stage = PyStage(\n    name=\"Data Analysis\",\n    run=\"\"\"\nimport pandas as pd\ndf = pd.read_csv('/data/input.csv')\nresult = len(df)\nprint(f\"Processed {result} rows\")\n\"\"\",\n    vars={\"input_file\": \"/data/input.csv\"}\n)\n</code></pre>"},{"location":"api/stages/#stage-types-summary","title":"Stage Types Summary","text":"Stage Type Purpose Use Case Inheritance <code>EmptyStage</code> Logging and delays Debugging, notifications, timing <code>BaseAsyncStage</code> <code>BashStage</code> Shell script execution File operations, system commands <code>BaseRetryStage</code> <code>PyStage</code> Python code execution Data processing, API calls, analysis <code>BaseRetryStage</code> <code>VirtualPyStage</code> Python in virtual env Isolated Python execution <code>PyStage</code> <code>CallStage</code> Function calls Reusable business logic <code>BaseRetryStage</code> <code>TriggerStage</code> Workflow orchestration Multi-workflow pipelines <code>BaseNestedStage</code> <code>ParallelStage</code> Concurrent execution Performance optimization <code>BaseNestedStage</code> <code>ForEachStage</code> Iterative processing Batch operations <code>BaseNestedStage</code> <code>CaseStage</code> Conditional execution Branching logic <code>BaseNestedStage</code> <code>UntilStage</code> Retry loops Polling, retry logic <code>BaseNestedStage</code> <code>RaiseStage</code> Error simulation Testing, error handling <code>BaseAsyncStage</code> <code>DockerStage</code> Container execution Containerized workloads <code>BaseStage</code>"},{"location":"api/stages/#stage-class-hierarchy","title":"Stage Class Hierarchy","text":"<pre><code>classDiagram\n    BaseStage &lt;|-- BaseAsyncStage\n    BaseStage &lt;|-- DockerStage\n    BaseAsyncStage &lt;|-- BaseRetryStage\n    BaseAsyncStage &lt;|-- EmptyStage\n    BaseAsyncStage &lt;|-- RaiseStage\n    BaseRetryStage &lt;|-- BashStage\n    BaseRetryStage &lt;|-- PyStage\n    BaseRetryStage &lt;|-- CallStage\n    BaseRetryStage &lt;|-- BaseNestedStage\n    PyStage &lt;|-- VirtualPyStage\n    BaseNestedStage &lt;|-- TriggerStage\n    BaseNestedStage &lt;|-- ParallelStage\n    BaseNestedStage &lt;|-- ForEachStage\n    BaseNestedStage &lt;|-- UntilStage\n    BaseNestedStage &lt;|-- CaseStage\n\n    class BaseStage {\n        +extras: dict\n        +id: str\n        +name: str\n        +desc: str\n        +condition: str\n        +process()\n        +execute()\n        +is_skipped()\n    }\n\n    class BaseAsyncStage {\n        +async_process()\n        +axecute()\n    }\n\n    class BaseRetryStage {\n        +retry: int\n    }\n\n    class BaseNestedStage {\n        +is_nested: bool\n    }</code></pre>"},{"location":"api/stages/#base-classes","title":"Base Classes","text":""},{"location":"api/stages/#basestage","title":"BaseStage","text":"<p>Abstract base class for all stage implementations.</p> <p>Key Features</p> <ul> <li>Common stage lifecycle management</li> <li>Parameter templating and validation</li> <li>Error handling and retry logic</li> <li>Output collection and context management</li> </ul>"},{"location":"api/stages/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>id</code> str | None <code>None</code> Unique stage identifier for output tracking <code>name</code> str Required Human-readable stage name for logging <code>desc</code> str | None <code>None</code> Stage description for documentation <code>condition</code> str | None <code>None</code> Conditional expression for execution <code>extras</code> dict <code>{}</code> Additional configuration parameters"},{"location":"api/stages/#methods","title":"Methods","text":""},{"location":"api/stages/#executeparams-run_id-context-parent_run_idnone-eventnone","title":"<code>execute(params, run_id, context, *, parent_run_id=None, event=None)</code>","text":"<p>Execute the stage with provided parameters.</p> <p>Parameters: - <code>params</code> (dict): Parameter values for stage execution - <code>run_id</code> (str): Unique execution identifier - <code>context</code> (dict): Execution context from previous stages - <code>parent_run_id</code> (str, optional): Parent workflow run identifier - <code>event</code> (Event, optional): Threading event for cancellation</p> <p>Returns: - <code>Result</code>: Stage execution result with status and outputs</p>"},{"location":"api/stages/#is_skippedparams","title":"<code>is_skipped(params)</code>","text":"<p>Check if stage should be skipped based on condition.</p> <p>Parameters: - <code>params</code> (dict): Current parameter context</p> <p>Returns: - <code>bool</code>: True if stage should be skipped</p>"},{"location":"api/stages/#baseasyncstage","title":"BaseAsyncStage","text":"<p>Base class for stages with async execution capabilities.</p>"},{"location":"api/stages/#methods_1","title":"Methods","text":""},{"location":"api/stages/#async_processparams-run_id-context-parent_run_idnone-eventnone","title":"<code>async_process(params, run_id, context, *, parent_run_id=None, event=None)</code>","text":"<p>Async execution method that must be implemented by subclasses.</p>"},{"location":"api/stages/#axecuteparams-run_idnone-eventnone","title":"<code>axecute(params, *, run_id=None, event=None)</code>","text":"<p>Async handler for stage execution with error handling.</p>"},{"location":"api/stages/#baseretrystage","title":"BaseRetryStage","text":"<p>Base class for stages with retry capabilities.</p>"},{"location":"api/stages/#attributes_1","title":"Attributes","text":"Attribute Type Default Description <code>retry</code> int <code>0</code> Number of retry attempts on failure"},{"location":"api/stages/#basenestedstage","title":"BaseNestedStage","text":"<p>Base class for stages that contain other stages.</p>"},{"location":"api/stages/#properties","title":"Properties","text":"Property Type Description <code>is_nested</code> bool Always returns <code>True</code> for nested stages"},{"location":"api/stages/#stage-implementations","title":"Stage Implementations","text":""},{"location":"api/stages/#empty-stage","title":"Empty Stage","text":"<p>Empty stage executor that logs messages and optionally delays execution.</p> <p>Empty Stage Usage</p> Basic LoggingWith DelayConditional Execution <pre><code>stages:\n  - name: \"Start Processing\"\n    echo: \"Beginning data pipeline execution\"\n</code></pre> <pre><code>stages:\n  - name: \"Wait for System\"\n    echo: \"Waiting for system to be ready...\"\n    sleep: 30\n</code></pre> <pre><code>stages:\n  - name: \"Debug Info\"\n    if: \"${{ params.debug_mode == true }}\"\n    echo: \"Debug mode enabled - showing detailed logs\"\n</code></pre>"},{"location":"api/stages/#attributes_2","title":"Attributes","text":"Attribute Type Default Description <code>echo</code> str | None <code>None</code> Message to log to stdout <code>sleep</code> float <code>0</code> Seconds to sleep before execution (0-1800)"},{"location":"api/stages/#use-cases","title":"Use Cases","text":"<ul> <li>Debugging: Add logging points to track workflow progress</li> <li>Timing: Introduce delays for system synchronization</li> <li>Notifications: Log important events or milestones</li> <li>Conditional Logic: Execute based on parameter values</li> </ul>"},{"location":"api/stages/#bash-stage","title":"Bash Stage","text":"<p>Bash stage executor for shell script execution.</p> <p>Security Considerations</p> <ul> <li>Scripts run with system permissions</li> <li>Validate all inputs to prevent injection attacks</li> <li>Consider using <code>PyStage</code> for complex logic</li> <li>Avoid executing untrusted scripts</li> </ul> <p>Bash Stage Usage</p> Simple CommandWith Environment VariablesError Handling <pre><code>stages:\n  - name: \"Check Disk Space\"\n    bash: \"df -h /data\"\n</code></pre> <pre><code>stages:\n  - name: \"Process Files\"\n    env:\n      INPUT_DIR: \"/data/input\"\n      OUTPUT_DIR: \"/data/output\"\n    bash: |\n      echo \"Processing files in $INPUT_DIR\"\n      ls -la \"$INPUT_DIR\"\n      mkdir -p \"$OUTPUT_DIR\"\n</code></pre> <pre><code>stages:\n  - name: \"Safe File Operation\"\n    bash: |\n      set -e  # Exit on any error\n      if [ ! -f \"/data/input.csv\" ]; then\n        echo \"Input file not found\"\n        exit 1\n      fi\n      echo \"Processing input file...\"\n</code></pre>"},{"location":"api/stages/#attributes_3","title":"Attributes","text":"Attribute Type Default Description <code>bash</code> str Required Bash script to execute <code>env</code> dict[str, Any] <code>{}</code> Environment variables for script <code>retry</code> int <code>0</code> Number of retry attempts on failure"},{"location":"api/stages/#limitations","title":"Limitations","text":"<ul> <li>Multiline scripts: Complex multiline scripts are written to temporary files</li> <li>Cross-platform: Windows requires WSL or bash compatibility</li> <li>Error handling: Script errors must be handled explicitly</li> <li>Output parsing: Script output is captured as strings</li> </ul>"},{"location":"api/stages/#python-stage","title":"Python Stage","text":"<p>Python stage for executing Python code with full access to installed packages.</p> <p>Security Warning</p> <ul> <li>Uses <code>exec()</code> function which can be dangerous</li> <li>Validate all code before execution</li> <li>Consider using <code>CallStage</code> for production code</li> <li>Avoid executing untrusted code</li> </ul> <p>Python Stage Usage</p> Basic PythonWith VariablesError Handling <pre><code>stages:\n  - name: \"Data Analysis\"\n    run: |\n      import pandas as pd\n      import numpy as np\n\n      df = pd.read_csv('/data/input.csv')\n      summary = df.describe()\n      print(f\"Dataset shape: {df.shape}\")\n</code></pre> <pre><code>stages:\n  - name: \"Process Data\"\n    vars:\n      input_file: \"/data/input.csv\"\n      threshold: 1000\n    run: |\n      import pandas as pd\n\n      df = pd.read_csv(input_file)\n      if len(df) &gt; threshold:\n          print(f\"Large dataset: {len(df)} rows\")\n      else:\n          print(f\"Small dataset: {len(df)} rows\")\n</code></pre> <pre><code>stages:\n  - name: \"Safe Processing\"\n    run: |\n      try:\n          import pandas as pd\n          df = pd.read_csv('/data/input.csv')\n          result = len(df)\n          print(f\"Successfully processed {result} rows\")\n      except Exception as e:\n          print(f\"Error processing data: {e}\")\n          raise\n</code></pre>"},{"location":"api/stages/#attributes_4","title":"Attributes","text":"Attribute Type Default Description <code>run</code> str Required Python code to execute <code>vars</code> dict[str, Any] <code>{}</code> Variables available in execution context <code>retry</code> int <code>0</code> Number of retry attempts on failure"},{"location":"api/stages/#best-practices","title":"Best Practices","text":"<ul> <li>Import statements: Place imports at the top of the code block</li> <li>Error handling: Use try-catch blocks for robust execution</li> <li>Output: Use <code>print()</code> for logging, return values for data</li> <li>Dependencies: Ensure required packages are installed</li> <li>Security: Validate all inputs and avoid <code>eval()</code> or <code>exec()</code></li> </ul>"},{"location":"api/stages/#virtual-python-stage","title":"Virtual Python Stage","text":"<p>Virtual Python stage for executing Python code in isolated virtual environments using <code>uv</code>.</p> <p>Virtual Python Stage Usage</p> Basic Virtual Environment <pre><code>stages:\n  - name: \"Isolated Analysis\"\n    version: \"3.11\"\n    deps: [\"pandas\", \"numpy\", \"matplotlib\"]\n    run: |\n      import pandas as pd\n      import numpy as np\n\n      df = pd.DataFrame(np.random.randn(100, 4))\n      print(f\"Generated dataset: {df.shape}\")\n</code></pre>"},{"location":"api/stages/#attributes_5","title":"Attributes","text":"Attribute Type Default Description <code>run</code> str Required Python code to execute <code>vars</code> dict[str, Any] <code>{}</code> Variables available in execution context <code>version</code> str <code>\"3.9\"</code> Python version for virtual environment <code>deps</code> list[str] Required Python dependencies to install <code>retry</code> int <code>0</code> Number of retry attempts on failure"},{"location":"api/stages/#call-stage","title":"Call Stage","text":"<p>Call stage for executing registered functions with arguments.</p> <p>Call Stage Usage</p> Basic Function CallWith Complex ArgumentsConditional Call <pre><code>stages:\n  - name: \"Process Data\"\n    uses: \"tasks/process_csv@latest\"\n    with:\n      input_path: \"/data/input.csv\"\n      output_path: \"/data/output.parquet\"\n</code></pre> <pre><code>stages:\n  - name: \"API Call\"\n    uses: \"api/send_notification@v1\"\n    with:\n      message: \"Workflow completed successfully\"\n      recipients: [\"admin@example.com\"]\n      priority: \"high\"\n      metadata:\n        workflow_id: \"${{ params.workflow_id }}\"\n        execution_time: \"${{ params.execution_time }}\"\n</code></pre> <pre><code>stages:\n  - name: \"Send Alert\"\n    if: \"${{ params.send_alerts == true }}\"\n    uses: \"notifications/send_alert@production\"\n    with:\n      level: \"${{ params.alert_level }}\"\n      message: \"Data processing completed\"\n</code></pre>"},{"location":"api/stages/#attributes_6","title":"Attributes","text":"Attribute Type Default Description <code>uses</code> str Required Function reference in format <code>module/function@tag</code> <code>args</code> dict[str, Any] <code>{}</code> Arguments passed to the function <code>retry</code> int <code>0</code> Number of retry attempts on failure"},{"location":"api/stages/#function-registration","title":"Function Registration","text":"<p>Functions must be registered using the <code>@tag</code> decorator:</p> <pre><code>from ddeutil.workflow import tag\n\n@tag(\"tasks\", alias=\"process_csv\")\ndef process_csv_file(input_path: str, output_path: str) -&gt; dict:\n    \"\"\"Process CSV file and convert to Parquet format.\"\"\"\n    import pandas as pd\n\n    df = pd.read_csv(input_path)\n    df.to_parquet(output_path)\n\n    return {\n        \"rows_processed\": len(df),\n        \"output_size\": output_path\n    }\n</code></pre>"},{"location":"api/stages/#trigger-stage","title":"Trigger Stage","text":"<p>Trigger stage for orchestrating other workflows.</p> <p>Trigger Stage Usage</p> Simple TriggerWith Complex Parameters <pre><code>stages:\n  - name: \"Run Data Pipeline\"\n    trigger: \"data-pipeline\"\n    params:\n      date: \"${{ params.processing_date }}\"\n      environment: \"production\"\n</code></pre> <pre><code>stages:\n  - name: \"Multi-Environment Deploy\"\n    trigger: \"deployment-workflow\"\n    params:\n      application: \"web-app\"\n      environments: [\"dev\", \"staging\", \"prod\"]\n      version: \"${{ params.app_version }}\"\n      rollback_on_failure: true\n</code></pre>"},{"location":"api/stages/#attributes_7","title":"Attributes","text":"Attribute Type Default Description <code>trigger</code> str Required Name of workflow to trigger <code>params</code> dict[str, Any] <code>{}</code> Parameters passed to triggered workflow"},{"location":"api/stages/#parallel-stage","title":"Parallel Stage","text":"<p>Parallel stage for concurrent execution of multiple stages.</p> <p>Parallel Stage Usage</p> Basic Parallel ExecutionWith Shared Context <pre><code>stages:\n  - name: \"Parallel Processing\"\n    parallel:\n      branch01:\n        - name: \"Process Region A\"\n          bash: \"process_data.sh --region=us-east\"\n        - name: \"Validate Region A\"\n          uses: \"validation/check_data@latest\"\n      branch02:\n        - name: \"Process Region B\"\n          bash: \"process_data.sh --region=us-west\"\n</code></pre> <pre><code>stages:\n  - name: \"Multi-Service Health Check\"\n    max-workers: 3\n    parallel:\n      database:\n        - name: \"Check Database\"\n          uses: \"health/database_check@latest\"\n          with:\n            host: \"${{ params.db_host }}\"\n      api:\n        - name: \"Check API\"\n          uses: \"health/api_check@latest\"\n          with:\n            endpoint: \"${{ params.api_url }}\"\n      cache:\n        - name: \"Check Cache\"\n          uses: \"health/cache_check@latest\"\n          with:\n            redis_url: \"${{ params.redis_url }}\"\n</code></pre>"},{"location":"api/stages/#attributes_8","title":"Attributes","text":"Attribute Type Default Description <code>parallel</code> dict[str, list[Stage]] Required Mapping of branch names to stage lists <code>max_workers</code> int <code>2</code> Maximum number of concurrent workers (1-20)"},{"location":"api/stages/#foreach-stage","title":"ForEach Stage","text":"<p>ForEach stage for iterative processing of collections.</p> <p>ForEach Stage Usage</p> Process File ListMatrix ProcessingWith Index <pre><code>stages:\n  - name: \"Process Files\"\n    foreach: \"${{ params.file_list }}\"\n    concurrent: 2\n    stages:\n      - name: \"Process ${{ item }}\"\n        bash: \"process_file.sh '${{ item }}'\"\n</code></pre> <pre><code>stages:\n  - name: \"Multi-Environment Deploy\"\n    foreach: \"${{ params.environments }}\"\n    stages:\n      - name: \"Deploy to ${{ item }}\"\n        uses: \"deploy/application@latest\"\n        with:\n          environment: \"${{ item }}\"\n          version: \"${{ params.app_version }}\"\n</code></pre> <pre><code>stages:\n  - name: \"Process with Index\"\n    foreach: [\"a\", \"b\", \"c\"]\n    use-index-as-key: true\n    stages:\n      - name: \"Process item ${{ item }} at index ${{ loop }}\"\n        echo: \"Processing item ${{ item }} (index: ${{ loop }})\"\n</code></pre>"},{"location":"api/stages/#attributes_9","title":"Attributes","text":"Attribute Type Default Description <code>foreach</code> Union[list, str] Required Items to iterate over <code>stages</code> list[Stage] Required Stages to execute for each item <code>concurrent</code> int <code>1</code> Number of concurrent executions (1-10) <code>use_index_as_key</code> bool <code>False</code> Use loop index as key instead of item value"},{"location":"api/stages/#case-stage","title":"Case Stage","text":"<p>Case stage for conditional execution based on parameter values.</p> <p>Case Stage Usage</p> Environment-Specific LogicError Handling Cases <pre><code>stages:\n  - name: \"Environment Setup\"\n    case: \"${{ params.environment }}\"\n    match:\n      - case: \"development\"\n        stages:\n          - name: \"Setup Dev Environment\"\n            bash: \"setup_dev.sh\"\n      - case: \"staging\"\n        stages:\n          - name: \"Setup Staging Environment\"\n            bash: \"setup_staging.sh\"\n      - case: \"production\"\n        stages:\n          - name: \"Setup Production Environment\"\n            bash: \"setup_prod.sh\"\n      - case: \"_\"\n        stages:\n          - name: \"Default Setup\"\n            bash: \"setup_default.sh\"\n</code></pre> <pre><code>stages:\n  - name: \"Error Recovery\"\n    case: \"${{ params.error_type }}\"\n    match:\n      - case: \"network\"\n        stages:\n          - name: \"Retry Network Operation\"\n            uses: \"retry/network@latest\"\n      - case: \"database\"\n        stages:\n          - name: \"Database Recovery\"\n            uses: \"recovery/database@latest\"\n      - case: \"_\"\n        stages:\n          - name: \"Generic Error Handling\"\n            uses: \"error/generic@latest\"\n</code></pre>"},{"location":"api/stages/#attributes_10","title":"Attributes","text":"Attribute Type Default Description <code>case</code> str Required Case condition to evaluate <code>match</code> list[Match] Required List of case matches with stages <code>skip_not_match</code> bool <code>False</code> Skip execution if no case matches"},{"location":"api/stages/#until-stage","title":"Until Stage","text":"<p>Until stage for retry loops and polling operations.</p> <p>Until Stage Usage</p> Polling for CompletionRetry with Backoff <pre><code>stages:\n  - name: \"Wait for Job Completion\"\n    item: 0\n    until: \"${{ item &gt;= 3 }}\"\n    max-loop: 30\n    stages:\n      - name: \"Check Job Status\"\n        uses: \"jobs/check_status@latest\"\n        with:\n          job_id: \"${{ params.job_id }}\"\n      - name: \"Increment Counter\"\n        run: \"item = ${{ item }} + 1\"\n</code></pre> <pre><code>stages:\n  - name: \"Retry API Call\"\n    item: 0\n    until: \"${{ result.success == true }}\"\n    max-loop: 5\n    stages:\n      - name: \"API Request\"\n        uses: \"api/make_request@latest\"\n        with:\n          endpoint: \"${{ params.api_endpoint }}\"\n      - name: \"Increment Attempt\"\n        run: \"item = ${{ item }} + 1\"\n</code></pre>"},{"location":"api/stages/#attributes_11","title":"Attributes","text":"Attribute Type Default Description <code>item</code> Union[str, int, bool] <code>0</code> Initial value for loop iteration <code>until</code> str Required Condition to stop the loop <code>stages</code> list[Stage] Required Stages to execute in each loop <code>max_loop</code> int <code>10</code> Maximum number of loop iterations (1-100)"},{"location":"api/stages/#raise-stage","title":"Raise Stage","text":"<p>Raise stage for simulating errors and testing error handling.</p> <p>Raise Stage Usage</p> Simulate ErrorConditional Error <pre><code>stages:\n  - name: \"Test Error Handling\"\n    raise: \"Simulated error for testing\"\n</code></pre> <pre><code>stages:\n  - name: \"Conditional Error\"\n    if: \"${{ params.simulate_failure == true }}\"\n    raise: \"Simulated failure for testing error paths\"\n</code></pre>"},{"location":"api/stages/#attributes_12","title":"Attributes","text":"Attribute Type Default Description <code>message</code> str Required Error message to raise (aliased as <code>raise</code>)"},{"location":"api/stages/#docker-stage","title":"Docker Stage","text":"<p>Docker stage for containerized execution (not yet implemented).</p> <p>Implementation Status</p> <p>The Docker stage is currently not implemented and will raise <code>NotImplementedError</code>.</p>"},{"location":"api/stages/#attributes_13","title":"Attributes","text":"Attribute Type Default Description <code>image</code> str Required Docker image URL <code>tag</code> str <code>\"latest\"</code> Docker image tag <code>env</code> dict[str, Any] <code>{}</code> Environment variables for container <code>volume</code> dict[str, Any] <code>{}</code> Volume mappings <code>auth</code> dict[str, Any] <code>{}</code> Docker registry authentication"},{"location":"api/stages/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/stages/#custom-stage-development","title":"Custom Stage Development","text":"<p>Create custom stages by inheriting from <code>BaseStage</code>:</p> <pre><code>from ddeutil.workflow.stages import BaseStage, Result, SUCCESS\n\nclass CustomStage(BaseStage):\n    \"\"\"Custom stage for specific business logic.\"\"\"\n\n    def process(self, params: dict, run_id: str, context: dict, **kwargs) -&gt; Result:\n        \"\"\"Custom execution logic.\"\"\"\n        # Your custom logic here\n        result = self.process_data(params)\n\n        return Result(\n            status=SUCCESS,\n            context=context,\n            run_id=run_id,\n            updated={\"custom_output\": result}\n        )\n\n    def process_data(self, params: dict) -&gt; dict:\n        \"\"\"Process data according to business requirements.\"\"\"\n        # Implementation here\n        return {\"processed\": True}\n</code></pre>"},{"location":"api/stages/#stage-composition","title":"Stage Composition","text":"<p>Combine stages for complex workflows:</p> <pre><code>stages:\n  - name: \"Data Pipeline\"\n    parallel:\n      branch01:\n        - name: \"Extract Data\"\n          stages:\n            - name: \"Download Files\"\n              bash: \"download_data.sh\"\n            - name: \"Validate Files\"\n              uses: \"validation/check_files@latest\"\n      branch02:\n        - name: \"Prepare Environment\"\n          stages:\n            - name: \"Setup Database\"\n              uses: \"database/setup@latest\"\n            - name: \"Configure Services\"\n              uses: \"config/setup@latest\"\n\n  - name: \"Process Data\"\n    foreach: \"${{ params.data_sources }}\"\n    stages:\n      - name: \"Process ${{ item }}\"\n        uses: \"processing/transform@latest\"\n        with:\n          source: \"${{ item }}\"\n</code></pre>"},{"location":"api/stages/#error-handling-patterns","title":"Error Handling Patterns","text":"<pre><code>stages:\n  - name: \"Robust Processing\"\n    stages:\n      - name: \"Attempt Operation\"\n        try:\n          - name: \"Primary Method\"\n            uses: \"api/primary_method@latest\"\n        catch:\n          - name: \"Fallback Method\"\n            uses: \"api/fallback_method@latest\"\n        finally:\n          - name: \"Cleanup\"\n            uses: \"utils/cleanup@latest\"\n</code></pre>"},{"location":"api/stages/#configuration","title":"Configuration","text":"<p>Stage behavior can be configured through environment variables:</p> Variable Default Description <code>WORKFLOW_CORE_STAGE_DEFAULT_ID</code> <code>false</code> Enable default stage IDs <code>WORKFLOW_CORE_STAGE_TIMEOUT</code> <code>3600</code> Default stage timeout in seconds <code>WORKFLOW_CORE_STAGE_RETRY_DELAY</code> <code>5</code> Default retry delay in seconds"},{"location":"api/stages/#best-practices_1","title":"Best Practices","text":""},{"location":"api/stages/#1-stage-design","title":"1. Stage Design","text":"<ul> <li>Single responsibility: Each stage should do one thing well</li> <li>Idempotency: Stages should be safe to retry</li> <li>Error handling: Always handle potential failures gracefully</li> <li>Logging: Provide clear, actionable log messages</li> </ul>"},{"location":"api/stages/#2-performance","title":"2. Performance","text":"<ul> <li>Parallel execution: Use <code>ParallelStage</code> for independent operations</li> <li>Resource management: Clean up resources in finally blocks</li> <li>Caching: Cache expensive operations when possible</li> <li>Batch processing: Process data in batches for efficiency</li> </ul>"},{"location":"api/stages/#3-security","title":"3. Security","text":"<ul> <li>Input validation: Validate all inputs before processing</li> <li>Code review: Review all custom code before deployment</li> <li>Least privilege: Use minimal required permissions</li> <li>Secret management: Use secure methods for sensitive data</li> </ul>"},{"location":"api/stages/#4-monitoring","title":"4. Monitoring","text":"<ul> <li>Metrics: Track stage execution times and success rates</li> <li>Alerts: Set up alerts for stage failures</li> <li>Tracing: Use trace IDs to track execution flow</li> <li>Auditing: Log important stage events for compliance</li> </ul>"},{"location":"api/stages/#5-testing","title":"5. Testing","text":"<ul> <li>Unit tests: Test individual stage logic</li> <li>Integration tests: Test stage interactions</li> <li>Error scenarios: Test failure modes and recovery</li> <li>Performance tests: Validate stage performance under load</li> </ul>"},{"location":"api/traces/","title":"Traces","text":"<p>The Traces module provides comprehensive tracing and logging capabilities for workflow execution monitoring. It supports multiple trace backends including console output, file-based logging, SQLite database storage, REST API integration, and Elasticsearch.</p>"},{"location":"api/traces/#overview","title":"Overview","text":"<p>The traces system provides:</p> <ul> <li>Console Handler: Real-time stdout/stderr logging</li> <li>File Handler: Persistent logging to local filesystem with structured metadata</li> <li>SQLite Handler: Database-backed logging for scalable deployments</li> <li>REST API Handler: Integration with external logging services (Datadog, Grafana, CloudWatch)</li> <li>Elasticsearch Handler: High-performance distributed logging with search capabilities</li> </ul>"},{"location":"api/traces/#core-components","title":"Core Components","text":""},{"location":"api/traces/#trace","title":"<code>Trace</code>","text":"<p>The main trace manager that coordinates multiple handlers and provides a unified logging interface.</p> <p>Basic Usage</p> <pre><code>from ddeutil.workflow.traces import get_trace\n\n# Get trace manager with default handlers\ntrace = get_trace(run_id=\"workflow-123\")\n\n# Log messages\ntrace.info(\"Workflow started\")\ntrace.debug(\"Processing stage dependencies\")\ntrace.warning(\"Retrying failed operation\")\ntrace.error(\"Job failed: connection timeout\")\ntrace.exception(\"Critical error occurred\")\n</code></pre>"},{"location":"api/traces/#metadata","title":"<code>Metadata</code>","text":"<p>Comprehensive metadata model capturing execution context, performance metrics, and distributed tracing information.</p> <p>Metadata Fields</p> <pre><code>from ddeutil.workflow.traces import Metadata\n\n# Metadata includes:\n# - Basic info: run_id, parent_run_id, level, message, datetime\n# - System info: process, thread, filename, lineno, cut_id\n# - Observability: workflow_name, stage_name, job_name\n# - Performance: duration_ms, memory_usage_mb, cpu_usage_percent\n# - Distributed tracing: trace_id, span_id, parent_span_id\n# - Error context: exception_type, exception_message, stack_trace\n# - Business context: user_id, tenant_id, environment\n# - System context: hostname, ip_address, python_version\n# - Custom: tags, metadata\n</code></pre>"},{"location":"api/traces/#handlers","title":"Handlers","text":""},{"location":"api/traces/#consolehandler","title":"<code>ConsoleHandler</code>","text":"<p>Basic console logging implementation that outputs to stdout/stderr.</p> <p>Console Handler</p> <pre><code>from ddeutil.workflow.traces import ConsoleHandler, Trace\n\nhandler = ConsoleHandler(type=\"console\")\ntrace = Trace(\n    run_id=\"workflow-123\",\n    handlers=[handler]\n)\ntrace.info(\"Console logging initialized\")\n</code></pre>"},{"location":"api/traces/#filehandler","title":"<code>FileHandler</code>","text":"<p>File-based trace implementation that persists logs to the local filesystem with structured metadata.</p> <p>File Handler Usage</p> <pre><code>from ddeutil.workflow.traces import FileHandler, Trace\n\n# Create file handler\nhandler = FileHandler(\n    type=\"file\",\n    path=\"./logs/traces\",\n    format=\"{datetime} ({process:5d}, {thread:5d}) ({cut_id}) {message:120s} ({filename}:{lineno})\"\n)\n\ntrace = Trace(\n    run_id=\"workflow-123\",\n    parent_run_id=\"parent-456\",\n    handlers=[handler]\n)\n\n# Log messages\ntrace.info(\"Workflow started\")\ntrace.warning(\"Retrying failed operation\")\n\n# Messages are automatically saved to:\n# ./logs/traces/run_id=workflow-123/\n#   \u251c\u2500\u2500 stdout.txt\n#   \u251c\u2500\u2500 stderr.txt\n#   \u2514\u2500\u2500 metadata.txt\n</code></pre>"},{"location":"api/traces/#finding-traces","title":"Finding Traces","text":"<p>The <code>FileHandler</code> class provides utilities to search and retrieve trace logs.</p> <p>Trace Discovery</p> <pre><code>from ddeutil.workflow.traces import FileHandler\nfrom pathlib import Path\n\nhandler = FileHandler(type=\"file\", path=\"./logs/traces\")\n\n# Find all traces\nfor trace_data in handler.find_traces():\n    print(f\"Run ID: {trace_data.meta[0].run_id if trace_data.meta else 'Unknown'}\")\n    print(f\"Stdout: {trace_data.stdout}\")\n    print(f\"Stderr: {trace_data.stderr}\")\n\n# Find specific trace by run ID\ntrace_data = handler.find_trace_with_id(\"workflow-123\")\nprint(f\"Metadata entries: {len(trace_data.meta)}\")\n</code></pre>"},{"location":"api/traces/#sqlitehandler","title":"<code>SQLiteHandler</code>","text":"<p>SQLite-based trace implementation for scalable logging with structured metadata storage.</p> <p>SQLite Handler</p> <pre><code>from ddeutil.workflow.traces import SQLiteHandler, Trace\n\nhandler = SQLiteHandler(\n    type=\"sqlite\",\n    path=\"./logs/workflow_traces.db\",\n    table_name=\"traces\"\n)\n\ntrace = Trace(\n    run_id=\"workflow-789\",\n    handlers=[handler]\n)\ntrace.info(\"SQLite trace initialized\")\n\n# Traces are stored in SQLite with comprehensive schema including\n# all metadata fields for querying and analysis\n</code></pre>"},{"location":"api/traces/#sqlite-schema","title":"SQLite Schema","text":"<p>The SQLite handler creates a comprehensive table structure:</p> <pre><code>CREATE TABLE traces (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    run_id TEXT NOT NULL,\n    parent_run_id TEXT,\n    level TEXT NOT NULL,\n    message TEXT NOT NULL,\n    error_flag BOOLEAN NOT NULL,\n    datetime TEXT NOT NULL,\n    process INTEGER NOT NULL,\n    thread INTEGER NOT NULL,\n    filename TEXT NOT NULL,\n    lineno INTEGER NOT NULL,\n    cut_id TEXT,\n    workflow_name TEXT,\n    stage_name TEXT,\n    job_name TEXT,\n    duration_ms REAL,\n    memory_usage_mb REAL,\n    cpu_usage_percent REAL,\n    trace_id TEXT,\n    span_id TEXT,\n    parent_span_id TEXT,\n    exception_type TEXT,\n    exception_message TEXT,\n    stack_trace TEXT,\n    error_code TEXT,\n    user_id TEXT,\n    tenant_id TEXT,\n    environment TEXT,\n    hostname TEXT,\n    ip_address TEXT,\n    python_version TEXT,\n    package_version TEXT,\n    tags TEXT,\n    metadata TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n</code></pre>"},{"location":"api/traces/#restapihandler","title":"<code>RestAPIHandler</code>","text":"<p>REST API integration for external logging services.</p> <p>REST API Handler</p> <pre><code>from ddeutil.workflow.traces import RestAPIHandler, Trace\n\n# Datadog integration\nhandler = RestAPIHandler(\n    type=\"restapi\",\n    service_type=\"datadog\",\n    api_url=\"https://http-intake.logs.datadoghq.com/v1/input\",\n    api_key=\"your-datadog-api-key\",\n    timeout=10.0,\n    max_retries=3\n)\n\ntrace = Trace(\n    run_id=\"workflow-123\",\n    handlers=[handler]\n)\ntrace.info(\"Sending logs to Datadog\")\n</code></pre> <p>Supported service types: - <code>datadog</code>: Datadog log ingestion - <code>grafana</code>: Grafana Loki - <code>cloudwatch</code>: AWS CloudWatch Logs - <code>generic</code>: Generic REST API</p>"},{"location":"api/traces/#elastichandler","title":"<code>ElasticHandler</code>","text":"<p>High-performance Elasticsearch logging with bulk indexing and search capabilities.</p> <p>Elasticsearch Handler</p> <pre><code>from ddeutil.workflow.traces import ElasticHandler, Trace\n\nhandler = ElasticHandler(\n    type=\"elastic\",\n    hosts=[\"http://localhost:9200\"],\n    username=\"elastic\",\n    password=\"password\",\n    index=\"workflow-traces\",\n    timeout=30.0,\n    max_retries=3\n)\n\ntrace = Trace(\n    run_id=\"workflow-123\",\n    handlers=[handler]\n)\ntrace.info(\"Elasticsearch logging initialized\")\n</code></pre>"},{"location":"api/traces/#data-models","title":"Data Models","text":""},{"location":"api/traces/#tracedata","title":"<code>TraceData</code>","text":"<p>Complete trace data including stdout, stderr, and metadata.</p> Field Type Description <code>stdout</code> str Standard output content <code>stderr</code> str Standard error content <code>meta</code> list[Metadata] List of trace metadata entries"},{"location":"api/traces/#message","title":"<code>Message</code>","text":"<p>Message model with prefix parsing and emoji support.</p> <p>Message Formatting</p> <pre><code>from ddeutil.workflow.traces import Message\n\n# Parse message with prefix\nmsg = Message.from_str(\"[WORKFLOW]: Starting execution\")\nprint(msg.name)  # \"WORKFLOW\"\nprint(msg.message)  # \"Starting execution\"\n\n# Prepare with emoji\nformatted = msg.prepare({\"log_add_emoji\": True})\nprint(formatted)  # \"\ud83c\udfc3 [WORKFLOW]: Starting execution\"\n</code></pre>"},{"location":"api/traces/#logging-levels","title":"Logging Levels","text":"<p>The traces system supports standard logging levels:</p> <ul> <li><code>debug</code>: Detailed diagnostic information</li> <li><code>info</code>: General information about workflow progress</li> <li><code>warning</code>: Warning messages for potential issues</li> <li><code>error</code>: Error messages for failed operations</li> <li><code>exception</code>: Critical errors with exception details</li> </ul>"},{"location":"api/traces/#async-support","title":"Async Support","text":"<p>All handlers support asynchronous logging for non-blocking operations:</p> <p>Async Logging</p> <pre><code>import asyncio\nfrom ddeutil.workflow.traces import get_trace\n\nasync def async_workflow():\n    trace = get_trace(\"async-workflow-123\")\n\n    await trace.amit(\"Async workflow started\", level=\"info\")\n    await trace.amit(\"Processing async operation\", level=\"debug\")\n    await trace.amit(\"Async operation failed\", level=\"error\")\n\nasyncio.run(async_workflow())\n</code></pre>"},{"location":"api/traces/#buffer-support","title":"Buffer Support","text":"<p>The <code>Trace</code> supports buffered logging for high-performance scenarios:</p> <p>Buffered Logging</p> <pre><code>from ddeutil.workflow.traces import get_trace\n\n# Use context manager for buffered logging\nwith get_trace(\"workflow-123\") as trace:\n    # All logs are buffered and flushed at exit\n    trace.info(\"Workflow started\")\n    trace.debug(\"Processing stage 1\")\n    trace.info(\"Processing stage 2\")\n    trace.info(\"Workflow completed\")\n\n# All logs are automatically flushed when exiting the context\n</code></pre>"},{"location":"api/traces/#factory-function","title":"Factory Function","text":""},{"location":"api/traces/#get_trace","title":"<code>get_trace</code>","text":"<p>Factory function that returns a <code>Trace</code> instance with handlers configured from the core configuration.</p> <p>Dynamic Trace Creation</p> <pre><code>from ddeutil.workflow.traces import get_trace\n\n# Automatically selects handlers based on configuration\ntrace = get_trace(\n    run_id=\"workflow-123\",\n    parent_run_id=\"parent-456\",\n    extras={\"custom_config\": \"value\"}\n)\n\n# Configuration determines handler types:\n# - Console handler for immediate output\n# - File handler for persistent storage\n# - SQLite handler for database storage\n# - REST API handler for external services\n# - Elasticsearch handler for distributed logging\n</code></pre>"},{"location":"api/traces/#configuration","title":"Configuration","text":"<p>Trace behavior is controlled by configuration settings:</p> Setting Description <code>trace_handlers</code> List of handler configurations <code>log_format</code> Console log message format <code>log_format_file</code> File log message format <code>log_datetime_format</code> Datetime format for logs <code>log_tz</code> Timezone for log timestamps <code>log_add_emoji</code> Whether to include emojis in messages <code>logs_trace_frame_layer</code> Stack frame layer for metadata"},{"location":"api/traces/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>ConsoleHandler: Best for development and debugging</li> <li>FileHandler: Good for small to medium-scale deployments</li> <li>SQLiteHandler: Recommended for production with concurrent access</li> <li>RestAPIHandler: Ideal for integration with external monitoring systems</li> <li>ElasticHandler: Best for large-scale distributed deployments with search requirements</li> </ul> <p>Choose the appropriate handler(s) based on your deployment scale, persistence requirements, and monitoring needs.</p>"},{"location":"api/utils/","title":"Utilities","text":"<p>The Utils module provides essential utility functions for workflow operations including ID generation, datetime handling, data transformation, and cross-product operations.</p>"},{"location":"api/utils/#id-generation","title":"ID Generation","text":""},{"location":"api/utils/#gen_id","title":"<code>gen_id</code>","text":"<p>Generate running ID for tracking workflow executions. Uses MD5 algorithm or simple mode based on configuration.</p> <p>ID Generation</p> <pre><code>from ddeutil.workflow.utils import gen_id\n\n# Simple hash-based ID\nid1 = gen_id(\"workflow-name\")\n# Output: \"a1b2c3d4e5\"\n\n# Case-insensitive ID\nid2 = gen_id(\"WORKFLOW-NAME\", sensitive=False)\n# Output: \"a1b2c3d4e5\" (same as lowercase)\n\n# Unique ID with timestamp\nid3 = gen_id(\"workflow-name\", unique=True)\n# Output: \"20240115103000123456Ta1b2c3d4e5\"\n\n# Simple mode (configurable)\nid4 = gen_id(\"workflow-name\", simple_mode=True)\n# Output: \"20240115103000123456Ta1b2c3d4e5\"\n</code></pre>"},{"location":"api/utils/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>value</code> Any Required Value to generate ID from <code>sensitive</code> bool <code>True</code> Case-sensitive ID generation <code>unique</code> bool <code>False</code> Add timestamp for uniqueness <code>simple_mode</code> bool | None <code>None</code> Use simple mode (from config) <code>extras</code> dict | None <code>None</code> Override config values"},{"location":"api/utils/#default_gen_id","title":"<code>default_gen_id</code>","text":"<p>Generate a default running ID for manual executions.</p> <p>Default ID</p> <pre><code>from ddeutil.workflow.utils import default_gen_id\n\n# Generate default ID with timestamp\ndefault_id = default_gen_id()\n# Output: \"20240115103000123456Tmanual123abc\"\n</code></pre>"},{"location":"api/utils/#cut_id","title":"<code>cut_id</code>","text":"<p>Cut running ID to specified length for display purposes.</p> <p>ID Cutting</p> <pre><code>from ddeutil.workflow.utils import cut_id\n\n# Full ID: \"20240115103000123456T1354680202\"\nshort_id = cut_id(\"20240115103000123456T1354680202\")\n# Output: \"202401151030680202\"\n\n# Custom length\ncustom_id = cut_id(\"20240115103000123456T1354680202\", num=8)\n# Output: \"2024011580202\"\n</code></pre>"},{"location":"api/utils/#datetime-utilities","title":"DateTime Utilities","text":""},{"location":"api/utils/#get_dt_now","title":"<code>get_dt_now</code>","text":"<p>Get current datetime with timezone and offset support.</p> <p>Current DateTime</p> <pre><code>from ddeutil.workflow.utils import get_dt_now\nfrom zoneinfo import ZoneInfo\n\n# Current UTC time\nnow_utc = get_dt_now()\n\n# Current time in specific timezone\nnow_asia = get_dt_now(tz=ZoneInfo(\"Asia/Bangkok\"))\n\n# Current time with offset\nnow_offset = get_dt_now(offset=3600)  # +1 hour\n</code></pre>"},{"location":"api/utils/#get_d_now","title":"<code>get_d_now</code>","text":"<p>Get current date with timezone and offset support.</p> <p>Current Date</p> <pre><code>from ddeutil.workflow.utils import get_d_now\n\n# Current date\ntoday = get_d_now()\n\n# Date with timezone\ntoday_tz = get_d_now(tz=ZoneInfo(\"America/New_York\"))\n</code></pre>"},{"location":"api/utils/#replace_sec","title":"<code>replace_sec</code>","text":"<p>Replace seconds and microseconds in datetime to zero.</p> <p>Time Replacement</p> <pre><code>from ddeutil.workflow.utils import replace_sec\nfrom datetime import datetime\n\ndt = datetime(2024, 1, 15, 10, 30, 45, 123456)\nclean_dt = replace_sec(dt)\n# Output: datetime(2024, 1, 15, 10, 30, 0, 0)\n</code></pre>"},{"location":"api/utils/#clear_tz","title":"<code>clear_tz</code>","text":"<p>Remove timezone information from datetime object.</p> <p>Timezone Removal</p> <pre><code>from ddeutil.workflow.utils import clear_tz\n\n# Datetime with timezone\ndt_with_tz = datetime.now(tz=ZoneInfo(\"UTC\"))\ndt_naive = clear_tz(dt_with_tz)\n# Output: datetime without timezone info\n</code></pre>"},{"location":"api/utils/#get_diff_sec","title":"<code>get_diff_sec</code>","text":"<p>Get difference in seconds between datetime and current time.</p> <p>Time Difference</p> <pre><code>from ddeutil.workflow.utils import get_diff_sec\nfrom datetime import datetime, timedelta\n\nfuture_dt = datetime.now() + timedelta(minutes=5)\ndiff = get_diff_sec(future_dt)\n# Output: 300 (seconds)\n</code></pre>"},{"location":"api/utils/#datetime-checking","title":"Date/Time Checking","text":""},{"location":"api/utils/#reach_next_minute","title":"<code>reach_next_minute</code>","text":"<p>Check if datetime is in the next minute relative to current time.</p> <p>Minute Check</p> <pre><code>from ddeutil.workflow.utils import reach_next_minute\nfrom datetime import datetime, timedelta\n\nfuture_dt = datetime.now() + timedelta(minutes=2)\nis_next_minute = reach_next_minute(future_dt)\n# Output: True\n</code></pre>"},{"location":"api/utils/#wait_until_next_minute","title":"<code>wait_until_next_minute</code>","text":"<p>Wait with sleep until the next minute with optional offset.</p> <p>Wait Function</p> <pre><code>from ddeutil.workflow.utils import wait_until_next_minute\nfrom datetime import datetime\n\n# Wait until next minute\nwait_until_next_minute(datetime.now())\n\n# Wait with 2 second offset\nwait_until_next_minute(datetime.now(), second=2)\n</code></pre>"},{"location":"api/utils/#delay","title":"<code>delay</code>","text":"<p>Delay execution with random offset for load distribution.</p> <p>Random Delay</p> <pre><code>from ddeutil.workflow.utils import delay\n\n# Delay 1 second + random 0.00-0.99 seconds\ndelay(1.0)\n\n# Just random delay\ndelay()  # 0.00-0.99 seconds\n</code></pre>"},{"location":"api/utils/#data-transformation","title":"Data Transformation","text":""},{"location":"api/utils/#to_train","title":"<code>to_train</code>","text":"<p>Convert camelCase to train-case (kebab-case).</p> <p>Case Conversion</p> <pre><code>from ddeutil.workflow.utils import to_train\n\n# Convert camelCase\nresult = to_train(\"camelCaseString\")\n# Output: \"camel-case-string\"\n\n# Convert PascalCase\nresult = to_train(\"PascalCaseString\")\n# Output: \"pascal-case-string\"\n</code></pre>"},{"location":"api/utils/#prepare_newline","title":"<code>prepare_newline</code>","text":"<p>Prepare newline characters in strings for consistent formatting.</p> <p>Newline Preparation</p> <pre><code>from ddeutil.workflow.utils import prepare_newline\n\n# Replace custom markers with newlines\ntext = \"Line 1||Line 2||Line 3\"\nresult = prepare_newline(text)\n# Output: \"Line 1\\nLine 2\\nLine 3\"\n</code></pre>"},{"location":"api/utils/#filter_func","title":"<code>filter_func</code>","text":"<p>Filter out function objects from data structures, replacing with function names.</p> <p>Function Filtering</p> <pre><code>from ddeutil.workflow.utils import filter_func\n\ndef my_function():\n    return \"hello\"\n\ndata = {\n    \"name\": \"test\",\n    \"func\": my_function,\n    \"nested\": {\n        \"callback\": lambda x: x * 2\n    }\n}\n\nfiltered = filter_func(data)\n# Output: {\n#     \"name\": \"test\",\n#     \"func\": \"my_function\",\n#     \"nested\": {\"callback\": \"&lt;lambda&gt;\"}\n# }\n</code></pre>"},{"location":"api/utils/#dump_all","title":"<code>dump_all</code>","text":"<p>Recursively dump all nested Pydantic models to dictionaries.</p> <p>Model Dumping</p> <pre><code>from ddeutil.workflow.utils import dump_all\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclass Team(BaseModel):\n    name: str\n    members: list[User]\n\nteam = Team(\n    name=\"Dev Team\",\n    members=[User(name=\"Alice\", age=30), User(name=\"Bob\", age=25)]\n)\n\n# Dump all nested models\nresult = dump_all(team)\n# Output: Plain dict with all nested models converted\n</code></pre>"},{"location":"api/utils/#matrix-operations","title":"Matrix Operations","text":""},{"location":"api/utils/#cross_product","title":"<code>cross_product</code>","text":"<p>Generate cross product of matrix values for parameter combinations.</p> <p>Cross Product</p> <pre><code>from ddeutil.workflow.utils import cross_product\n\nmatrix = {\n    \"env\": [\"dev\", \"prod\"],\n    \"version\": [\"1.0\", \"2.0\"],\n    \"region\": [\"us\", \"eu\"]\n}\n\n# Generate all combinations\nfor combination in cross_product(matrix):\n    print(combination)\n\n# Output:\n# {\"env\": \"dev\", \"version\": \"1.0\", \"region\": \"us\"}\n# {\"env\": \"dev\", \"version\": \"1.0\", \"region\": \"eu\"}\n# {\"env\": \"dev\", \"version\": \"2.0\", \"region\": \"us\"}\n# {\"env\": \"dev\", \"version\": \"2.0\", \"region\": \"eu\"}\n# {\"env\": \"prod\", \"version\": \"1.0\", \"region\": \"us\"}\n# {\"env\": \"prod\", \"version\": \"1.0\", \"region\": \"eu\"}\n# {\"env\": \"prod\", \"version\": \"2.0\", \"region\": \"us\"}\n# {\"env\": \"prod\", \"version\": \"2.0\", \"region\": \"eu\"}\n</code></pre>"},{"location":"api/utils/#file-operations","title":"File Operations","text":""},{"location":"api/utils/#make_exec","title":"<code>make_exec</code>","text":"<p>Make a file executable by changing its permissions.</p> <p>File Permissions</p> <pre><code>from ddeutil.workflow.utils import make_exec\nfrom pathlib import Path\n\n# Make script executable\nscript_path = Path(\"script.sh\")\nmake_exec(script_path)\n\n# Also works with string paths\nmake_exec(\"/path/to/script.py\")\n</code></pre>"},{"location":"api/utils/#object-utilities","title":"Object Utilities","text":""},{"location":"api/utils/#obj_name","title":"<code>obj_name</code>","text":"<p>Get the name of an object, class, or string.</p> <p>Object Names</p> <pre><code>from ddeutil.workflow.utils import obj_name\n\nclass MyClass:\n    pass\n\ninstance = MyClass()\n\n# Get class name from instance\nname1 = obj_name(instance)  # \"MyClass\"\n\n# Get class name from class\nname2 = obj_name(MyClass)   # \"MyClass\"\n\n# Return string as-is\nname3 = obj_name(\"CustomName\")  # \"CustomName\"\n\n# Handle None\nname4 = obj_name(None)      # None\n</code></pre>"},{"location":"api/utils/#configuration","title":"Configuration","text":"<p>Utility functions can be configured through environment variables:</p> Variable Default Description <code>WORKFLOW_CORE_WORKFLOW_ID_SIMPLE_MODE</code> <code>false</code> Enable simple ID generation mode <code>WORKFLOW_CORE_TZ</code> <code>UTC</code> Default timezone for datetime operations <p>Performance Notes</p> <ul> <li>ID generation functions are optimized for high-frequency use</li> <li>DateTime utilities handle timezone conversions efficiently</li> <li>Cross product operations use generators for memory efficiency</li> <li>Function filtering preserves object structure while removing callables</li> </ul>"},{"location":"api/workflow/","title":"Workflow API Reference","text":"<p>The Workflow module provides the core orchestration functionality for the workflow system. It manages job execution, scheduling, parameter handling, and provides comprehensive execution capabilities for complex workflows.</p>"},{"location":"api/workflow/#overview","title":"Overview","text":"<p>The workflow system implements timeout strategy at the workflow execution layer because the main purpose is to use Workflow as an orchestrator for complex job execution scenarios. The system supports both immediate execution and scheduled execution via cron-like expressions.</p>"},{"location":"api/workflow/#workflow-execution-flow","title":"Workflow Execution Flow","text":"<pre><code>flowchart TD\n    A[\"Workflow.execute()\"] --&gt; B[\"Generate run_id\"]\n    B --&gt; C[\"Initialize trace and context\"]\n    C --&gt; D[\"Parameterize input params\"]\n    D --&gt; E[\"Create job queue\"]\n    E --&gt; F[\"Initialize ThreadPoolExecutor\"]\n    F --&gt; G[\"Process job queue\"]\n\n    G --&gt; H{\"Job dependency check\"}\n    H --&gt;|WAIT| I[\"Re-queue job with backoff\"]\n    H --&gt;|FAILED| J[\"Return FAILED Result\"]\n    H --&gt;|SKIP| K[\"Mark job as skipped\"]\n    H --&gt;|SUCCESS| L[\"Submit job for execution\"]\n\n    I --&gt; M{\"Timeout check\"}\n    M --&gt;|No timeout| G\n    M --&gt;|Timeout| N[\"Cancel all futures\"]\n    N --&gt; O[\"Return TIMEOUT Result\"]\n\n    L --&gt; P{\"Parallel execution?\"}\n    P --&gt;|Yes| Q[\"Execute in thread pool\"]\n    P --&gt;|No| R[\"Execute sequentially\"]\n\n    Q --&gt; S[\"Collect job results\"]\n    R --&gt; S\n    S --&gt; T[\"Validate final status\"]\n    T --&gt; U[\"Return Result\"]\n\n    K --&gt; S\n    J --&gt; U\n    O --&gt; U\n\n    style A fill:#e1f5fe\n    style U fill:#c8e6c9\n    style J fill:#ffcdd2\n    style O fill:#ffcdd2</code></pre>"},{"location":"api/workflow/#workflow-release-flow","title":"Workflow Release Flow","text":"<pre><code>flowchart TD\n    A[\"Workflow.release()\"] --&gt; B[\"Generate run_id\"]\n    B --&gt; C[\"Validate release datetime\"]\n    C --&gt; D[\"Initialize trace and context\"]\n    D --&gt; E[\"Create release data\"]\n    E --&gt; F[\"Template parameters with release data\"]\n    F --&gt; G[\"Execute workflow\"]\n    G --&gt; H[\"Write audit log\"]\n    H --&gt; I[\"Return Result\"]\n\n    style A fill:#e1f5fe\n    style I fill:#c8e6c9</code></pre>"},{"location":"api/workflow/#quick-start","title":"Quick Start","text":"<pre><code>from ddeutil.workflow import Workflow\nfrom datetime import datetime\n\n# Load workflow from configuration\nworkflow = Workflow.from_conf('data-pipeline')\n\n# Execute immediately\nresult = workflow.execute({\n    'input_path': '/data/input',\n    'output_path': '/data/output',\n    'processing_date': '2024-01-01'\n})\n\n# Schedule for later execution\nrelease_time = datetime(2024, 1, 1, 9, 0, 0)\nresult = workflow.release(\n    release=release_time,\n    params={'mode': 'batch'}\n)\n\nprint(f\"Workflow status: {result.status}\")\n</code></pre>"},{"location":"api/workflow/#classes","title":"Classes","text":""},{"location":"api/workflow/#workflow","title":"Workflow","text":"<p>Main workflow orchestration model for job and schedule management.</p> <p>The Workflow class is the core component of the workflow orchestration system. It manages job execution, scheduling via cron expressions, parameter handling, and provides comprehensive execution capabilities for complex workflows.</p>"},{"location":"api/workflow/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>extras</code> <code>dict</code> <code>{}</code> Extra parameters for overriding configuration values <code>name</code> <code>str</code> - Unique workflow identifier <code>desc</code> <code>str \\| None</code> <code>None</code> Workflow description supporting markdown content <code>params</code> <code>dict[str, Param]</code> <code>{}</code> Parameter definitions for the workflow <code>on</code> <code>Event</code> <code>Event()</code> Event definitions for the workflow <code>jobs</code> <code>dict[str, Job]</code> <code>{}</code> Collection of jobs within this workflow <code>tags</code> <code>list[str]</code> <code>[]</code> List of tags for grouping workflows <code>created_at</code> <code>datetime</code> <code>get_dt_now()</code> Workflow creation timestamp <code>updated_dt</code> <code>datetime</code> <code>get_dt_now()</code> Workflow last update timestamp"},{"location":"api/workflow/#methods","title":"Methods","text":""},{"location":"api/workflow/#from_confname-pathnone-extrasnone","title":"<code>from_conf(name, *, path=None, extras=None)</code>","text":"<p>Create Workflow instance from configuration file.</p> <p>Parameters: - <code>name</code> (str): Workflow name to load from configuration - <code>path</code> (Path, optional): Optional custom configuration path to search - <code>extras</code> (dict, optional): Additional parameters to override configuration values</p> <p>Returns: - <code>Workflow</code>: Validated Workflow instance loaded from configuration</p> <p>Raises: - <code>ValueError</code>: If workflow type doesn't match or configuration invalid - <code>FileNotFoundError</code>: If workflow configuration file not found</p>"},{"location":"api/workflow/#executeparams-run_idnone-parent_run_idnone-eventnone-timeout3600-max_job_parallel2","title":"<code>execute(params, *, run_id=None, parent_run_id=None, event=None, timeout=3600, max_job_parallel=2)</code>","text":"<p>Execute the workflow with provided parameters.</p> <p>Parameters: - <code>params</code> (dict): Parameter values for workflow execution - <code>run_id</code> (str, optional): Unique identifier for this execution run - <code>parent_run_id</code> (str, optional): Parent workflow run identifier - <code>event</code> (Event, optional): Threading event for cancellation control - <code>timeout</code> (float): Maximum execution time in seconds - <code>max_job_parallel</code> (int): Maximum number of concurrent jobs</p> <p>Returns: - <code>Result</code>: Execution result with status and output data</p>"},{"location":"api/workflow/#releaserelease-params-release_typenormal-run_idnone-parent_run_idnone-auditnone-override_log_namenone-resultnone-timeout600-excludednone","title":"<code>release(release, params, *, release_type='normal', run_id=None, parent_run_id=None, audit=None, override_log_name=None, result=None, timeout=600, excluded=None)</code>","text":"<p>Release workflow execution at specified datetime.</p> <p>Parameters: - <code>release</code> (datetime): Scheduled release datetime - <code>params</code> (dict): Parameter values for execution - <code>release_type</code> (ReleaseType): Type of release execution - <code>run_id</code> (str, optional): Unique run identifier - <code>parent_run_id</code> (str, optional): Parent run identifier - <code>audit</code> (Audit, optional): Audit logging configuration - <code>override_log_name</code> (str, optional): Custom log name override - <code>result</code> (Result, optional): Pre-existing result context - <code>timeout</code> (int): Execution timeout in seconds - <code>excluded</code> (list[str], optional): Jobs to exclude from execution</p> <p>Returns: - <code>Result</code>: Release execution result</p>"},{"location":"api/workflow/#reruncontext-run_idnone-eventnone-timeout3600-max_job_parallel2","title":"<code>rerun(context, *, run_id=None, event=None, timeout=3600, max_job_parallel=2)</code>","text":"<p>Re-execute workflow with previous context data.</p> <p>Parameters: - <code>context</code> (dict): Previous execution context - <code>run_id</code> (str, optional): Unique run identifier - <code>event</code> (Event, optional): Threading event for cancellation control - <code>timeout</code> (float): Maximum execution time in seconds - <code>max_job_parallel</code> (int): Maximum number of concurrent jobs</p> <p>Returns: - <code>Result</code>: Re-execution result</p>"},{"location":"api/workflow/#execute_jobjob-run_id-context-parent_run_idnone-eventnone","title":"<code>execute_job(job, run_id, context, *, parent_run_id=None, event=None)</code>","text":"<p>Execute a single job within the workflow.</p> <p>Parameters: - <code>job</code> (Job): Job instance to execute - <code>run_id</code> (str): Execution run identifier - <code>context</code> (dict): Execution context - <code>parent_run_id</code> (str, optional): Parent run identifier - <code>event</code> (Event, optional): Threading event for cancellation control</p> <p>Returns: - <code>tuple[Status, DictData]</code>: Job execution status and context</p>"},{"location":"api/workflow/#jobname","title":"<code>job(name)</code>","text":"<p>Get a job by name or ID.</p> <p>Parameters: - <code>name</code> (str): Job name or ID</p> <p>Returns: - <code>Job</code>: Job instance</p> <p>Raises: - <code>ValueError</code>: If job not found</p>"},{"location":"api/workflow/#parameterizeparams","title":"<code>parameterize(params)</code>","text":"<p>Prepare and validate parameters for execution.</p> <p>Parameters: - <code>params</code> (dict): Input parameters</p> <p>Returns: - <code>dict</code>: Validated and prepared parameters</p> <p>Raises: - <code>WorkflowError</code>: If required parameters are missing</p>"},{"location":"api/workflow/#validate_releasedt","title":"<code>validate_release(dt)</code>","text":"<p>Validate release datetime against workflow schedule.</p> <p>Parameters: - <code>dt</code> (datetime): Release datetime to validate</p> <p>Returns: - <code>datetime</code>: Validated release datetime</p> <p>Raises: - <code>WorkflowError</code>: If release datetime not supported</p>"},{"location":"api/workflow/#releasetype","title":"ReleaseType","text":"<p>Release type enumeration for workflow execution modes.</p>"},{"location":"api/workflow/#values","title":"Values","text":"<ul> <li><code>NORMAL</code>: Standard workflow release execution</li> <li><code>RERUN</code>: Re-execution of previously failed workflow</li> <li><code>EVENT</code>: Event-triggered workflow execution</li> <li><code>FORCE</code>: Forced execution bypassing normal conditions</li> </ul>"},{"location":"api/workflow/#usage-examples","title":"Usage Examples","text":""},{"location":"api/workflow/#basic-workflow-creation-and-execution","title":"Basic Workflow Creation and Execution","text":"<pre><code>from ddeutil.workflow import Workflow\n\n# Load workflow from configuration\nworkflow = Workflow.from_conf('data-pipeline')\n\n# Execute with parameters\nresult = workflow.execute({\n    'input_path': '/data/input',\n    'output_path': '/data/output',\n    'processing_date': '2024-01-01'\n})\n\nif result.status == 'SUCCESS':\n    print(\"Workflow completed successfully\")\n    print(f\"Output: {result.context.get('outputs', {})}\")\nelse:\n    print(f\"Workflow failed: {result.errors}\")\n</code></pre>"},{"location":"api/workflow/#workflow-with-custom-configuration","title":"Workflow with Custom Configuration","text":"<pre><code>from pathlib import Path\nfrom ddeutil.workflow import Workflow\n\n# Load with custom path and extras\nworkflow = Workflow.from_conf(\n    'data-pipeline',\n    path=Path('./custom-configs'),\n    extras={'environment': 'production'}\n)\n\n# Execute with timeout and job parallelism control\nresult = workflow.execute(\n    params={'batch_size': 1000},\n    timeout=1800,  # 30 minutes\n    max_job_parallel=4\n)\n</code></pre>"},{"location":"api/workflow/#scheduled-workflow-execution","title":"Scheduled Workflow Execution","text":"<pre><code>from datetime import datetime\nfrom ddeutil.workflow import Workflow, NORMAL\n\nworkflow = Workflow.from_conf('scheduled-pipeline')\n\n# Schedule for specific time\nrelease_time = datetime(2024, 1, 1, 9, 0, 0)\nresult = workflow.release(\n    release=release_time,\n    params={'mode': 'batch'},\n    release_type=NORMAL,\n    timeout=3600\n)\n</code></pre>"},{"location":"api/workflow/#workflow-re-execution","title":"Workflow Re-execution","text":"<pre><code>from ddeutil.workflow import Workflow\n\nworkflow = Workflow.from_conf('failed-pipeline')\n\n# Re-execute failed workflow\nprevious_context = {\n    'params': {'input_file': '/data/input.csv'},\n    'jobs': {\n        'setup': {'status': 'SUCCESS'},\n        'process': {'status': 'FAILED', 'errors': {...}}\n    }\n}\n\nresult = workflow.rerun(\n    context=previous_context,\n    timeout=1800\n)\n</code></pre>"},{"location":"api/workflow/#error-handling","title":"Error Handling","text":"<pre><code>from ddeutil.workflow import Workflow, WorkflowError\n\ntry:\n    workflow = Workflow.from_conf('my-workflow')\n    result = workflow.execute({'param1': 'value1'})\n\n    if result.status == 'FAILED':\n        for error in result.errors:\n            print(f\"Error in {error.name}: {error.message}\")\n\nexcept WorkflowError as e:\n    print(f\"Workflow execution error: {e}\")\nexcept FileNotFoundError:\n    print(\"Workflow configuration file not found\")\n</code></pre>"},{"location":"api/workflow/#advanced-workflow-patterns","title":"Advanced Workflow Patterns","text":""},{"location":"api/workflow/#conditional-execution","title":"Conditional Execution","text":"<pre><code>from ddeutil.workflow import Workflow\n\nworkflow = Workflow.from_conf('conditional-pipeline')\n\n# Execute based on conditions\nif workflow.params.get('environment') == 'production':\n    result = workflow.execute({\n        'mode': 'production',\n        'backup_enabled': True\n    })\nelse:\n    result = workflow.execute({\n        'mode': 'development',\n        'backup_enabled': False\n    })\n</code></pre>"},{"location":"api/workflow/#workflow-chaining","title":"Workflow Chaining","text":"<pre><code>from ddeutil.workflow import Workflow\n\n# Execute workflows in sequence\nworkflows = ['setup', 'process', 'cleanup']\n\nfor wf_name in workflows:\n    workflow = Workflow.from_conf(wf_name)\n    result = workflow.execute({'step': wf_name})\n\n    if result.status != 'SUCCESS':\n        print(f\"Workflow {wf_name} failed, stopping chain\")\n        break\n</code></pre>"},{"location":"api/workflow/#parallel-workflow-execution","title":"Parallel Workflow Execution","text":"<pre><code>import asyncio\nfrom ddeutil.workflow import Workflow\n\nasync def execute_workflow(name, params):\n    workflow = Workflow.from_conf(name)\n    return workflow.execute(params)\n\n# Execute multiple workflows in parallel\nasync def run_parallel_workflows():\n    tasks = [\n        execute_workflow('workflow-a', {'param': 'value1'}),\n        execute_workflow('workflow-b', {'param': 'value2'}),\n        execute_workflow('workflow-c', {'param': 'value3'})\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n\n# Run parallel workflows\nresults = asyncio.run(run_parallel_workflows())\n</code></pre>"},{"location":"api/workflow/#yaml-configuration","title":"YAML Configuration","text":""},{"location":"api/workflow/#basic-workflow-configuration","title":"Basic Workflow Configuration","text":"<pre><code>my-workflow:\n  type: Workflow\n  desc: |\n    Sample data processing workflow\n    with multiple stages\n\n  params:\n    input_file:\n      type: str\n      description: Input file path\n    environment:\n      type: str\n      default: development\n\n  on:\n    - cron: \"0 9 * * 1-5\"  # Weekdays at 9 AM\n      timezone: \"UTC\"\n\n  jobs:\n    data-ingestion:\n      runs-on:\n        type: local\n      stages:\n        - name: \"Download data\"\n          bash: |\n            wget ${{ params.data_url }} -O /tmp/data.csv\n\n        - name: \"Validate data\"\n          run: |\n            import pandas as pd\n            df = pd.read_csv('/tmp/data.csv')\n            assert len(df) &gt; 0, \"Data file is empty\"\n\n    data-processing:\n      needs: [data-ingestion]\n      strategy:\n        matrix:\n          env: [dev, staging, prod]\n        max-parallel: 2\n      stages:\n        - name: \"Process data for ${{ matrix.env }}\"\n          run: |\n            process_data(env='${{ matrix.env }}')\n</code></pre>"},{"location":"api/workflow/#advanced-configuration-with-dependencies","title":"Advanced Configuration with Dependencies","text":"<pre><code>complex-workflow:\n  type: Workflow\n\n  params:\n    start_date: datetime\n    end_date: datetime\n    batch_size:\n      type: int\n      default: 1000\n\n  jobs:\n    setup:\n      stages:\n        - name: \"Initialize workspace\"\n          bash: mkdir -p /tmp/workflow-data\n\n    parallel-processing:\n      needs: [setup]\n      strategy:\n        matrix:\n          region: [us-east, us-west, eu-central]\n          shard: [1, 2, 3, 4]\n        max-parallel: 4\n        fail-fast: false\n      stages:\n        - name: \"Process ${{ matrix.region }}-${{ matrix.shard }}\"\n          bash: |\n            process_region.sh \\\n              --region=${{ matrix.region }} \\\n              --shard=${{ matrix.shard }} \\\n              --start-date=${{ params.start_date }} \\\n              --end-date=${{ params.end_date }}\n\n    aggregation:\n      needs: [parallel-processing]\n      stages:\n        - name: \"Aggregate results\"\n          run: |\n            aggregate_results()\n</code></pre>"},{"location":"api/workflow/#workflow-with-error-handling","title":"Workflow with Error Handling","text":"<pre><code>robust-workflow:\n  type: Workflow\n\n  params:\n    retry_count:\n      type: int\n      default: 3\n\n  jobs:\n    main-process:\n      stages:\n        - name: \"Attempt processing\"\n          retry: ${{ params.retry_count }}\n          run: |\n            process_data()\n\n    fallback:\n      needs: [main-process]\n      if: \"${{ needs.main-process.result == 'failure' }}\"\n      stages:\n        - name: \"Fallback processing\"\n          run: |\n            fallback_process()\n\n    cleanup:\n      needs: [main-process, fallback]\n      if: \"${{ always() }}\"\n      stages:\n        - name: \"Cleanup resources\"\n          run: |\n            cleanup_resources()\n</code></pre>"},{"location":"api/workflow/#scheduled-workflow-with-multiple-triggers","title":"Scheduled Workflow with Multiple Triggers","text":"<pre><code>scheduled-workflow:\n  type: Workflow\n\n  on:\n    # Daily at 2 AM UTC\n    - cron: \"0 2 * * *\"\n      timezone: \"UTC\"\n\n    # Every Monday at 9 AM local time\n    - cron: \"0 9 * * 1\"\n      timezone: \"America/New_York\"\n\n    # Manual trigger\n    - manual: true\n\n  jobs:\n    scheduled-task:\n      stages:\n        - name: \"Scheduled execution\"\n          run: |\n            print(f\"Executing at {datetime.now()}\")\n            perform_scheduled_task()\n</code></pre>"},{"location":"api/workflow/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"api/workflow/#fan-outfan-in-pattern","title":"Fan-Out/Fan-In Pattern","text":"<pre><code>fan-out-fan-in:\n  type: Workflow\n\n  jobs:\n    split:\n      stages:\n        - name: \"Split data\"\n          run: |\n            partitions = split_data()\n            result.outputs = {\"partitions\": partitions}\n\n    process-partitions:\n      needs: [split]\n      strategy:\n        matrix:\n          partition: ${{ fromJson(needs.split.outputs.partitions) }}\n        max-parallel: 5\n      stages:\n        - name: \"Process partition ${{ matrix.partition }}\"\n          run: |\n            process_partition(${{ matrix.partition }})\n\n    merge:\n      needs: [process-partitions]\n      stages:\n        - name: \"Merge results\"\n          run: |\n            merge_results()\n</code></pre>"},{"location":"api/workflow/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>circuit-breaker:\n  type: Workflow\n\n  jobs:\n    check-health:\n      stages:\n        - name: \"Check system health\"\n          run: |\n            health = check_system_health()\n            if not health.is_healthy:\n                raise Exception(\"System unhealthy\")\n\n    main-process:\n      needs: [check-health]\n      stages:\n        - name: \"Main processing\"\n          run: |\n            process_data()\n\n    fallback:\n      needs: [check-health]\n      if: \"${{ needs.check-health.result == 'failure' }}\"\n      stages:\n        - name: \"Fallback mode\"\n          run: |\n            fallback_processing()\n</code></pre>"},{"location":"api/workflow/#retry-with-exponential-backoff","title":"Retry with Exponential Backoff","text":"<pre><code>retry-pattern:\n  type: Workflow\n\n  jobs:\n    retry-job:\n      stages:\n        - name: \"Retry with backoff\"\n          retry: 5\n          run: |\n            import time\n            attempt = context.get('attempt', 1)\n            delay = 2 ** (attempt - 1)  # Exponential backoff\n            time.sleep(delay)\n\n            # Attempt the operation\n            result = risky_operation()\n</code></pre>"},{"location":"api/workflow/#configuration-management","title":"Configuration Management","text":""},{"location":"api/workflow/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>from ddeutil.workflow import Workflow\nfrom pathlib import Path\n\n# Load environment-specific configuration\nenv = os.getenv('ENVIRONMENT', 'development')\nconfig_path = Path(f'./configs/{env}')\n\nworkflow = Workflow.from_conf(\n    'data-pipeline',\n    path=config_path,\n    extras={\n        'environment': env,\n        'debug_mode': env == 'development'\n    }\n)\n</code></pre>"},{"location":"api/workflow/#dynamic-parameter-resolution","title":"Dynamic Parameter Resolution","text":"<pre><code>from ddeutil.workflow import Workflow\nfrom datetime import datetime, timedelta\n\n# Calculate dynamic parameters\ntoday = datetime.now()\nyesterday = today - timedelta(days=1)\n\nworkflow = Workflow.from_conf('daily-process')\n\nresult = workflow.execute({\n    'processing_date': yesterday.strftime('%Y-%m-%d'),\n    'output_date': today.strftime('%Y-%m-%d'),\n    'batch_size': 1000 if today.weekday() &lt; 5 else 500  # Smaller batches on weekends\n})\n</code></pre>"},{"location":"api/workflow/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"api/workflow/#workflow-metrics","title":"Workflow Metrics","text":"<pre><code>from ddeutil.workflow import Workflow\nimport time\n\nworkflow = Workflow.from_conf('monitored-workflow')\n\n# Track execution metrics\nstart_time = time.time()\nresult = workflow.execute({'param': 'value'})\nexecution_time = time.time() - start_time\n\n# Log metrics\nprint(f\"Workflow execution time: {execution_time:.2f} seconds\")\nprint(f\"Workflow status: {result.status}\")\nprint(f\"Jobs executed: {len(result.context.get('jobs', {}))}\")\n</code></pre>"},{"location":"api/workflow/#audit-trail","title":"Audit Trail","text":"<pre><code>from ddeutil.workflow import Workflow, FileAudit\nfrom datetime import datetime\n\nworkflow = Workflow.from_conf('audited-workflow')\n\n# Execute with audit logging\nresult = workflow.release(\n    release=datetime.now(),\n    params={'mode': 'production'},\n    audit=FileAudit,\n    timeout=3600\n)\n\n# Audit log contains:\n# - Execution metadata\n# - Input parameters\n# - Job execution results\n# - Timing information\n# - Error details (if any)\n</code></pre>"},{"location":"api/workflow/#best-practices","title":"Best Practices","text":""},{"location":"api/workflow/#1-workflow-design","title":"1. Workflow Design","text":"<ul> <li>Modularity: Break complex workflows into smaller, focused jobs</li> <li>Reusability: Design workflows to be reusable across different contexts</li> <li>Idempotency: Ensure workflows can be safely re-executed</li> <li>Error handling: Implement proper error handling and recovery mechanisms</li> </ul>"},{"location":"api/workflow/#2-parameter-management","title":"2. Parameter Management","text":"<ul> <li>Validation: Use parameter types and constraints for validation</li> <li>Defaults: Provide sensible default values for optional parameters</li> <li>Documentation: Document all parameters with clear descriptions</li> <li>Sensitivity: Handle sensitive parameters securely</li> </ul>"},{"location":"api/workflow/#3-scheduling","title":"3. Scheduling","text":"<ul> <li>Timezone awareness: Always specify timezones for scheduled executions</li> <li>Conflict avoidance: Avoid scheduling conflicts between related workflows</li> <li>Resource consideration: Consider system resources when scheduling</li> <li>Monitoring: Monitor scheduled execution success rates</li> </ul>"},{"location":"api/workflow/#4-performance","title":"4. Performance","text":"<ul> <li>Parallelization: Use matrix strategies for parallel job execution</li> <li>Resource limits: Set appropriate timeouts and resource limits</li> <li>Caching: Cache expensive operations when possible</li> <li>Optimization: Profile and optimize slow workflows</li> </ul>"},{"location":"api/workflow/#5-security","title":"5. Security","text":"<ul> <li>Access control: Implement proper access controls for workflows</li> <li>Secret management: Use secure methods for handling secrets</li> <li>Input validation: Validate all inputs to prevent injection attacks</li> <li>Audit logging: Enable audit logging for compliance</li> </ul>"},{"location":"api/workflow/#6-testing","title":"6. Testing","text":"<ul> <li>Unit testing: Test individual workflow components</li> <li>Integration testing: Test workflow interactions</li> <li>Error scenarios: Test failure modes and recovery</li> <li>Performance testing: Validate workflow performance under load</li> </ul>"},{"location":"api/workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/workflow/#common-issues","title":"Common Issues","text":""},{"location":"api/workflow/#workflow-not-found","title":"Workflow Not Found","text":"<pre><code># Problem: Workflow configuration file not found\ntry:\n    workflow = Workflow.from_conf('missing-workflow')\nexcept FileNotFoundError:\n    print(\"Check configuration path and workflow name\")\n    print(\"Available workflows:\", list_available_workflows())\n</code></pre>"},{"location":"api/workflow/#parameter-validation-errors","title":"Parameter Validation Errors","text":"<pre><code># Problem: Invalid parameter values\ntry:\n    result = workflow.execute({'invalid_param': 'value'})\nexcept ParamError as e:\n    print(f\"Parameter error: {e.param_name}\")\n    print(f\"Validation error: {e.validation_error}\")\n    print(\"Valid parameters:\", workflow.params.keys())\n</code></pre>"},{"location":"api/workflow/#timeout-issues","title":"Timeout Issues","text":"<pre><code># Problem: Workflow execution timeout\ntry:\n    result = workflow.execute(params, timeout=300)  # 5 minutes\nexcept WorkflowTimeoutError:\n    print(\"Workflow timed out - consider:\")\n    print(\"1. Increasing timeout value\")\n    print(\"2. Optimizing workflow performance\")\n    print(\"3. Breaking into smaller workflows\")\n</code></pre>"},{"location":"api/workflow/#job-dependency-issues","title":"Job Dependency Issues","text":"<pre><code># Problem: Job dependency failures\nresult = workflow.execute(params)\n\nif result.status == 'FAILED':\n    failed_jobs = [\n        job_id for job_id, job_result in result.context.get('jobs', {}).items()\n        if job_result.get('status') == 'FAILED'\n    ]\n    print(f\"Failed jobs: {failed_jobs}\")\n\n    # Check dependency chain\n    for job_id in failed_jobs:\n        job = workflow.jobs.get(job_id)\n        if job:\n            print(f\"Job {job_id} depends on: {job.needs}\")\n</code></pre>"},{"location":"api/workflow/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Enable debug logging: Set log level to DEBUG for detailed execution information</li> <li>Use trace logs: Check trace logs for detailed execution flow</li> <li>Validate configuration: Use configuration validation tools</li> <li>Test incrementally: Test individual jobs before running full workflow</li> <li>Monitor resources: Check system resources during execution</li> </ol>"},{"location":"api/workflow/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/workflow/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>WORKFLOW_CORE_CONF_PATH</code> <code>./conf</code> Configuration file path <code>WORKFLOW_CORE_TIMEOUT</code> <code>3600</code> Default workflow timeout <code>WORKFLOW_CORE_MAX_JOB_PARALLEL</code> <code>2</code> Default max parallel jobs <code>WORKFLOW_CORE_AUDIT_ENABLED</code> <code>true</code> Enable audit logging"},{"location":"api/workflow/#file-structure","title":"File Structure","text":"<pre><code>conf/\n\u251c\u2500\u2500 workflows/\n\u2502   \u251c\u2500\u2500 data-pipeline.yml\n\u2502   \u251c\u2500\u2500 etl-process.yml\n\u2502   \u2514\u2500\u2500 deployment.yml\n\u251c\u2500\u2500 jobs/\n\u2502   \u251c\u2500\u2500 common.yml\n\u2502   \u2514\u2500\u2500 templates.yml\n\u2514\u2500\u2500 stages/\n    \u251c\u2500\u2500 validation.yml\n    \u2514\u2500\u2500 processing.yml\n</code></pre>"},{"location":"deploy/docker/","title":"Deploy Docker","text":"<p>All Dockerfile will store at <code>./.container</code> folder.</p>"},{"location":"examples/api/","title":"Extract API","text":"<p>The call stage is the call Python function from any registry location.</p>"},{"location":"examples/api/#getting-started","title":"Getting Started","text":"<p>First, you should start create your call.</p>"},{"location":"examples/api/#restapi-integration-examples","title":"RestAPI Integration Examples","text":"<p>This guide demonstrates how to integrate REST APIs into your workflows using call stages and the workflow orchestration system.</p>"},{"location":"examples/api/#overview","title":"Overview","text":"<p>The workflow system provides powerful capabilities for:</p> <ul> <li>API data extraction: Fetch data from REST endpoints</li> <li>Authentication handling: OAuth, Bearer tokens, API keys</li> <li>Data transformation: Process and validate API responses</li> <li>Error handling: Retry logic and fallback strategies</li> <li>Batch processing: Handle large datasets efficiently</li> </ul>"},{"location":"examples/api/#basic-api-call","title":"Basic API Call","text":""},{"location":"examples/api/#simple-get-request","title":"Simple GET Request","text":"<p>Basic API Call</p> YAML WorkflowPython Function <pre><code>extract-user-data:\n  type: Workflow\n  params:\n    user_id: str\n    api_key: str\n  jobs:\n    get-user:\n      stages:\n        - name: \"Fetch User Data\"\n          id: fetch-user\n          uses: api/get-user-data@rest\n          with:\n            url: \"https://api.example.com/users/${{ params.user_id }}\"\n            headers:\n              Authorization: \"Bearer ${{ params.api_key }}\"\n              Content-Type: \"application/json\"\n</code></pre> <pre><code>from ddeutil.workflow import tag, Result\nimport requests\n\n@tag(\"api\", alias=\"get-user-data\")\ndef get_user_data(\n    url: str,\n    headers: dict = None,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Fetch user data from REST API.\"\"\"\n\n    response = requests.get(url, headers=headers or {})\n    response.raise_for_status()\n\n    data = response.json()\n    result.trace.info(f\"Retrieved user: {data.get('name')}\")\n\n    return {\n        \"user_data\": data,\n        \"status_code\": response.status_code\n    }\n</code></pre>"},{"location":"examples/api/#authentication-examples","title":"Authentication Examples","text":""},{"location":"examples/api/#oauth-20-bearer-token","title":"OAuth 2.0 Bearer Token","text":"<p>OAuth Authentication</p> YAML WorkflowPython Functions <pre><code>oauth-api-call:\n  type: Workflow\n  params:\n    client_id: str\n    client_secret: str\n    api_endpoint: str\n  jobs:\n    authenticate-and-call:\n      stages:\n        - name: \"Get OAuth Token\"\n          id: get-token\n          uses: auth/oauth-token@bearer\n          with:\n            token_url: \"https://api.example.com/oauth/token\"\n            client_id: ${{ params.client_id }}\n            client_secret: ${{ params.client_secret }}\n            scope: \"read:users\"\n\n        - name: \"Call Protected API\"\n          id: api-call\n          uses: api/authenticated-call@rest\n          with:\n            url: ${{ params.api_endpoint }}\n            token: ${{ stages.get-token.output.access_token }}\n</code></pre> <pre><code>from ddeutil.workflow import tag, Result\nimport requests\n\n@tag(\"auth\", alias=\"oauth-token\")\ndef get_oauth_token(\n    token_url: str,\n    client_id: str,\n    client_secret: str,\n    scope: str = None,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Get OAuth 2.0 access token.\"\"\"\n\n    data = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret\n    }\n\n    if scope:\n        data[\"scope\"] = scope\n\n    response = requests.post(token_url, data=data)\n    response.raise_for_status()\n\n    token_data = response.json()\n    result.trace.info(\"OAuth token obtained successfully\")\n\n    return {\n        \"access_token\": token_data[\"access_token\"],\n        \"expires_in\": token_data.get(\"expires_in\", 3600)\n    }\n\n@tag(\"api\", alias=\"authenticated-call\")\ndef authenticated_api_call(\n    url: str,\n    token: str,\n    method: str = \"GET\",\n    data: dict = None,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Make authenticated API call.\"\"\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.request(\n        method=method.upper(),\n        url=url,\n        headers=headers,\n        json=data\n    )\n    response.raise_for_status()\n\n    return {\n        \"data\": response.json(),\n        \"status_code\": response.status_code,\n        \"headers\": dict(response.headers)\n    }\n</code></pre>"},{"location":"examples/api/#advanced-api-patterns","title":"Advanced API Patterns","text":""},{"location":"examples/api/#pagination-handling","title":"Pagination Handling","text":"<p>Paginated API Calls</p> YAML WorkflowPython Function <pre><code>paginated-extraction:\n  type: Workflow\n  params:\n    base_url: str\n    api_key: str\n    page_size: int\n  jobs:\n    extract-all-pages:\n      stages:\n        - name: \"Extract Paginated Data\"\n          id: paginate\n          uses: api/paginated-extract@rest\n          with:\n            base_url: ${{ params.base_url }}\n            api_key: ${{ params.api_key }}\n            page_size: ${{ params.page_size }}\n            max_pages: 10\n</code></pre> <pre><code>@tag(\"api\", alias=\"paginated-extract\")\ndef extract_paginated_data(\n    base_url: str,\n    api_key: str,\n    page_size: int = 100,\n    max_pages: int = None,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Extract data from paginated API.\"\"\"\n\n    all_data = []\n    page = 1\n    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    while max_pages is None or page &lt;= max_pages:\n        url = f\"{base_url}?page={page}&amp;size={page_size}\"\n\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n\n        data = response.json()\n        items = data.get(\"items\", [])\n\n        if not items:\n            break\n\n        all_data.extend(items)\n        result.trace.info(f\"Extracted page {page}: {len(items)} items\")\n\n        # Check if there are more pages\n        if not data.get(\"has_next\", False):\n            break\n\n        page += 1\n\n    result.trace.info(f\"Total items extracted: {len(all_data)}\")\n\n    return {\n        \"data\": all_data,\n        \"total_pages\": page - 1,\n        \"total_items\": len(all_data)\n    }\n</code></pre>"},{"location":"examples/api/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>Resilient API Calls</p> Python Function with Retries <pre><code>import time\nfrom typing import Optional\n\n@tag(\"api\", alias=\"resilient-call\")\ndef resilient_api_call(\n    url: str,\n    headers: dict = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n    timeout: int = 30,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Make API call with retry logic and error handling.\"\"\"\n\n    headers = headers or {}\n    last_exception = None\n\n    for attempt in range(max_retries + 1):\n        try:\n            result.trace.info(f\"API call attempt {attempt + 1}\")\n\n            response = requests.get(\n                url,\n                headers=headers,\n                timeout=timeout\n            )\n\n            # Handle different HTTP status codes\n            if response.status_code == 429:  # Rate limited\n                retry_after = int(response.headers.get(\"Retry-After\", retry_delay))\n                result.trace.warning(f\"Rate limited, waiting {retry_after}s\")\n                time.sleep(retry_after)\n                continue\n\n            response.raise_for_status()\n\n            result.trace.info(\"API call successful\")\n            return {\n                \"data\": response.json(),\n                \"status_code\": response.status_code,\n                \"attempt\": attempt + 1\n            }\n\n        except requests.exceptions.RequestException as e:\n            last_exception = e\n            result.trace.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n\n            if attempt &lt; max_retries:\n                result.trace.info(f\"Retrying in {retry_delay}s...\")\n                time.sleep(retry_delay)\n                retry_delay *= 2  # Exponential backoff\n\n    # All retries failed\n    result.trace.error(f\"All {max_retries + 1} attempts failed\")\n    raise last_exception\n</code></pre>"},{"location":"examples/api/#data-processing-examples","title":"Data Processing Examples","text":""},{"location":"examples/api/#json-data-transformation","title":"JSON Data Transformation","text":"<p>API Response Processing</p> YAML WorkflowPython Functions <pre><code>process-api-data:\n  type: Workflow\n  params:\n    api_url: str\n    target_format: str\n  jobs:\n    extract-and-transform:\n      stages:\n        - name: \"Fetch Raw Data\"\n          id: fetch\n          uses: api/get-data@rest\n          with:\n            url: ${{ params.api_url }}\n\n        - name: \"Transform Data\"\n          id: transform\n          uses: data/transform-json@processor\n          with:\n            input_data: ${{ stages.fetch.output.data }}\n            target_format: ${{ params.target_format }}\n\n        - name: \"Validate Results\"\n          id: validate\n          uses: data/validate-schema@validator\n          with:\n            data: ${{ stages.transform.output.transformed_data }}\n</code></pre> <pre><code>from typing import List, Dict, Any\nimport json\nfrom pydantic import BaseModel, ValidationError\n\n@tag(\"data\", alias=\"transform-json\")\ndef transform_json_data(\n    input_data: List[Dict[str, Any]],\n    target_format: str,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Transform JSON data to target format.\"\"\"\n\n    transformed_data = []\n\n    for item in input_data:\n        if target_format == \"flat\":\n            # Flatten nested objects\n            flat_item = {}\n            def flatten(obj, prefix=\"\"):\n                for key, value in obj.items():\n                    if isinstance(value, dict):\n                        flatten(value, f\"{prefix}{key}_\")\n                    else:\n                        flat_item[f\"{prefix}{key}\"] = value\n            flatten(item)\n            transformed_data.append(flat_item)\n\n        elif target_format == \"normalized\":\n            # Normalize field names\n            normalized_item = {\n                key.lower().replace(\" \", \"_\"): value\n                for key, value in item.items()\n            }\n            transformed_data.append(normalized_item)\n\n    result.trace.info(f\"Transformed {len(transformed_data)} records\")\n\n    return {\n        \"transformed_data\": transformed_data,\n        \"original_count\": len(input_data),\n        \"transformed_count\": len(transformed_data)\n    }\n\nclass UserSchema(BaseModel):\n    id: int\n    name: str\n    email: str\n    active: bool = True\n\n@tag(\"data\", alias=\"validate-schema\")\ndef validate_data_schema(\n    data: List[Dict[str, Any]],\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Validate data against Pydantic schema.\"\"\"\n\n    valid_records = []\n    invalid_records = []\n\n    for i, record in enumerate(data):\n        try:\n            user = UserSchema(**record)\n            valid_records.append(user.model_dump())\n        except ValidationError as e:\n            invalid_records.append({\n                \"record_index\": i,\n                \"record\": record,\n                \"errors\": e.errors()\n            })\n\n    result.trace.info(f\"Validation complete: {len(valid_records)} valid, {len(invalid_records)} invalid\")\n\n    if invalid_records:\n        result.trace.warning(f\"Found {len(invalid_records)} invalid records\")\n\n    return {\n        \"valid_records\": valid_records,\n        \"invalid_records\": invalid_records,\n        \"validation_success\": len(invalid_records) == 0\n    }\n</code></pre>"},{"location":"examples/api/#batch-processing","title":"Batch Processing","text":""},{"location":"examples/api/#parallel-api-calls","title":"Parallel API Calls","text":"<p>Parallel Processing</p> YAML WorkflowPython Function <pre><code>parallel-api-calls:\n  type: Workflow\n  params:\n    user_ids: list[int]\n    api_base_url: str\n  jobs:\n    process-users:\n      strategy:\n        matrix:\n          user_id: ${{ params.user_ids }}\n        max_workers: 5\n      stages:\n        - name: \"Process User\"\n          uses: api/process-single-user@batch\n          with:\n            user_id: ${{ matrix.user_id }}\n            base_url: ${{ params.api_base_url }}\n</code></pre> <pre><code>@tag(\"api\", alias=\"process-single-user\")\ndef process_single_user(\n    user_id: int,\n    base_url: str,\n    result: Result = None\n) -&gt; dict:\n    \"\"\"Process a single user via API.\"\"\"\n\n    # Get user details\n    user_url = f\"{base_url}/users/{user_id}\"\n    user_response = requests.get(user_url)\n    user_response.raise_for_status()\n    user_data = user_response.json()\n\n    # Get user's orders\n    orders_url = f\"{base_url}/users/{user_id}/orders\"\n    orders_response = requests.get(orders_url)\n    orders_response.raise_for_status()\n    orders_data = orders_response.json()\n\n    # Process and combine data\n    processed_data = {\n        \"user_id\": user_id,\n        \"user_name\": user_data.get(\"name\"),\n        \"email\": user_data.get(\"email\"),\n        \"total_orders\": len(orders_data),\n        \"total_spent\": sum(order.get(\"amount\", 0) for order in orders_data)\n    }\n\n    result.trace.info(f\"Processed user {user_id}: {processed_data['total_orders']} orders\")\n\n    return processed_data\n</code></pre>"},{"location":"examples/api/#configuration-and-best-practices","title":"Configuration and Best Practices","text":""},{"location":"examples/api/#environment-based-configuration","title":"Environment-based Configuration","text":"<p>Configuration Management</p> <pre><code># workflow-config.yml\napi-workflow:\n  type: Workflow\n  params:\n    environment: str\n    api_timeout: int\n  jobs:\n    api-call:\n      stages:\n        - name: \"Configure API Call\"\n          uses: api/configured-call@rest\n          with:\n            base_url: ${{\n              params.environment == 'prod'\n              and 'https://api.prod.example.com'\n              or 'https://api.dev.example.com'\n            }}\n            timeout: ${{ params.api_timeout | coalesce(30) }}\n            api_key: ${API_KEY}  # From environment variable\n</code></pre>"},{"location":"examples/api/#error-handling-best-practices","title":"Error Handling Best Practices","text":"<p>Best Practices</p> <ol> <li>Always use timeouts to prevent hanging requests</li> <li>Implement retry logic with exponential backoff</li> <li>Handle rate limiting by respecting <code>Retry-After</code> headers</li> <li>Log meaningful messages for debugging and monitoring</li> <li>Validate API responses before processing</li> <li>Use environment variables for sensitive data like API keys</li> <li>Implement circuit breakers for unreliable APIs</li> <li>Cache responses when appropriate to reduce API calls</li> </ol>"},{"location":"examples/api/#security-considerations","title":"Security Considerations","text":"<p>Security</p> <ul> <li>Never hardcode credentials in workflow files</li> <li>Use environment variables or secure secret management</li> <li>Validate and sanitize all API inputs</li> <li>Implement proper authentication for sensitive endpoints</li> <li>Log requests/responses carefully to avoid exposing sensitive data</li> <li>Use HTTPS for all API communications</li> <li>Implement rate limiting to prevent abuse</li> </ul> <p>This comprehensive guide covers the most common API integration patterns in workflow orchestration. Adapt these examples to your specific use cases and API requirements.</p>"},{"location":"examples/event-triggers/","title":"Event Trigger Examples for Data Pipeline Workflows","text":"<p>This document provides comprehensive examples of how to use the various event trigger types in real-world data pipeline scenarios.</p>"},{"location":"examples/event-triggers/#overview","title":"Overview","text":"<p>The workflow system supports multiple types of event triggers that are initiated by external services and systems:</p> <ul> <li>Cron Scheduling: Time-based triggers (existing)</li> <li>Release Events: Workflow-to-workflow triggers (existing)</li> <li>File Events: File system event triggers from external services (new)</li> <li>Webhook Events: API/webhook-based triggers from external systems (new)</li> <li>Database Events: Database change event triggers from external services (new)</li> <li>Sensor Events: Sensor-based event triggers from monitoring systems (new)</li> <li>Polling Events: Polling-based triggers for systems without event capabilities (new)</li> <li>Message Queue Events: Message queue-based triggers (new)</li> <li>Stream Processing Events: Real-time stream processing triggers (new)</li> <li>Batch Processing Events: Batch job completion triggers (new)</li> </ul>"},{"location":"examples/event-triggers/#event-trigger-flow-architecture","title":"Event Trigger Flow Architecture","text":"<p>All event triggers follow this flow:</p> <pre><code>External Service/System \u2192 Event Trigger \u2192 Workflow Execution\n</code></pre> <p>The workflow system primarily relies on external services to detect events and trigger workflows through webhook mechanisms. However, polling events are also supported for systems without event capabilities or as fallback mechanisms.</p>"},{"location":"examples/event-triggers/#overview-flowchart","title":"Overview Flowchart","text":"<pre><code>graph TB\n    subgraph \"External Services\"\n        AS[Azure Storage]\n        S3[AWS S3]\n        GCS[Google Cloud Storage]\n        API[External APIs]\n        DB[Database Systems]\n        MON[Monitoring Systems]\n        LEG[Legacy Systems]\n        MQ[Message Queues]\n        ST[Stream Systems]\n        BT[Batch Systems]\n    end\n\n    subgraph \"Event Detection\"\n        EG[Event Grid]\n        SN[S3 Notifications]\n        CF[Cloud Functions]\n        WH[Webhooks]\n        CDC[CDC Systems]\n        AM[Alert Manager]\n        POL[Polling Engine]\n        MQH[Queue Handlers]\n        STH[Stream Handlers]\n        BTH[Batch Handlers]\n    end\n\n    subgraph \"Workflow System\"\n        ET[Event Trigger]\n        WF[Workflow Execution]\n        JO[Job Orchestration]\n    end\n\n    subgraph \"Data Pipeline\"\n        EX[Extract]\n        TR[Transform]\n        LO[Load]\n        NO[Notify]\n    end\n\n    %% File Events\n    AS --&gt; EG\n    S3 --&gt; SN\n    GCS --&gt; CF\n\n    %% Webhook Events\n    API --&gt; WH\n\n    %% Database Events\n    DB --&gt; CDC\n\n    %% Sensor Events\n    MON --&gt; AM\n\n    %% Polling Events\n    LEG --&gt; POL\n\n    %% Message Queue Events\n    MQ --&gt; MQH\n\n    %% Stream Events\n    ST --&gt; STH\n\n    %% Batch Events\n    BT --&gt; BTH\n\n    %% Event Processing\n    EG --&gt; ET\n    SN --&gt; ET\n    CF --&gt; ET\n    WH --&gt; ET\n    CDC --&gt; ET\n    AM --&gt; ET\n    POL --&gt; ET\n    MQH --&gt; ET\n    STH --&gt; ET\n    BTH --&gt; ET\n\n    %% Workflow Execution\n    ET --&gt; WF\n    WF --&gt; JO\n\n    %% Pipeline Stages\n    JO --&gt; EX\n    EX --&gt; TR\n    TR --&gt; LO\n    LO --&gt; NO\n\n    classDef external fill:#e1f5fe\n    classDef detection fill:#f3e5f5\n    classDef workflow fill:#e8f5e8\n    classDef pipeline fill:#fff3e0\n\n    class AS,S3,GCS,API,DB,MON,LEG,MQ,ST,BT external\n    class EG,SN,CF,WH,CDC,AM,POL,MQH,STH,BTH detection\n    class ET,WF,JO workflow\n    class EX,TR,LO,NO pipeline</code></pre>"},{"location":"examples/event-triggers/#advanced-event-trigger-patterns","title":"Advanced Event Trigger Patterns","text":""},{"location":"examples/event-triggers/#1-event-driven-data-pipeline-patterns","title":"1. Event-Driven Data Pipeline Patterns","text":""},{"location":"examples/event-triggers/#pattern-1-multi-stage-etl-with-event-chaining","title":"Pattern 1: Multi-Stage ETL with Event Chaining","text":"<pre><code>event-driven-etl-pipeline:\n  type: Workflow\n  params:\n    data_source: str\n    processing_stage: str\n    batch_id: str\n\n  on:\n    # Initial trigger from file arrival\n    file:\n      - path: \"/data/raw\"\n        pattern: \"*.csv\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 30\n\n    # Secondary trigger from upstream processing\n    release:\n      - \"data-validation-workflow\"\n      - \"data-enrichment-workflow\"\n\n    # Monitoring trigger for pipeline health\n    sensor:\n      - sensor_type: \"pipeline_health\"\n        threshold: 95.0\n        operator: \"lt\"\n        source: \"datadog-monitoring\"\n        check_interval_seconds: 0\n        window_size: 300\n\n  jobs:\n    validate-trigger-source:\n      stages:\n        - name: \"Determine Trigger Source\"\n          run: |\n            source = params.get('trigger_source', 'unknown')\n            stage = params.get('processing_stage', 'unknown')\n\n            if source == 'file_event':\n                result.outputs = {\"next_stage\": \"extract\"}\n            elif source == 'release_event':\n                result.outputs = {\"next_stage\": \"transform\"}\n            elif source == 'sensor_event':\n                result.outputs = {\"next_stage\": \"monitor\"}\n            else:\n                result.outputs = {\"next_stage\": \"unknown\"}\n\n    extract-data:\n      needs: [\"validate-trigger-source\"]\n      if: \"${{ stages.validate-trigger-source.outputs.next_stage == 'extract' }}\"\n      stages:\n        - name: \"Extract Raw Data\"\n          uses: \"extractors/azure_storage@latest\"\n          with:\n            container: \"raw-data\"\n            file_pattern: \"*.csv\"\n\n        - name: \"Validate Schema\"\n          uses: \"validation/schema_check@latest\"\n          with:\n            schema_file: \"schemas/raw_data.json\"\n\n        - name: \"Trigger Validation Workflow\"\n          trigger: \"data-validation-workflow\"\n          params:\n            batch_id: \"${{ params.batch_id }}\"\n            data_path: \"${{ stages.extract-raw-data.outputs.data_path }}\"\n\n    transform-data:\n      needs: [\"validate-trigger-source\"]\n      if: \"${{ stages.validate-trigger-source.outputs.next_stage == 'transform' }}\"\n      stages:\n        - name: \"Transform Data\"\n          uses: \"transformers/data_processor@latest\"\n          with:\n            input_path: \"${{ params.data_path }}\"\n            transformation_rules: \"rules/transform.json\"\n\n        - name: \"Quality Check\"\n          uses: \"quality/data_quality@latest\"\n          with:\n            data_path: \"${{ stages.transform-data.outputs.output_path }}\"\n\n        - name: \"Trigger Enrichment Workflow\"\n          trigger: \"data-enrichment-workflow\"\n          params:\n            batch_id: \"${{ params.batch_id }}\"\n            processed_data: \"${{ stages.transform-data.outputs.output_path }}\"\n\n    monitor-pipeline:\n      needs: [\"validate-trigger-source\"]\n      if: \"${{ stages.validate-trigger-source.outputs.next_stage == 'monitor' }}\"\n      stages:\n        - name: \"Analyze Pipeline Health\"\n          uses: \"monitoring/pipeline_analyzer@latest\"\n          with:\n            metrics: \"${{ params.metric_data }}\"\n\n        - name: \"Send Alert\"\n          uses: \"notifications/send_alert@latest\"\n          with:\n            level: \"warning\"\n            message: \"Pipeline health below threshold\"\n            details: \"${{ stages.analyze-pipeline-health.outputs.analysis }}\"\n</code></pre>"},{"location":"examples/event-triggers/#pattern-2-real-time-stream-processing","title":"Pattern 2: Real-Time Stream Processing","text":"<pre><code>real-time-stream-pipeline:\n  type: Workflow\n  params:\n    stream_data: dict\n    processing_window: int\n    aggregation_type: str\n\n  on:\n    # Kafka stream events\n    webhook:\n      - endpoint: \"/webhook/kafka-stream\"\n        method: \"POST\"\n        headers:\n          \"X-Kafka-Topic\": \"${{ env.KAFKA_TOPIC }}\"\n        timeout_seconds: 15\n\n    # Time-based window processing\n    schedule:\n      - cron: \"*/5 * * * *\"  # Every 5 minutes\n        timezone: \"UTC\"\n\n    # Stream health monitoring\n    sensor:\n      - sensor_type: \"stream_lag\"\n        threshold: 300  # 5 minutes\n        operator: \"gt\"\n        source: \"kafka-monitoring\"\n        check_interval_seconds: 0\n        window_size: 60\n\n  jobs:\n    process-stream-window:\n      stages:\n        - name: \"Collect Stream Data\"\n          uses: \"streaming/collect_window@latest\"\n          with:\n            window_size: \"${{ params.processing_window }}\"\n            aggregation_type: \"${{ params.aggregation_type }}\"\n\n        - name: \"Process Stream Events\"\n          uses: \"streaming/process_events@latest\"\n          with:\n            events: \"${{ stages.collect-stream-window.outputs.events }}\"\n\n        - name: \"Update Real-Time Dashboard\"\n          uses: \"streaming/update_dashboard@latest\"\n          with:\n            processed_data: \"${{ stages.process-stream-events.outputs.results }}\"\n\n        - name: \"Store to Time-Series DB\"\n          uses: \"storage/timeseries_store@latest\"\n          with:\n            data: \"${{ stages.process-stream-events.outputs.results }}\"\n            database: \"influxdb\"\n</code></pre>"},{"location":"examples/event-triggers/#pattern-3-batch-processing-with-event-coordination","title":"Pattern 3: Batch Processing with Event Coordination","text":"<pre><code>batch-processing-pipeline:\n  type: Workflow\n  params:\n    batch_size: int\n    processing_date: date\n    data_sources: list\n\n  on:\n    # Batch job completion events\n    webhook:\n      - endpoint: \"/webhook/batch-complete\"\n        method: \"POST\"\n        headers:\n          \"X-Batch-ID\": \"${{ env.BATCH_ID }}\"\n        timeout_seconds: 30\n\n    # Daily batch schedule\n    schedule:\n      - cron: \"0 2 * * *\"  # Daily at 2 AM\n        timezone: \"UTC\"\n\n    # Batch processing monitoring\n    sensor:\n      - sensor_type: \"batch_duration\"\n        threshold: 3600  # 1 hour\n        operator: \"gt\"\n        source: \"batch-monitoring\"\n        check_interval_seconds: 0\n        window_size: 300\n\n  jobs:\n    coordinate-batch:\n      stages:\n        - name: \"Check Batch Prerequisites\"\n          uses: \"batch/check_prerequisites@latest\"\n          with:\n            date: \"${{ params.processing_date }}\"\n            sources: \"${{ params.data_sources }}\"\n\n        - name: \"Start Parallel Processing\"\n          uses: \"batch/start_parallel_jobs@latest\"\n          with:\n            batch_size: \"${{ params.batch_size }}\"\n            sources: \"${{ stages.check-batch-prerequisites.outputs.ready_sources }}\"\n\n        - name: \"Monitor Batch Progress\"\n          uses: \"batch/monitor_progress@latest\"\n          with:\n            batch_id: \"${{ stages.start-parallel-jobs.outputs.batch_id }}\"\n\n        - name: \"Consolidate Results\"\n          uses: \"batch/consolidate_results@latest\"\n          with:\n            batch_id: \"${{ stages.start-parallel-jobs.outputs.batch_id }}\"\n\n        - name: \"Trigger Downstream Workflows\"\n          trigger: \"data-warehouse-load\"\n          params:\n            consolidated_data: \"${{ stages.consolidate-results.outputs.data_path }}\"\n            batch_id: \"${{ stages.start-parallel-jobs.outputs.batch_id }}\"\n</code></pre>"},{"location":"examples/event-triggers/#2-advanced-file-processing-patterns","title":"2. Advanced File Processing Patterns","text":""},{"location":"examples/event-triggers/#pattern-4-multi-format-file-processing","title":"Pattern 4: Multi-Format File Processing","text":"<pre><code>multi-format-file-pipeline:\n  type: Workflow\n  params:\n    file_path: str\n    file_format: str\n    processing_rules: dict\n\n  on:\n    file:\n      # CSV files\n      - path: \"/data/csv\"\n        pattern: \"*.csv\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 30\n\n      # JSON files\n      - path: \"/data/json\"\n        pattern: \"*.json\"\n        event_type: \"created\"\n        source: \"aws-s3\"\n        debounce_seconds: 45\n\n      # Parquet files\n      - path: \"/data/parquet\"\n        pattern: \"*.parquet\"\n        event_type: \"created\"\n        source: \"google-cloud-storage\"\n        debounce_seconds: 60\n\n      # Excel files\n      - path: \"/data/excel\"\n        pattern: \"*.xlsx\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 30\n\n  jobs:\n    detect-file-format:\n      stages:\n        - name: \"Analyze File Format\"\n          run: |\n            file_path = params.get('file_path', '')\n            file_extension = file_path.split('.')[-1].lower()\n\n            format_handlers = {\n                'csv': 'csv-processor',\n                'json': 'json-processor',\n                'parquet': 'parquet-processor',\n                'xlsx': 'excel-processor'\n            }\n\n            processor = format_handlers.get(file_extension, 'generic-processor')\n            result.outputs = {\"processor\": processor, \"format\": file_extension}\n\n    process-csv:\n      needs: [\"detect-file-format\"]\n      if: \"${{ stages.analyze-file-format.outputs.format == 'csv' }}\"\n      stages:\n        - name: \"Process CSV File\"\n          uses: \"processors/csv_processor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            delimiter: \",\"\n            encoding: \"utf-8\"\n\n        - name: \"Validate CSV Data\"\n          uses: \"validation/csv_validator@latest\"\n          with:\n            data: \"${{ stages.process-csv-file.outputs.data }}\"\n            schema: \"schemas/csv_schema.json\"\n\n    process-json:\n      needs: [\"detect-file-format\"]\n      if: \"${{ stages.analyze-file-format.outputs.format == 'json' }}\"\n      stages:\n        - name: \"Process JSON File\"\n          uses: \"processors/json_processor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            json_path: \"${{ params.processing_rules.json_path }}\"\n\n        - name: \"Flatten JSON Structure\"\n          uses: \"transformers/json_flattener@latest\"\n          with:\n            json_data: \"${{ stages.process-json-file.outputs.data }}\"\n\n    process-parquet:\n      needs: [\"detect-file-format\"]\n      if: \"${{ stages.analyze-file-format.outputs.format == 'parquet' }}\"\n      stages:\n        - name: \"Process Parquet File\"\n          uses: \"processors/parquet_processor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            columns: \"${{ params.processing_rules.columns }}\"\n\n        - name: \"Optimize Parquet\"\n          uses: \"optimizers/parquet_optimizer@latest\"\n          with:\n            data: \"${{ stages.process-parquet-file.outputs.data }}\"\n\n    process-excel:\n      needs: [\"detect-file-format\"]\n      if: \"${{ stages.analyze-file-format.outputs.format == 'xlsx' }}\"\n      stages:\n        - name: \"Process Excel File\"\n          uses: \"processors/excel_processor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            sheet_name: \"${{ params.processing_rules.sheet_name }}\"\n\n        - name: \"Extract Multiple Sheets\"\n          uses: \"processors/excel_sheet_extractor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            sheets: \"${{ params.processing_rules.sheets }}\"\n\n    consolidate-results:\n      needs: [\"process-csv\", \"process-json\", \"process-parquet\", \"process-excel\"]\n      trigger_rule: \"any_success\"\n      stages:\n        - name: \"Consolidate All Formats\"\n          uses: \"consolidators/multi_format@latest\"\n          with:\n            csv_data: \"${{ stages.process-csv.outputs.data }}\"\n            json_data: \"${{ stages.process-json.outputs.data }}\"\n            parquet_data: \"${{ stages.process-parquet.outputs.data }}\"\n            excel_data: \"${{ stages.process-excel.outputs.data }}\"\n\n        - name: \"Load to Data Warehouse\"\n          uses: \"loaders/warehouse_loader@latest\"\n          with:\n            consolidated_data: \"${{ stages.consolidate-all-formats.outputs.data }}\"\n</code></pre>"},{"location":"examples/event-triggers/#pattern-5-file-processing-with-quality-gates","title":"Pattern 5: File Processing with Quality Gates","text":"<pre><code>quality-gated-file-pipeline:\n  type: Workflow\n  params:\n    file_path: str\n    quality_threshold: float\n    processing_mode: str\n\n  on:\n    file:\n      - path: \"/data/quality-checked\"\n        pattern: \"*.csv\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 30\n\n  jobs:\n    quality-assessment:\n      stages:\n        - name: \"Assess Data Quality\"\n          uses: \"quality/data_assessor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            quality_metrics: [\"completeness\", \"accuracy\", \"consistency\", \"timeliness\"]\n\n        - name: \"Calculate Quality Score\"\n          run: |\n            metrics = stages['assess-data-quality'].outputs.get('metrics', {})\n\n            completeness = metrics.get('completeness', 0)\n            accuracy = metrics.get('accuracy', 0)\n            consistency = metrics.get('consistency', 0)\n            timeliness = metrics.get('timeliness', 0)\n\n            quality_score = (completeness + accuracy + consistency + timeliness) / 4\n            result.outputs = {\"quality_score\": quality_score}\n\n        - name: \"Quality Gate Decision\"\n          run: |\n            quality_score = stages['calculate-quality-score'].outputs.get('quality_score', 0)\n            threshold = params.get('quality_threshold', 0.8)\n\n            if quality_score &gt;= threshold:\n                result.outputs = {\"proceed\": True, \"reason\": \"Quality threshold met\"}\n            else:\n                result.outputs = {\"proceed\": False, \"reason\": f\"Quality score {quality_score} below threshold {threshold}\"}\n\n    process-high-quality:\n      needs: [\"quality-assessment\"]\n      if: \"${{ stages.quality-gate-decision.outputs.proceed }}\"\n      stages:\n        - name: \"Process High-Quality Data\"\n          uses: \"processors/high_quality_processor@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            quality_score: \"${{ stages.calculate-quality-score.outputs.quality_score }}\"\n\n        - name: \"Load to Production\"\n          uses: \"loaders/production_loader@latest\"\n          with:\n            processed_data: \"${{ stages.process-high-quality-data.outputs.data }}\"\n\n    handle-low-quality:\n      needs: [\"quality-assessment\"]\n      if: \"${{ not stages.quality-gate-decision.outputs.proceed }}\"\n      stages:\n        - name: \"Log Quality Issues\"\n          uses: \"logging/quality_logger@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            quality_score: \"${{ stages.calculate-quality-score.outputs.quality_score }}\"\n            reason: \"${{ stages.quality-gate-decision.outputs.reason }}\"\n\n        - name: \"Send Quality Alert\"\n          uses: \"notifications/quality_alert@latest\"\n          with:\n            level: \"warning\"\n            message: \"Data quality below threshold\"\n            details: \"${{ stages.log-quality-issues.outputs.log_entry }}\"\n\n        - name: \"Move to Quarantine\"\n          uses: \"storage/move_to_quarantine@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n            quarantine_reason: \"${{ stages.quality-gate-decision.outputs.reason }}\"\n</code></pre>"},{"location":"examples/event-triggers/#3-advanced-database-integration-patterns","title":"3. Advanced Database Integration Patterns","text":""},{"location":"examples/event-triggers/#pattern-6-multi-database-cdc-pipeline","title":"Pattern 6: Multi-Database CDC Pipeline","text":"<pre><code>multi-database-cdc-pipeline:\n  type: Workflow\n  params:\n    source_database: str\n    change_data: dict\n    operation_type: str\n    table_name: str\n\n  on:\n    database:\n      # Primary database CDC\n      - connection_string: \"${{ env.PRIMARY_DB_URL }}\"\n        table: \"orders\"\n        operation: \"any\"\n        source: \"debezium-cdc\"\n        check_interval_seconds: 0\n\n      # Secondary database CDC\n      - connection_string: \"${{ env.SECONDARY_DB_URL }}\"\n        table: \"inventory\"\n        operation: \"any\"\n        source: \"debezium-cdc\"\n        check_interval_seconds: 0\n\n      # Audit database changes\n      - connection_string: \"${{ env.AUDIT_DB_URL }}\"\n        table: \"audit_logs\"\n        operation: \"insert\"\n        source: \"debezium-cdc\"\n        check_interval_seconds: 0\n\n  jobs:\n    route-database-changes:\n      stages:\n        - name: \"Route by Database Source\"\n          run: |\n            source_db = params.get('source_database', 'unknown')\n            table = params.get('table_name', 'unknown')\n            operation = params.get('operation_type', 'unknown')\n\n            routing_rules = {\n                'primary_db': {\n                    'orders': 'order-processor',\n                    'customers': 'customer-processor'\n                },\n                'secondary_db': {\n                    'inventory': 'inventory-processor',\n                    'products': 'product-processor'\n                },\n                'audit_db': {\n                    'audit_logs': 'audit-processor'\n                }\n            }\n\n            processor = routing_rules.get(source_db, {}).get(table, 'generic-processor')\n            result.outputs = {\"processor\": processor, \"source\": source_db}\n\n    process-order-changes:\n      needs: [\"route-database-changes\"]\n      if: \"${{ stages.route-database-changes.outputs.processor == 'order-processor' }}\"\n      stages:\n        - name: \"Process Order Changes\"\n          uses: \"processors/order_processor@latest\"\n          with:\n            change_data: \"${{ params.change_data }}\"\n            operation: \"${{ params.operation_type }}\"\n\n        - name: \"Update Order Analytics\"\n          uses: \"analytics/order_analytics@latest\"\n          with:\n            processed_changes: \"${{ stages.process-order-changes.outputs.processed_data }}\"\n\n        - name: \"Sync to Data Warehouse\"\n          uses: \"warehouse/sync_orders@latest\"\n          with:\n            analytics_data: \"${{ stages.update-order-analytics.outputs.analytics }}\"\n\n    process-inventory-changes:\n      needs: [\"route-database-changes\"]\n      if: \"${{ stages.route-database-changes.outputs.processor == 'inventory-processor' }}\"\n      stages:\n        - name: \"Process Inventory Changes\"\n          uses: \"processors/inventory_processor@latest\"\n          with:\n            change_data: \"${{ params.change_data }}\"\n            operation: \"${{ params.operation_type }}\"\n\n        - name: \"Update Inventory Levels\"\n          uses: \"inventory/update_levels@latest\"\n          with:\n            processed_changes: \"${{ stages.process-inventory-changes.outputs.processed_data }}\"\n\n        - name: \"Check Reorder Points\"\n          uses: \"inventory/check_reorder@latest\"\n          with:\n            inventory_levels: \"${{ stages.update-inventory-levels.outputs.levels }}\"\n\n    process-audit-changes:\n      needs: [\"route-database-changes\"]\n      if: \"${{ stages.route-database-changes.outputs.processor == 'audit-processor' }}\"\n      stages:\n        - name: \"Process Audit Changes\"\n          uses: \"processors/audit_processor@latest\"\n          with:\n            change_data: \"${{ params.change_data }}\"\n            operation: \"${{ params.operation_type }}\"\n\n        - name: \"Compliance Check\"\n          uses: \"compliance/audit_check@latest\"\n          with:\n            audit_data: \"${{ stages.process-audit-changes.outputs.processed_data }}\"\n\n        - name: \"Generate Compliance Report\"\n          uses: \"reports/compliance_report@latest\"\n          with:\n            compliance_results: \"${{ stages.compliance-check.outputs.results }}\"\n</code></pre>"},{"location":"examples/event-triggers/#pattern-7-database-migration-with-event-tracking","title":"Pattern 7: Database Migration with Event Tracking","text":"<pre><code>database-migration-pipeline:\n  type: Workflow\n  params:\n    migration_id: str\n    source_schema: str\n    target_schema: str\n    migration_type: str\n\n  on:\n    database:\n      # Migration completion events\n      - connection_string: \"${{ env.MIGRATION_DB_URL }}\"\n        table: \"migration_logs\"\n        operation: \"insert\"\n        source: \"migration-tracker\"\n        check_interval_seconds: 0\n\n      # Data validation events\n      - connection_string: \"${{ env.VALIDATION_DB_URL }}\"\n        table: \"validation_results\"\n        operation: \"insert\"\n        source: \"validation-tracker\"\n        check_interval_seconds: 0\n\n    # Migration schedule\n    schedule:\n      - cron: \"0 1 * * *\"  # Daily at 1 AM\n        timezone: \"UTC\"\n\n  jobs:\n    track-migration-progress:\n      stages:\n        - name: \"Monitor Migration Status\"\n          uses: \"migration/monitor_status@latest\"\n          with:\n            migration_id: \"${{ params.migration_id }}\"\n\n        - name: \"Check Migration Completion\"\n          run: |\n            status = stages['monitor-migration-status'].outputs.get('status', 'unknown')\n\n            if status == 'completed':\n                result.outputs = {\"next_action\": \"validate\"}\n            elif status == 'failed':\n                result.outputs = {\"next_action\": \"rollback\"}\n            else:\n                result.outputs = {\"next_action\": \"wait\"}\n\n    validate-migration:\n      needs: [\"track-migration-progress\"]\n      if: \"${{ stages.track-migration-progress.outputs.next_action == 'validate' }}\"\n      stages:\n        - name: \"Run Data Validation\"\n          uses: \"validation/migration_validator@latest\"\n          with:\n            source_schema: \"${{ params.source_schema }}\"\n            target_schema: \"${{ params.target_schema }}\"\n            migration_id: \"${{ params.migration_id }}\"\n\n        - name: \"Compare Data Integrity\"\n          uses: \"validation/data_integrity@latest\"\n          with:\n            source_data: \"${{ stages.run-data-validation.outputs.source_sample }}\"\n            target_data: \"${{ stages.run-data-validation.outputs.target_sample }}\"\n\n        - name: \"Generate Validation Report\"\n          uses: \"reports/validation_report@latest\"\n          with:\n            validation_results: \"${{ stages.compare-data-integrity.outputs.comparison }}\"\n\n        - name: \"Send Validation Notification\"\n          uses: \"notifications/migration_notification@latest\"\n          with:\n            migration_id: \"${{ params.migration_id }}\"\n            validation_status: \"${{ stages.compare-data-integrity.outputs.status }}\"\n\n    rollback-migration:\n      needs: [\"track-migration-progress\"]\n      if: \"${{ stages.track-migration-progress.outputs.next_action == 'rollback' }}\"\n      stages:\n        - name: \"Initiate Rollback\"\n          uses: \"migration/rollback@latest\"\n          with:\n            migration_id: \"${{ params.migration_id }}\"\n            source_schema: \"${{ params.source_schema }}\"\n\n        - name: \"Verify Rollback\"\n          uses: \"migration/verify_rollback@latest\"\n          with:\n            migration_id: \"${{ params.migration_id }}\"\n\n        - name: \"Send Rollback Alert\"\n          uses: \"notifications/rollback_alert@latest\"\n          with:\n            migration_id: \"${{ params.migration_id }}\"\n            rollback_status: \"${{ stages.verify-rollback.outputs.status }}\"\n</code></pre>"},{"location":"examples/event-triggers/#4-advanced-monitoring-and-alerting-patterns","title":"4. Advanced Monitoring and Alerting Patterns","text":""},{"location":"examples/event-triggers/#pattern-8-intelligent-alerting-with-escalation","title":"Pattern 8: Intelligent Alerting with Escalation","text":"<pre><code>intelligent-alerting-pipeline:\n  type: Workflow\n  params:\n    alert_data: dict\n    alert_severity: str\n    alert_source: str\n    escalation_level: int\n\n  on:\n    sensor:\n      # System health monitoring\n      - sensor_type: \"system_health\"\n        threshold: 90.0\n        operator: \"lt\"\n        source: \"prometheus-alertmanager\"\n        check_interval_seconds: 0\n        window_size: 300\n\n      # Business metrics monitoring\n      - sensor_type: \"business_metrics\"\n        threshold: 1000\n        operator: \"lt\"\n        source: \"datadog-monitoring\"\n        check_interval_seconds: 0\n        window_size: 3600\n\n      # Performance monitoring\n      - sensor_type: \"performance_metrics\"\n        threshold: 2000\n        operator: \"gt\"\n        source: \"newrelic-monitoring\"\n        check_interval_seconds: 0\n        window_size: 600\n\n  jobs:\n    analyze-alert-context:\n      stages:\n        - name: \"Analyze Alert Context\"\n          uses: \"monitoring/alert_analyzer@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            alert_severity: \"${{ params.alert_severity }}\"\n            alert_source: \"${{ params.alert_source }}\"\n\n        - name: \"Determine Escalation Level\"\n          run: |\n            severity = params.get('alert_severity', 'low')\n            current_level = params.get('escalation_level', 0)\n\n            severity_levels = {\n                'critical': 3,\n                'high': 2,\n                'medium': 1,\n                'low': 0\n            }\n\n            required_level = severity_levels.get(severity, 0)\n            escalation_needed = required_level &gt; current_level\n\n            result.outputs = {\n                \"escalation_needed\": escalation_needed,\n                \"required_level\": required_level,\n                \"current_level\": current_level\n            }\n\n    handle-critical-alert:\n      needs: [\"analyze-alert-context\"]\n      if: \"${{ params.alert_severity == 'critical' }}\"\n      stages:\n        - name: \"Immediate Response\"\n          uses: \"monitoring/immediate_response@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n\n        - name: \"Page On-Call Team\"\n          uses: \"notifications/page_oncall@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            escalation_level: \"${{ stages.analyze-alert-context.outputs.required_level }}\"\n\n        - name: \"Initiate Auto-Recovery\"\n          uses: \"recovery/auto_recovery@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            recovery_actions: \"${{ stages.immediate-response.outputs.recovery_plan }}\"\n\n    handle-high-alert:\n      needs: [\"analyze-alert-context\"]\n      if: \"${{ params.alert_severity == 'high' }}\"\n      stages:\n        - name: \"Send High Priority Alert\"\n          uses: \"notifications/high_priority_alert@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n\n        - name: \"Schedule Investigation\"\n          uses: \"monitoring/schedule_investigation@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            priority: \"high\"\n\n    handle-medium-alert:\n      needs: [\"analyze-alert-context\"]\n      if: \"${{ params.alert_severity == 'medium' }}\"\n      stages:\n        - name: \"Send Medium Priority Alert\"\n          uses: \"notifications/medium_priority_alert@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n\n        - name: \"Log for Review\"\n          uses: \"logging/alert_logger@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            review_required: True\n\n    escalate-alert:\n      needs: [\"analyze-alert-context\"]\n      if: \"${{ stages.analyze-alert-context.outputs.escalation_needed }}\"\n      stages:\n        - name: \"Escalate Alert\"\n          uses: \"escalation/escalate_alert@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            current_level: \"${{ stages.analyze-alert-context.outputs.current_level }}\"\n            required_level: \"${{ stages.analyze-alert-context.outputs.required_level }}\"\n\n        - name: \"Notify Management\"\n          uses: \"notifications/management_notification@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            escalation_details: \"${{ stages.escalate-alert.outputs.escalation_details }}\"\n</code></pre>"},{"location":"examples/event-triggers/#pattern-9-predictive-monitoring","title":"Pattern 9: Predictive Monitoring","text":"<pre><code>predictive-monitoring-pipeline:\n  type: Workflow\n  params:\n    metric_data: dict\n    prediction_window: int\n    confidence_threshold: float\n\n  on:\n    sensor:\n      # Anomaly detection\n      - sensor_type: \"anomaly_detection\"\n        threshold: 0.8\n        operator: \"gt\"\n        source: \"ml-monitoring\"\n        check_interval_seconds: 0\n        window_size: 1800\n\n      # Trend analysis\n      - sensor_type: \"trend_analysis\"\n        threshold: 0.7\n        operator: \"gt\"\n        source: \"ml-monitoring\"\n        check_interval_seconds: 0\n        window_size: 3600\n\n  jobs:\n    analyze-predictions:\n      stages:\n        - name: \"Analyze Metric Trends\"\n          uses: \"ml/trend_analyzer@latest\"\n          with:\n            metric_data: \"${{ params.metric_data }}\"\n            prediction_window: \"${{ params.prediction_window }}\"\n\n        - name: \"Generate Predictions\"\n          uses: \"ml/prediction_engine@latest\"\n          with:\n            trend_data: \"${{ stages.analyze-metric-trends.outputs.trends }}\"\n            confidence_threshold: \"${{ params.confidence_threshold }}\"\n\n        - name: \"Assess Risk Level\"\n          run: |\n            predictions = stages['generate-predictions'].outputs.get('predictions', [])\n            confidence = stages['generate-predictions'].outputs.get('confidence', 0)\n\n            risk_levels = {\n                'high': 0.9,\n                'medium': 0.7,\n                'low': 0.5\n            }\n\n            risk_level = 'low'\n            for level, threshold in risk_levels.items():\n                if confidence &gt;= threshold:\n                    risk_level = level\n\n            result.outputs = {\"risk_level\": risk_level, \"confidence\": confidence}\n\n    handle-high-risk:\n      needs: [\"analyze-predictions\"]\n      if: \"${{ stages.assess-risk-level.outputs.risk_level == 'high' }}\"\n      stages:\n        - name: \"Send High Risk Alert\"\n          uses: \"notifications/high_risk_alert@latest\"\n          with:\n            predictions: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            confidence: \"${{ stages.assess-risk-level.outputs.confidence }}\"\n\n        - name: \"Initiate Preventive Actions\"\n          uses: \"prevention/preventive_actions@latest\"\n          with:\n            predictions: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            risk_level: \"${{ stages.assess-risk-level.outputs.risk_level }}\"\n\n    handle-medium-risk:\n      needs: [\"analyze-predictions\"]\n      if: \"${{ stages.assess-risk-level.outputs.risk_level == 'medium' }}\"\n      stages:\n        - name: \"Send Medium Risk Alert\"\n          uses: \"notifications/medium_risk_alert@latest\"\n          with:\n            predictions: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            confidence: \"${{ stages.assess-risk-level.outputs.confidence }}\"\n\n        - name: \"Schedule Monitoring\"\n          uses: \"monitoring/schedule_monitoring@latest\"\n          with:\n            predictions: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            monitoring_interval: \"5m\"\n\n    handle-low-risk:\n      needs: [\"analyze-predictions\"]\n      if: \"${{ stages.assess-risk-level.outputs.risk_level == 'low' }}\"\n      stages:\n        - name: \"Log Prediction\"\n          uses: \"logging/prediction_logger@latest\"\n          with:\n            predictions: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            confidence: \"${{ stages.assess-risk-level.outputs.confidence }}\"\n\n        - name: \"Update Prediction Model\"\n          uses: \"ml/update_model@latest\"\n          with:\n            prediction_data: \"${{ stages.generate-predictions.outputs.predictions }}\"\n            actual_outcomes: \"${{ params.metric_data }}\"\n</code></pre>"},{"location":"examples/event-triggers/#message-queue-triggers","title":"Message Queue Triggers","text":""},{"location":"examples/event-triggers/#rabbitmq-message-processing","title":"RabbitMQ Message Processing","text":"<pre><code>rabbitmq-message-pipeline:\n  type: Workflow\n  params:\n    message_data: dict\n    queue_name: str\n    message_id: str\n\n  on:\n    message_queue:\n      - queue_name: \"data-processing-queue\"\n        message_pattern: \".*order.*\"\n        source: \"rabbitmq\"\n        batch_size: 20\n        visibility_timeout: 300\n        dead_letter_queue: \"failed-messages\"\n\n  jobs:\n    process-message:\n      stages:\n        - name: \"Parse Message\"\n          uses: \"queue/parse_message@latest\"\n          with:\n            message_data: \"${{ params.message_data }}\"\n            queue_name: \"${{ params.queue_name }}\"\n\n        - name: \"Validate Message\"\n          uses: \"validation/message_validator@latest\"\n          with:\n            parsed_data: \"${{ stages.parse-message.outputs.parsed_data }}\"\n\n        - name: \"Process Order Data\"\n          uses: \"processors/order_processor@latest\"\n          with:\n            validated_data: \"${{ stages.validate-message.outputs.validated_data }}\"\n\n        - name: \"Acknowledge Message\"\n          uses: \"queue/acknowledge_message@latest\"\n          with:\n            message_id: \"${{ params.message_id }}\"\n            queue_name: \"${{ params.queue_name }}\"\n</code></pre> <p>Flow Trigger: RabbitMQ \u2192 Message Queue \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant RMQ as RabbitMQ\n    participant MQ as Message Queue\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    RMQ-&gt;&gt;MQ: Message Arrival\n    MQ-&gt;&gt;WH: HTTP POST with Message\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Parse Message\n    WF-&gt;&gt;WF: Validate Message\n    WF-&gt;&gt;DB: Process Order Data\n    WF-&gt;&gt;MQ: Acknowledge Message</code></pre>"},{"location":"examples/event-triggers/#apache-kafka-stream-processing","title":"Apache Kafka Stream Processing","text":"<pre><code>kafka-stream-pipeline:\n  type: Workflow\n  params:\n    stream_data: dict\n    topic_name: str\n    partition_id: int\n\n  on:\n    message_queue:\n      - queue_name: \"user-events-topic\"\n        message_pattern: \".*user.*\"\n        source: \"kafka\"\n        batch_size: 50\n        visibility_timeout: 600\n        dead_letter_queue: \"failed-events\"\n\n  jobs:\n    process-stream:\n      stages:\n        - name: \"Consume Kafka Messages\"\n          uses: \"kafka/consume_messages@latest\"\n          with:\n            topic: \"${{ params.topic_name }}\"\n            partition: \"${{ params.partition_id }}\"\n            batch_size: 50\n\n        - name: \"Process User Events\"\n          uses: \"processors/user_event_processor@latest\"\n          with:\n            stream_data: \"${{ stages.consume-kafka-messages.outputs.messages }}\"\n\n        - name: \"Update User Analytics\"\n          uses: \"analytics/user_analytics@latest\"\n          with:\n            processed_events: \"${{ stages.process-user-events.outputs.processed_data }}\"\n\n        - name: \"Commit Offset\"\n          uses: \"kafka/commit_offset@latest\"\n          with:\n            topic: \"${{ params.topic_name }}\"\n            partition: \"${{ params.partition_id }}\"\n</code></pre> <p>Flow Trigger: Apache Kafka \u2192 Stream Consumer \u2192 Webhook \u2192 Workflow</p>"},{"location":"examples/event-triggers/#stream-processing-triggers","title":"Stream Processing Triggers","text":""},{"location":"examples/event-triggers/#real-time-analytics-pipeline","title":"Real-Time Analytics Pipeline","text":"<pre><code>real-time-analytics-pipeline:\n  type: Workflow\n  params:\n    analytics_data: dict\n    window_size: int\n    aggregation_type: str\n\n  on:\n    stream_processing:\n      - stream_name: \"clickstream-events\"\n        window_size: 300\n        aggregation_type: \"count\"\n        source: \"kafka\"\n        checkpoint_interval: 60\n        watermark_delay: 30\n\n    # Fallback schedule for batch processing\n    schedule:\n      - cron: \"*/5 * * * *\"  # Every 5 minutes\n        timezone: \"UTC\"\n\n  jobs:\n    process-stream-window:\n      stages:\n        - name: \"Collect Stream Window\"\n          uses: \"streaming/collect_window@latest\"\n          with:\n            stream_name: \"clickstream-events\"\n            window_size: \"${{ params.window_size }}\"\n            aggregation_type: \"${{ params.aggregation_type }}\"\n\n        - name: \"Calculate Real-Time Metrics\"\n          uses: \"analytics/real_time_calculator@latest\"\n          with:\n            window_data: \"${{ stages.collect-stream-window.outputs.window_data }}\"\n\n        - name: \"Update Dashboard\"\n          uses: \"dashboard/update_metrics@latest\"\n          with:\n            metrics: \"${{ stages.calculate-real-time-metrics.outputs.metrics }}\"\n\n        - name: \"Store to Time-Series DB\"\n          uses: \"storage/timeseries_store@latest\"\n          with:\n            metrics: \"${{ stages.calculate-real-time-metrics.outputs.metrics }}\"\n            database: \"influxdb\"\n</code></pre> <p>Flow Trigger: Kafka Stream \u2192 Stream Processor \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant KS as Kafka Stream\n    participant SP as Stream Processor\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Time-Series DB\n\n    KS-&gt;&gt;SP: Stream Events\n    SP-&gt;&gt;WH: Window Complete Event\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Collect Stream Window\n    WF-&gt;&gt;WF: Calculate Metrics\n    WF-&gt;&gt;WF: Update Dashboard\n    WF-&gt;&gt;DB: Store Metrics</code></pre>"},{"location":"examples/event-triggers/#batch-processing-triggers","title":"Batch Processing Triggers","text":""},{"location":"examples/event-triggers/#spark-job-completion-pipeline","title":"Spark Job Completion Pipeline","text":"<pre><code>spark-batch-pipeline:\n  type: Workflow\n  params:\n    job_id: str\n    job_status: str\n    output_path: str\n\n  on:\n    batch_processing:\n      - job_name: \"daily-data-processing\"\n        job_status: \"completed\"\n        source: \"spark\"\n        timeout_minutes: 180\n        retry_count: 3\n        dependencies: [\"data-extraction\", \"data-validation\"]\n\n    # Fallback schedule\n    schedule:\n      - cron: \"0 3 * * *\"  # Daily at 3 AM\n        timezone: \"UTC\"\n\n  jobs:\n    handle-batch-completion:\n      stages:\n        - name: \"Validate Job Output\"\n          uses: \"batch/validate_output@latest\"\n          with:\n            job_id: \"${{ params.job_id }}\"\n            output_path: \"${{ params.output_path }}\"\n\n        - name: \"Process Batch Results\"\n          uses: \"batch/process_results@latest\"\n          with:\n            validated_output: \"${{ stages.validate-job-output.outputs.validated_data }}\"\n\n        - name: \"Load to Data Warehouse\"\n          uses: \"warehouse/load_batch_data@latest\"\n          with:\n            processed_results: \"${{ stages.process-batch-results.outputs.processed_data }}\"\n\n        - name: \"Trigger Downstream Workflows\"\n          trigger: \"analytics-workflow\"\n          params:\n            batch_data: \"${{ stages.process-batch-results.outputs.processed_data }}\"\n            job_id: \"${{ params.job_id }}\"\n</code></pre> <p>Flow Trigger: Spark Cluster \u2192 Job Monitor \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant SC as Spark Cluster\n    participant JM as Job Monitor\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DW as Data Warehouse\n\n    SC-&gt;&gt;JM: Job Completed\n    JM-&gt;&gt;WH: HTTP POST with Job Status\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Validate Job Output\n    WF-&gt;&gt;WF: Process Batch Results\n    WF-&gt;&gt;DW: Load to Data Warehouse\n    WF-&gt;&gt;WF: Trigger Downstream Workflows</code></pre>"},{"location":"examples/event-triggers/#emr-job-orchestration","title":"EMR Job Orchestration","text":"<pre><code>emr-job-orchestration:\n  type: Workflow\n  params:\n    cluster_id: str\n    job_flow_id: str\n    step_status: str\n\n  on:\n    batch_processing:\n      - job_name: \"emr-data-pipeline\"\n        job_status: \"completed\"\n        source: \"emr\"\n        timeout_minutes: 240\n        retry_count: 2\n        dependencies: [\"s3-data-availability\"]\n\n  jobs:\n    orchestrate-emr-jobs:\n      stages:\n        - name: \"Check EMR Cluster Status\"\n          uses: \"emr/check_cluster@latest\"\n          with:\n            cluster_id: \"${{ params.cluster_id }}\"\n\n        - name: \"Validate Job Results\"\n          uses: \"emr/validate_results@latest\"\n          with:\n            job_flow_id: \"${{ params.job_flow_id }}\"\n\n        - name: \"Process EMR Output\"\n          uses: \"emr/process_output@latest\"\n          with:\n            validated_results: \"${{ stages.validate-job-results.outputs.results }}\"\n\n        - name: \"Terminate Cluster\"\n          uses: \"emr/terminate_cluster@latest\"\n          with:\n            cluster_id: \"${{ params.cluster_id }}\"\n\n        - name: \"Send Completion Notification\"\n          uses: \"notifications/emr_completion@latest\"\n          with:\n            cluster_id: \"${{ params.cluster_id }}\"\n            job_flow_id: \"${{ params.job_flow_id }}\"\n            status: \"${{ params.step_status }}\"\n</code></pre> <p>Flow Trigger: AWS EMR \u2192 CloudWatch Events \u2192 Webhook \u2192 Workflow</p>"},{"location":"examples/event-triggers/#file-monitoring-triggers","title":"File Monitoring Triggers","text":""},{"location":"examples/event-triggers/#azure-storage-event-trigger","title":"Azure Storage Event Trigger","text":"<pre><code>azure-storage-triggered-etl:\n  type: Workflow\n  params:\n    blob_url: str\n    container_name: str\n    file_name: str\n\n  on:\n    file:\n      - path: \"/data/incoming\"\n        pattern: \"*.csv\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 30\n\n    # Fallback schedule\n    schedule:\n      - cron: \"0 2 * * *\"  # Daily at 2 AM\n        timezone: \"UTC\"\n\n  jobs:\n    process-file:\n      stages:\n        - name: \"Download from Azure\"\n          uses: \"azure/download_blob@latest\"\n          with:\n            blob_url: \"${{ params.blob_url }}\"\n            container: \"${{ params.container_name }}\"\n            file_name: \"${{ params.file_name }}\"\n\n        - name: \"Validate File\"\n          uses: \"validation/check_csv@latest\"\n          with:\n            file_path: \"${{ stages.download-from-azure.outputs.local_path }}\"\n\n        - name: \"Load to Database\"\n          uses: \"etl/load_csv@latest\"\n          with:\n            source: \"${{ stages.download-from-azure.outputs.local_path }}\"\n            target: \"warehouse.raw_data\"\n\n        - name: \"Send Notification\"\n          uses: \"notifications/send_alert@latest\"\n          with:\n            message: \"File processed successfully\"\n</code></pre> <p>Flow Trigger: Azure Storage Account \u2192 Event Grid \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant AS as Azure Storage\n    participant EG as Event Grid\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    AS-&gt;&gt;EG: File Created Event\n    EG-&gt;&gt;WH: HTTP POST with Event Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;AS: Download File\n    WF-&gt;&gt;WF: Validate File\n    WF-&gt;&gt;DB: Load to Database\n    WF-&gt;&gt;WF: Send Notification</code></pre>"},{"location":"examples/event-triggers/#aws-s3-event-trigger","title":"AWS S3 Event Trigger","text":"<pre><code>s3-triggered-processor:\n  type: Workflow\n  params:\n    bucket_name: str\n    object_key: str\n    event_type: str\n\n  on:\n    file:\n      - path: \"/data/raw\"\n        pattern: \"*.json\"\n        event_type: \"created\"\n        source: \"aws-s3\"\n        recursive: false\n        debounce_seconds: 60\n\n  jobs:\n    process-s3-object:\n      stages:\n        - name: \"Get S3 Object\"\n          uses: \"aws/get_s3_object@latest\"\n          with:\n            bucket: \"${{ params.bucket_name }}\"\n            key: \"${{ params.object_key }}\"\n\n        - name: \"Parse JSON Data\"\n          uses: \"processors/json_parser@latest\"\n          with:\n            data: \"${{ stages.get-s3-object.outputs.content }}\"\n\n        - name: \"Transform and Load\"\n          uses: \"etl/transform_json@latest\"\n          with:\n            data: \"${{ stages.parse-json-data.outputs.parsed_data }}\"\n</code></pre> <p>Flow Trigger: AWS S3 \u2192 S3 Event Notification \u2192 Lambda \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant S3 as AWS S3\n    participant SN as S3 Notifications\n    participant L as Lambda\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    S3-&gt;&gt;SN: Object Created Event\n    SN-&gt;&gt;L: Invoke Lambda Function\n    L-&gt;&gt;WH: HTTP POST with S3 Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;S3: Get S3 Object\n    WF-&gt;&gt;WF: Parse JSON Data\n    WF-&gt;&gt;DB: Transform and Load</code></pre>"},{"location":"examples/event-triggers/#google-cloud-storage-event-trigger","title":"Google Cloud Storage Event Trigger","text":"<pre><code>gcs-triggered-pipeline:\n  type: Workflow\n  params:\n    bucket_name: str\n    object_name: str\n\n  on:\n    file:\n      - path: \"/data/processed\"\n        pattern: \"*.parquet\"\n        event_type: \"created\"\n        source: \"google-cloud-storage\"\n        debounce_seconds: 45\n\n  jobs:\n    process-gcs-file:\n      stages:\n        - name: \"Download from GCS\"\n          uses: \"gcp/download_from_gcs@latest\"\n          with:\n            bucket: \"${{ params.bucket_name }}\"\n            object: \"${{ params.object_name }}\"\n\n        - name: \"Process Parquet\"\n          uses: \"processors/parquet_processor@latest\"\n          with:\n            file_path: \"${{ stages.download-from-gcs.outputs.local_path }}\"\n</code></pre> <p>Flow Trigger: Google Cloud Storage \u2192 Cloud Functions \u2192 Pub/Sub \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant GCS as Google Cloud Storage\n    participant CF as Cloud Functions\n    participant PS as Pub/Sub\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    GCS-&gt;&gt;CF: Object Created Event\n    CF-&gt;&gt;PS: Publish Message\n    PS-&gt;&gt;WH: HTTP POST with GCS Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;GCS: Download from GCS\n    WF-&gt;&gt;WF: Process Parquet\n    WF-&gt;&gt;DB: Load to Database</code></pre>"},{"location":"examples/event-triggers/#webhook-triggers","title":"Webhook Triggers","text":""},{"location":"examples/event-triggers/#external-api-integration","title":"External API Integration","text":"<pre><code>external-api-integration:\n  type: Workflow\n  params:\n    api_data: dict\n    source_system: str\n    event_timestamp: datetime\n\n  on:\n    webhook:\n      - endpoint: \"/webhook/external-api\"\n        method: \"POST\"\n        secret: \"${{ env.WEBHOOK_SECRET }}\"\n        headers:\n          \"X-Source-System\": \"${{ env.SOURCE_SYSTEM_HEADER }}\"\n        timeout_seconds: 30\n\n  jobs:\n    validate-and-process:\n      stages:\n        - name: \"Validate Webhook Data\"\n          run: |\n            if not params.get('api_data'):\n                raise Exception(\"No data received in webhook\")\n\n            # Validate required fields\n            required_fields = ['source', 'timestamp', 'payload']\n            for field in required_fields:\n                if field not in params['api_data']:\n                    raise Exception(f\"Missing required field: {field}\")\n\n        - name: \"Process API Data\"\n          uses: \"processors/api_data_processor@latest\"\n          with:\n            data: \"${{ params.api_data }}\"\n            source: \"${{ params.source_system }}\"\n\n        - name: \"Store Results\"\n          uses: \"storage/save_to_database@latest\"\n          with:\n            data: \"${{ stages.process-api-data.outputs.processed_data }}\"\n</code></pre> <p>Flow Trigger: External API \u2192 HTTP POST \u2192 Webhook Endpoint \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant API as External API\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    API-&gt;&gt;WH: HTTP POST with Data\n    WH-&gt;&gt;WF: Validate &amp; Trigger\n    WF-&gt;&gt;WF: Validate Webhook Data\n    WF-&gt;&gt;WF: Process API Data\n    WF-&gt;&gt;DB: Store Results\n    WF-&gt;&gt;API: Send Confirmation</code></pre>"},{"location":"examples/event-triggers/#third-party-service-integration","title":"Third-Party Service Integration","text":"<pre><code>third-party-integration:\n  type: Workflow\n  params:\n    event_data: dict\n    service_name: str\n\n  on:\n    webhook:\n      - endpoint: \"/webhook/salesforce\"\n        method: \"POST\"\n        headers:\n          \"X-Salesforce-Signature\": \"${{ env.SF_SIGNATURE }}\"\n        timeout_seconds: 45\n\n      - endpoint: \"/webhook/shopify\"\n        method: \"POST\"\n        headers:\n          \"X-Shopify-Hmac-Sha256\": \"${{ env.SHOPIFY_HMAC }}\"\n        timeout_seconds: 45\n\n      - endpoint: \"/webhook/zapier\"\n        method: \"POST\"\n        timeout_seconds: 60\n\n  jobs:\n    route-by-service:\n      stages:\n        - name: \"Route by Service\"\n          run: |\n            service = params.get('service_name', 'unknown')\n            if service == 'salesforce':\n                result.outputs = {\"processor\": \"salesforce-processor\"}\n            elif service == 'shopify':\n                result.outputs = {\"processor\": \"shopify-processor\"}\n            elif service == 'zapier':\n                result.outputs = {\"processor\": \"zapier-processor\"}\n            else:\n                result.outputs = {\"processor\": \"generic-processor\"}\n\n    process-event:\n      needs: [\"route-by-service\"]\n      stages:\n        - name: \"Process Event\"\n          trigger: \"${{ stages.route-by-service.outputs.processor }}\"\n          params:\n            event_data: \"${{ params.event_data }}\"\n            source: \"${{ params.service_name }}\"\n</code></pre> <p>Flow Trigger: Third-Party Service \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant SF as Salesforce\n    participant SP as Shopify\n    participant ZP as Zapier\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DB as Database\n\n    SF-&gt;&gt;WH: Salesforce Event\n    SP-&gt;&gt;WH: Shopify Event\n    ZP-&gt;&gt;WH: Zapier Event\n    WH-&gt;&gt;WF: Route by Service\n    WF-&gt;&gt;WF: Process Event\n    WF-&gt;&gt;DB: Store Processed Data</code></pre>"},{"location":"examples/event-triggers/#database-change-triggers","title":"Database Change Triggers","text":""},{"location":"examples/event-triggers/#database-change-data-capture-cdc-events","title":"Database Change Data Capture (CDC) Events","text":"<pre><code>database-cdc-pipeline:\n  type: Workflow\n  params:\n    table_name: str\n    change_data: dict\n    operation_type: str\n\n  on:\n    database:\n      - connection_string: \"${{ env.DATABASE_URL }}\"\n        table: \"user_events\"\n        operation: \"insert\"\n        source: \"debezium-cdc\"\n        check_interval_seconds: 0  # Real-time events\n\n    # Fallback schedule\n    schedule:\n      - cron: \"*/5 * * * *\"  # Every 5 minutes\n        timezone: \"UTC\"\n\n  jobs:\n    process-database-change:\n      stages:\n        - name: \"Validate Change Data\"\n          run: |\n            if not params.get('change_data'):\n                raise Exception(\"No change data received\")\n\n            operation = params.get('operation_type')\n            if operation not in ['insert', 'update', 'delete']:\n                raise Exception(f\"Invalid operation type: {operation}\")\n\n        - name: \"Process Change\"\n          uses: \"processors/cdc_processor@latest\"\n          with:\n            change_data: \"${{ params.change_data }}\"\n            operation: \"${{ params.operation_type }}\"\n            table: \"${{ params.table_name }}\"\n\n        - name: \"Update Data Warehouse\"\n          uses: \"warehouse/update_dimension@latest\"\n          with:\n            changes: \"${{ stages.process-change.outputs.processed_changes }}\"\n</code></pre> <p>Flow Trigger: Database \u2192 Debezium CDC \u2192 Kafka \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant DB as Database\n    participant DC as Debezium CDC\n    participant KF as Kafka\n    participant WH as Webhook\n    participant WF as Workflow\n    participant DW as Data Warehouse\n\n    DB-&gt;&gt;DC: Database Change Event\n    DC-&gt;&gt;KF: Publish Change to Topic\n    KF-&gt;&gt;WH: HTTP POST with Change Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Validate Change Data\n    WF-&gt;&gt;WF: Process Change\n    WF-&gt;&gt;DW: Update Data Warehouse</code></pre>"},{"location":"examples/event-triggers/#database-replication-events","title":"Database Replication Events","text":"<pre><code>database-replication-pipeline:\n  type: Workflow\n  params:\n    replication_data: dict\n    source_database: str\n\n  on:\n    database:\n      - connection_string: \"${{ env.REPLICATION_DB_URL }}\"\n        table: \"orders\"\n        operation: \"update\"\n        source: \"database-replication\"\n        check_interval_seconds: 0  # Real-time events\n\n  jobs:\n    sync-replication-data:\n      stages:\n        - name: \"Process Replication Event\"\n          uses: \"replication/process_event@latest\"\n          with:\n            data: \"${{ params.replication_data }}\"\n            source_db: \"${{ params.source_database }}\"\n\n        - name: \"Sync to Target\"\n          uses: \"replication/sync_to_target@latest\"\n          with:\n            processed_data: \"${{ stages.process-replication-event.outputs.processed_data }}\"\n</code></pre> <p>Flow Trigger: Source Database \u2192 Replication Service \u2192 Event Stream \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant SDB as Source Database\n    participant RS as Replication Service\n    participant ES as Event Stream\n    participant WH as Webhook\n    participant WF as Workflow\n    participant TDB as Target Database\n\n    SDB-&gt;&gt;RS: Database Replication Event\n    RS-&gt;&gt;ES: Publish to Event Stream\n    ES-&gt;&gt;WH: HTTP POST with Replication Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Process Replication Event\n    WF-&gt;&gt;TDB: Sync to Target</code></pre>"},{"location":"examples/event-triggers/#sensor-based-triggers","title":"Sensor-Based Triggers","text":""},{"location":"examples/event-triggers/#monitoring-system-alerts","title":"Monitoring System Alerts","text":"<pre><code>monitoring-alert-pipeline:\n  type: Workflow\n  params:\n    alert_data: dict\n    metric_name: str\n    threshold_value: float\n\n  on:\n    sensor:\n      - sensor_type: \"system_metrics\"\n        threshold: 80.0\n        operator: \"gt\"\n        source: \"prometheus-alertmanager\"\n        check_interval_seconds: 0  # Real-time alerts\n        window_size: 300\n\n  jobs:\n    handle-system-alert:\n      stages:\n        - name: \"Process Alert\"\n          uses: \"monitoring/process_alert@latest\"\n          with:\n            alert_data: \"${{ params.alert_data }}\"\n            metric: \"${{ params.metric_name }}\"\n            threshold: \"${{ params.threshold_value }}\"\n\n        - name: \"Send Notification\"\n          uses: \"notifications/send_alert@latest\"\n          with:\n            level: \"warning\"\n            message: \"System alert triggered\"\n            details: \"${{ stages.process-alert.outputs.alert_details }}\"\n\n        - name: \"Scale Resources\"\n          if: \"${{ stages.process-alert.outputs.needs_scaling }}\"\n          uses: \"infrastructure/auto_scale@latest\"\n          with:\n            scaling_action: \"${{ stages.process-alert.outputs.scaling_action }}\"\n</code></pre> <p>Flow Trigger: Prometheus \u2192 AlertManager \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant PM as Prometheus\n    participant AM as AlertManager\n    participant WH as Webhook\n    participant WF as Workflow\n    participant INF as Infrastructure\n\n    PM-&gt;&gt;AM: System Alert (CPU &gt; 80%)\n    AM-&gt;&gt;WH: HTTP POST with Alert Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Process Alert\n    WF-&gt;&gt;WF: Send Notification\n    WF-&gt;&gt;INF: Scale Resources</code></pre>"},{"location":"examples/event-triggers/#business-metrics-monitoring","title":"Business Metrics Monitoring","text":"<pre><code>business-metrics-pipeline:\n  type: Workflow\n  params:\n    metric_data: dict\n    business_unit: str\n\n  on:\n    sensor:\n      - sensor_type: \"business_metrics\"\n        threshold: 1000\n        operator: \"lt\"\n        source: \"datadog-monitoring\"\n        check_interval_seconds: 0  # Real-time alerts\n        window_size: 3600\n\n  jobs:\n    handle-business-alert:\n      stages:\n        - name: \"Analyze Business Metric\"\n          uses: \"business/analyze_metric@latest\"\n          with:\n            metric_data: \"${{ params.metric_data }}\"\n            business_unit: \"${{ params.business_unit }}\"\n\n        - name: \"Send Business Alert\"\n          uses: \"notifications/business_alert@latest\"\n          with:\n            alert_type: \"business_metric\"\n            details: \"${{ stages.analyze-business-metric.outputs.analysis }}\"\n\n        - name: \"Trigger Business Action\"\n          if: \"${{ stages.analyze-business-metric.outputs.action_needed }}\"\n          uses: \"business/trigger_action@latest\"\n          with:\n            action: \"${{ stages.analyze-business-metric.outputs.recommended_action }}\"\n</code></pre> <p>Flow Trigger: DataDog \u2192 Monitoring \u2192 Webhook \u2192 Workflow</p> <pre><code>sequenceDiagram\n    participant DD as DataDog\n    participant MON as Monitoring\n    participant WH as Webhook\n    participant WF as Workflow\n    participant BUS as Business System\n\n    DD-&gt;&gt;MON: Business Metric Alert\n    MON-&gt;&gt;WH: HTTP POST with Metric Data\n    WH-&gt;&gt;WF: Trigger Workflow\n    WF-&gt;&gt;WF: Analyze Business Metric\n    WF-&gt;&gt;WF: Send Business Alert\n    WF-&gt;&gt;BUS: Trigger Business Action</code></pre>"},{"location":"examples/event-triggers/#polling-based-triggers","title":"Polling-Based Triggers","text":""},{"location":"examples/event-triggers/#legacy-system-integration","title":"Legacy System Integration","text":"<pre><code>legacy-system-polling:\n  type: Workflow\n  params:\n    file_path: str\n    file_size: int\n\n  on:\n    polling:\n      - resource_type: \"file\"\n        resource_path: \"/data/legacy/input.csv\"\n        check_interval_seconds: 300  # Every 5 minutes\n        condition: \"file_size &gt; 0\"\n        timeout_seconds: 30\n        max_retries: 3\n\n    # Fallback schedule\n    schedule:\n      - cron: \"0 */2 * * *\"  # Every 2 hours\n        timezone: \"UTC\"\n\n  jobs:\n    process-legacy-file:\n      stages:\n        - name: \"Check File Status\"\n          run: |\n            import os\n            file_path = params.get('file_path')\n            if os.path.exists(file_path):\n                file_size = os.path.getsize(file_path)\n                result.outputs = {\"file_size\": file_size, \"exists\": True}\n            else:\n                result.outputs = {\"file_size\": 0, \"exists\": False}\n\n        - name: \"Process File\"\n          if: \"${{ stages.check-file-status.outputs.exists }}\"\n          uses: \"legacy/process_file@latest\"\n          with:\n            file_path: \"${{ params.file_path }}\"\n</code></pre> <p>Flow Trigger: Workflow System \u2192 Poll File System \u2192 Check Condition \u2192 Execute</p> <pre><code>sequenceDiagram\n    participant WF as Workflow System\n    participant FS as File System\n    participant WF2 as Workflow Execution\n\n    loop Every 5 minutes\n        WF-&gt;&gt;FS: Check File Status\n        FS-&gt;&gt;WF: Return File Size\n        alt File Size &gt; 0\n            WF-&gt;&gt;WF2: Trigger Workflow\n            WF2-&gt;&gt;FS: Process File\n        end\n    end</code></pre>"},{"location":"examples/event-triggers/#database-polling-for-legacy-systems","title":"Database Polling for Legacy Systems","text":"<pre><code>legacy-database-polling:\n  type: Workflow\n  params:\n    table_name: str\n    last_processed_id: int\n\n  on:\n    polling:\n      - resource_type: \"database\"\n        resource_path: \"legacy_db.orders\"\n        check_interval_seconds: 180  # Every 3 minutes\n        condition: \"new_records_count &gt; 0\"\n        timeout_seconds: 45\n        max_retries: 2\n\n  jobs:\n    check-new-records:\n      stages:\n        - name: \"Query New Records\"\n          run: |\n            import psycopg2\n            conn = psycopg2.connect(params.get('connection_string'))\n            cursor = conn.cursor()\n\n            cursor.execute(\"\"\"\n                SELECT COUNT(*) FROM orders\n                WHERE id &gt; %s AND created_at &gt; NOW() - INTERVAL '1 hour'\n            \"\"\", (params.get('last_processed_id'),))\n\n            count = cursor.fetchone()[0]\n            result.outputs = {\"new_records_count\": count}\n\n        - name: \"Process New Records\"\n          if: \"${{ stages.query-new-records.outputs.new_records_count &gt; 0 }}\"\n          uses: \"legacy/process_orders@latest\"\n          with:\n            last_id: \"${{ params.last_processed_id }}\"\n</code></pre> <p>Flow Trigger: Workflow System \u2192 Poll Database \u2192 Check Condition \u2192 Execute</p> <pre><code>sequenceDiagram\n    participant WF as Workflow System\n    participant DB as Legacy Database\n    participant WF2 as Workflow Execution\n\n    loop Every 3 minutes\n        WF-&gt;&gt;DB: Query New Records\n        DB-&gt;&gt;WF: Return Record Count\n        alt New Records &gt; 0\n            WF-&gt;&gt;WF2: Trigger Workflow\n            WF2-&gt;&gt;DB: Process New Records\n        end\n    end</code></pre>"},{"location":"examples/event-triggers/#api-polling-for-external-services","title":"API Polling for External Services","text":"<pre><code>api-polling-pipeline:\n  type: Workflow\n  params:\n    api_endpoint: str\n    api_key: str\n\n  on:\n    polling:\n      - resource_type: \"api\"\n        resource_path: \"https://api.external-service.com/status\"\n        check_interval_seconds: 600  # Every 10 minutes\n        condition: \"status == 'ready'\"\n        timeout_seconds: 60\n        max_retries: 3\n\n  jobs:\n    check-api-status:\n      stages:\n        - name: \"Check API Status\"\n          run: |\n            import requests\n\n            response = requests.get(\n                params.get('api_endpoint'),\n                headers={'Authorization': f\"Bearer {params.get('api_key')}\"},\n                timeout=30\n            )\n\n            if response.status_code == 200:\n                data = response.json()\n                result.outputs = {\"status\": data.get('status'), \"data\": data}\n            else:\n                raise Exception(f\"API check failed: {response.status_code}\")\n\n        - name: \"Process API Data\"\n          if: \"${{ stages.check-api-status.outputs.status == 'ready' }}\"\n          uses: \"api/process_data@latest\"\n          with:\n            data: \"${{ stages.check-api-status.outputs.data }}\"\n</code></pre> <p>Flow Trigger: Workflow System \u2192 Poll API \u2192 Check Condition \u2192 Execute</p> <pre><code>sequenceDiagram\n    participant WF as Workflow System\n    participant API as External API\n    participant WF2 as Workflow Execution\n\n    loop Every 10 minutes\n        WF-&gt;&gt;API: Check API Status\n        API-&gt;&gt;WF: Return Status Data\n        alt Status == 'ready'\n            WF-&gt;&gt;WF2: Trigger Workflow\n            WF2-&gt;&gt;API: Process API Data\n        end\n    end</code></pre>"},{"location":"examples/event-triggers/#queue-monitoring","title":"Queue Monitoring","text":"<pre><code>queue-monitoring-pipeline:\n  type: Workflow\n  params:\n    queue_name: str\n    threshold: int\n\n  on:\n    polling:\n      - resource_type: \"queue\"\n        resource_path: \"processing_queue\"\n        check_interval_seconds: 120  # Every 2 minutes\n        condition: \"queue_size &gt; threshold\"\n        timeout_seconds: 30\n        max_retries: 2\n\n  jobs:\n    monitor-queue:\n      stages:\n        - name: \"Check Queue Size\"\n          run: |\n            import redis\n\n            r = redis.Redis(host='localhost', port=6379, db=0)\n            queue_size = r.llen(params.get('queue_name'))\n\n            result.outputs = {\n                \"queue_size\": queue_size,\n                \"threshold\": params.get('threshold'),\n                \"needs_processing\": queue_size &gt; params.get('threshold')\n            }\n\n        - name: \"Scale Processing\"\n          if: \"${{ stages.check-queue-size.outputs.needs_processing }}\"\n          uses: \"infrastructure/scale_workers@latest\"\n          with:\n            queue_size: \"${{ stages.check-queue-size.outputs.queue_size }}\"\n</code></pre> <p>Flow Trigger: Workflow System \u2192 Poll Queue \u2192 Check Condition \u2192 Execute</p> <pre><code>sequenceDiagram\n    participant WF as Workflow System\n    participant Q as Redis Queue\n    participant WF2 as Workflow Execution\n    participant INF as Infrastructure\n\n    loop Every 2 minutes\n        WF-&gt;&gt;Q: Check Queue Size\n        Q-&gt;&gt;WF: Return Queue Length\n        alt Queue Size &gt; Threshold\n            WF-&gt;&gt;WF2: Trigger Workflow\n            WF2-&gt;&gt;INF: Scale Processing\n        end\n    end</code></pre>"},{"location":"examples/event-triggers/#combined-trigger-scenarios","title":"Combined Trigger Scenarios","text":""},{"location":"examples/event-triggers/#multi-source-data-pipeline","title":"Multi-Source Data Pipeline","text":"<pre><code>graph TB\n    subgraph \"Trigger Sources\"\n        CRON[Cron Schedule&lt;br/&gt;Every 6 hours]\n        FILE[File Event&lt;br/&gt;Azure Storage]\n        WEB[Webhook Event&lt;br/&gt;External API]\n        DB[Database Event&lt;br/&gt;CDC]\n        SENSOR[Sensor Event&lt;br/&gt;Prometheus]\n        POLL[Polling Event&lt;br/&gt;Legacy System]\n    end\n\n    subgraph \"Event Processing\"\n        ET[Event Trigger]\n        ROUTE[Route by Source]\n    end\n\n    subgraph \"Data Pipeline\"\n        EXTRACT[Extract Data]\n        TRANSFORM[Transform Data]\n        LOAD[Load to Warehouse]\n        NOTIFY[Send Notifications]\n    end\n\n    %% Trigger connections\n    CRON --&gt; ET\n    FILE --&gt; ET\n    WEB --&gt; ET\n    DB --&gt; ET\n    SENSOR --&gt; ET\n    POLL --&gt; ET\n\n    %% Processing flow\n    ET --&gt; ROUTE\n    ROUTE --&gt; EXTRACT\n    EXTRACT --&gt; TRANSFORM\n    TRANSFORM --&gt; LOAD\n    LOAD --&gt; NOTIFY\n\n    classDef trigger fill:#e3f2fd\n    classDef process fill:#f3e5f5\n    classDef pipeline fill:#e8f5e8\n\n    class CRON,FILE,WEB,DB,SENSOR,POLL trigger\n    class ET,ROUTE process\n    class EXTRACT,TRANSFORM,LOAD,NOTIFY pipeline</code></pre> <pre><code>multi-source-pipeline:\n  type: Workflow\n  params:\n    trigger_source: str\n    data_payload: dict\n\n  on:\n    # Time-based triggers\n    schedule:\n      - cron: \"0 */6 * * *\"  # Every 6 hours\n        timezone: \"UTC\"\n\n    # File-based triggers from cloud storage\n    file:\n      - path: \"/data/priority\"\n        pattern: \"urgent_*.csv\"\n        event_type: \"created\"\n        source: \"azure-storage\"\n        debounce_seconds: 10\n\n    # Webhook triggers from external systems\n    webhook:\n      - endpoint: \"/webhook/urgent-data\"\n        method: \"POST\"\n        timeout_seconds: 15\n\n    # Database triggers from CDC\n    database:\n      - connection_string: \"${{ env.PRIORITY_DB_URL }}\"\n        table: \"urgent_orders\"\n        operation: \"insert\"\n        source: \"debezium-cdc\"\n        check_interval_seconds: 0\n\n    # Sensor triggers from monitoring\n    sensor:\n      - sensor_type: \"queue_size\"\n        threshold: 1000\n        operator: \"gt\"\n        source: \"prometheus-alertmanager\"\n        check_interval_seconds: 0\n        window_size: 300\n\n    # Polling triggers for legacy systems\n    polling:\n      - resource_type: \"file\"\n        resource_path: \"/data/legacy/backup.csv\"\n        check_interval_seconds: 3600  # Every hour\n        condition: \"file_size &gt; 0\"\n        timeout_seconds: 30\n\n  jobs:\n    determine-trigger-source:\n      stages:\n        - name: \"Analyze Trigger Source\"\n          run: |\n            source = params.get('trigger_source', 'unknown')\n            result.outputs = {\"source\": source}\n\n    process-data:\n      needs: [\"determine-trigger-source\"]\n      stages:\n        - name: \"Extract Data\"\n          uses: \"extractors/multi_source@latest\"\n          with:\n            source: \"${{ stages.analyze-trigger-source.outputs.source }}\"\n            data: \"${{ params.data_payload }}\"\n\n        - name: \"Transform Data\"\n          uses: \"transformers/data_processor@latest\"\n          with:\n            raw_data: \"${{ stages.extract-data.outputs.extracted_data }}\"\n\n        - name: \"Load Results\"\n          uses: \"loaders/warehouse_loader@latest\"\n          with:\n            processed_data: \"${{ stages.transform-data.outputs.transformed_data }}\"\n\n        - name: \"Send Notifications\"\n          uses: \"notifications/completion_alert@latest\"\n          with:\n            source: \"${{ stages.analyze-trigger-source.outputs.source }}\"\n</code></pre>"},{"location":"examples/event-triggers/#configuration-examples","title":"Configuration Examples","text":""},{"location":"examples/event-triggers/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>from ddeutil.workflow import Event, FileEvent, WebhookEvent, DatabaseEvent, SensorEvent\n\n# Development environment\ndev_events = Event(\n    file=[\n        FileEvent(\n            path=\"./data/test\",\n            pattern=\"*.csv\",\n            source=\"local-filesystem\",\n            debounce_seconds=5\n        )\n    ],\n    webhook=[\n        WebhookEvent(\n            endpoint=\"/webhook/test\",\n            timeout_seconds=10\n        )\n    ]\n)\n\n# Production environment\nprod_events = Event(\n    file=[\n        FileEvent(\n            path=\"/data/production\",\n            pattern=\"*.csv\",\n            source=\"azure-storage\",\n            debounce_seconds=60,\n            recursive=True\n        )\n    ],\n    webhook=[\n        WebhookEvent(\n            endpoint=\"/webhook/production\",\n            secret=\"prod-secret-key\",\n            timeout_seconds=30\n        )\n    ],\n    database=[\n        DatabaseEvent(\n            connection_string=\"postgresql://prod-db\",\n            table=\"orders\",\n            source=\"debezium-cdc\",\n            check_interval_seconds=0\n        )\n    ],\n    sensor=[\n        SensorEvent(\n            sensor_type=\"system_health\",\n            threshold=90.0,\n            operator=\"gt\",\n            source=\"prometheus-alertmanager\",\n            check_interval_seconds=0\n        )\n    ],\n    polling=[\n        PollingEvent(\n            resource_type=\"file\",\n            resource_path=\"/data/legacy/backup.csv\",\n            check_interval_seconds=3600,\n            condition=\"file_size &gt; 0\",\n            timeout_seconds=30\n        )\n    ],\n    message_queue=[\n        MessageQueueEvent(\n            queue_name=\"data-processing-queue\",\n            source=\"rabbitmq\",\n            batch_size=20,\n            visibility_timeout=300\n        )\n    ],\n    stream_processing=[\n        StreamProcessingEvent(\n            stream_name=\"clickstream-events\",\n            window_size=300,\n            aggregation_type=\"count\",\n            source=\"kafka\"\n        )\n    ],\n    batch_processing=[\n        BatchProcessingEvent(\n            job_name=\"daily-data-processing\",\n            job_status=\"completed\",\n            source=\"spark\",\n            timeout_minutes=180\n        )\n    ]\n)\n</code></pre>"},{"location":"examples/event-triggers/#event-trigger-flow-summary","title":"Event Trigger Flow Summary","text":""},{"location":"examples/event-triggers/#file-events","title":"File Events","text":"<ul> <li>Azure Storage: Storage Account \u2192 Event Grid \u2192 Webhook \u2192 Workflow</li> <li>AWS S3: S3 Bucket \u2192 S3 Event Notification \u2192 Lambda \u2192 Webhook \u2192 Workflow</li> <li>Google Cloud Storage: GCS Bucket \u2192 Cloud Functions \u2192 Pub/Sub \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#webhook-events","title":"Webhook Events","text":"<ul> <li>External APIs: API Service \u2192 HTTP POST \u2192 Webhook Endpoint \u2192 Workflow</li> <li>Third-Party Services: Service \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#database-events","title":"Database Events","text":"<ul> <li>CDC: Database \u2192 Debezium \u2192 Kafka \u2192 Webhook \u2192 Workflow</li> <li>Replication: Source DB \u2192 Replication Service \u2192 Event Stream \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#sensor-events","title":"Sensor Events","text":"<ul> <li>Monitoring Systems: Prometheus \u2192 AlertManager \u2192 Webhook \u2192 Workflow</li> <li>Business Metrics: DataDog \u2192 Monitoring \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#polling-events","title":"Polling Events","text":"<ul> <li>Legacy Systems: Workflow System \u2192 Poll Resource \u2192 Check Condition \u2192 Execute</li> <li>Fallback Mechanisms: Workflow System \u2192 Poll External Service \u2192 Check Status \u2192 Execute</li> </ul>"},{"location":"examples/event-triggers/#message-queue-events","title":"Message Queue Events","text":"<ul> <li>RabbitMQ: RabbitMQ \u2192 Message Queue \u2192 Webhook \u2192 Workflow</li> <li>Apache Kafka: Kafka \u2192 Stream Consumer \u2192 Webhook \u2192 Workflow</li> <li>AWS SQS: SQS \u2192 Lambda \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#stream-processing-events","title":"Stream Processing Events","text":"<ul> <li>Kafka Streams: Kafka Stream \u2192 Stream Processor \u2192 Webhook \u2192 Workflow</li> <li>Apache Flink: Flink \u2192 Stream Job \u2192 Webhook \u2192 Workflow</li> <li>AWS Kinesis: Kinesis \u2192 Stream Processor \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#batch-processing-events","title":"Batch Processing Events","text":"<ul> <li>Apache Spark: Spark Cluster \u2192 Job Monitor \u2192 Webhook \u2192 Workflow</li> <li>AWS EMR: EMR \u2192 CloudWatch Events \u2192 Webhook \u2192 Workflow</li> <li>Azure HDInsight: HDInsight \u2192 Job Monitor \u2192 Webhook \u2192 Workflow</li> </ul>"},{"location":"examples/event-triggers/#complete-event-trigger-flow-diagram","title":"Complete Event Trigger Flow Diagram","text":"<pre><code>graph TB\n    subgraph \"External Services\"\n        subgraph \"Cloud Storage\"\n            AS[Azure Storage]\n            S3[AWS S3]\n            GCS[Google Cloud Storage]\n        end\n\n        subgraph \"APIs &amp; Services\"\n            API[External APIs]\n            SF[Salesforce]\n            SP[Shopify]\n        end\n\n        subgraph \"Databases\"\n            DB[Source Database]\n            LDB[Legacy Database]\n        end\n\n        subgraph \"Monitoring\"\n            PM[Prometheus]\n            DD[DataDog]\n        end\n\n        subgraph \"Legacy Systems\"\n            FS[File System]\n            Q[Redis Queue]\n        end\n    end\n\n    subgraph \"Event Detection &amp; Routing\"\n        subgraph \"Event-Driven\"\n            EG[Event Grid]\n            SN[S3 Notifications]\n            CF[Cloud Functions]\n            WH[Webhooks]\n            CDC[Debezium CDC]\n            AM[AlertManager]\n        end\n\n        subgraph \"Polling\"\n            POL[Polling Engine]\n        end\n    end\n\n    subgraph \"Workflow System\"\n        ET[Event Trigger]\n        WF[Workflow Execution]\n        JO[Job Orchestration]\n    end\n\n    subgraph \"Data Pipeline\"\n        EX[Extract]\n        TR[Transform]\n        LO[Load]\n        NO[Notify]\n    end\n\n    %% Cloud Storage Events\n    AS --&gt; EG\n    S3 --&gt; SN\n    GCS --&gt; CF\n\n    %% API Events\n    API --&gt; WH\n    SF --&gt; WH\n    SP --&gt; WH\n\n    %% Database Events\n    DB --&gt; CDC\n\n    %% Monitoring Events\n    PM --&gt; AM\n    DD --&gt; AM\n\n    %% Legacy System Polling\n    FS --&gt; POL\n    LDB --&gt; POL\n    Q --&gt; POL\n\n    %% Event Processing\n    EG --&gt; ET\n    SN --&gt; ET\n    CF --&gt; ET\n    WH --&gt; ET\n    CDC --&gt; ET\n    AM --&gt; ET\n    POL --&gt; ET\n\n    %% Workflow Execution\n    ET --&gt; WF\n    WF --&gt; JO\n\n    %% Pipeline Stages\n    JO --&gt; EX\n    EX --&gt; TR\n    TR --&gt; LO\n    LO --&gt; NO\n\n    classDef cloud fill:#e1f5fe\n    classDef api fill:#f3e5f5\n    classDef database fill:#e8f5e8\n    classDef monitoring fill:#fff3e0\n    classDef legacy fill:#ffebee\n    classDef event fill:#f1f8e9\n    classDef workflow fill:#e0f2f1\n    classDef pipeline fill:#fce4ec\n\n    class AS,S3,GCS cloud\n    class API,SF,SP api\n    class DB,LDB database\n    class PM,DD monitoring\n    class FS,Q legacy\n    class EG,SN,CF,WH,CDC,AM,POL event\n    class ET,WF,JO workflow\n    class EX,TR,LO,NO pipeline</code></pre>"},{"location":"examples/event-triggers/#best-practices","title":"Best Practices","text":""},{"location":"examples/event-triggers/#1-event-trigger-design","title":"1. Event Trigger Design","text":"<ul> <li>Use external services to detect and trigger events, not internal polling</li> <li>Implement proper authentication for all webhook endpoints</li> <li>Use debounce mechanisms to prevent duplicate processing</li> <li>Validate event sources before processing</li> </ul>"},{"location":"examples/event-triggers/#2-performance-considerations","title":"2. Performance Considerations","text":"<ul> <li>Limit the number of events per workflow (see validation limits)</li> <li>Use efficient event routing to minimize processing overhead</li> <li>Implement proper error handling for all trigger types</li> <li>Monitor event processing performance</li> </ul>"},{"location":"examples/event-triggers/#3-security","title":"3. Security","text":"<ul> <li>Use secrets for webhook authentication</li> <li>Validate webhook signatures</li> <li>Implement proper access controls</li> <li>Secure all external service connections</li> </ul>"},{"location":"examples/event-triggers/#4-monitoring","title":"4. Monitoring","text":"<ul> <li>Track event trigger frequencies to identify patterns</li> <li>Monitor external service health and availability</li> <li>Set up alerts for trigger failures</li> <li>Log event processing for debugging</li> </ul>"},{"location":"examples/event-triggers/#validation-limits","title":"Validation Limits","text":"<p>The system enforces the following limits to prevent resource exhaustion:</p> <ul> <li>Schedule events: Maximum 10 per workflow</li> <li>File events: Maximum 20 per workflow</li> <li>Webhook events: Maximum 10 per workflow</li> <li>Database events: Maximum 5 per workflow</li> <li>Sensor events: Maximum 15 per workflow</li> <li>Polling events: Maximum 8 per workflow</li> <li>Message queue events: Maximum 5 per workflow</li> <li>Stream processing events: Maximum 3 per workflow</li> <li>Batch processing events: Maximum 5 per workflow</li> </ul> <p>These limits ensure system stability while allowing for complex trigger configurations.</p>"},{"location":"examples/event-triggers/#advanced-event-types-and-scenarios","title":"Advanced Event Types and Scenarios","text":""},{"location":"examples/event-triggers/#data-quality-event","title":"Data Quality Event","text":"<p>Example Trigger: <pre><code>on:\n  data_quality:\n    - quality_metric: \"completeness\"\n      threshold: 0.95\n      operator: \"lt\"\n      source: \"great-expectations\"\n      dataset_name: \"orders\"\n      validation_rules: [\"no_nulls\", \"unique_id\"]\n      severity: \"high\"\n</code></pre> Scenarios: - Data fails a completeness check (e.g., missing values in required columns) - Uniqueness constraint is violated in a key field - Data accuracy drops below a business threshold - Timeliness of data ingestion is not met</p>"},{"location":"examples/event-triggers/#api-rate-limit-event","title":"API Rate Limit Event","text":"<p>Example Trigger: <pre><code>on:\n  api_rate_limit:\n    - api_name: \"external-analytics-api\"\n      rate_limit_type: \"requests_per_minute\"\n      current_usage: 98.5\n      reset_time: 120\n      source: \"aws-api-gateway\"\n      retry_strategy: \"exponential_backoff\"\n</code></pre> Scenarios: - API usage approaches or exceeds quota - Burst of requests triggers throttling - Third-party API returns HTTP 429 Too Many Requests - Scheduled batch jobs hit rate limits</p>"},{"location":"examples/event-triggers/#data-lineage-event","title":"Data Lineage Event","text":"<p>Example Trigger: <pre><code>on:\n  data_lineage:\n    - lineage_type: \"schema_change\"\n      source_dataset: \"raw_orders\"\n      target_dataset: \"clean_orders\"\n      transformation_type: \"column_rename\"\n      source: \"apache-atlas\"\n      impact_level: \"medium\"\n      dependencies: [\"raw_orders\"]\n</code></pre> Scenarios: - Schema changes detected in upstream datasets - New data flow or transformation is registered - Data dependency is updated or removed - Impact analysis shows downstream breakage risk</p>"},{"location":"examples/event-triggers/#ml-pipeline-event","title":"ML Pipeline Event","text":"<p>Example Trigger: <pre><code>on:\n  ml_pipeline:\n    - pipeline_stage: \"training\"\n      model_name: \"churn-predictor\"\n      metric_name: \"accuracy\"\n      threshold: 0.9\n      operator: \"lt\"\n      source: \"mlflow\"\n      model_version: \"v1.2.3\"\n      drift_detected: false\n</code></pre> Scenarios: - Model training completes (success or failure) - Model accuracy drops below threshold - Model drift is detected in production - New model version is deployed</p>"},{"location":"examples/event-triggers/#data-catalog-event","title":"Data Catalog Event","text":"<p>Example Trigger: <pre><code>on:\n  data_catalog:\n    - catalog_event_type: \"metadata_update\"\n      dataset_name: \"customer_profiles\"\n      metadata_type: \"schema\"\n      source: \"aws-glue-catalog\"\n      governance_level: \"confidential\"\n      tags: [\"pii\", \"customer\"]\n</code></pre> Scenarios: - Metadata is updated for a dataset (e.g., schema, tags) - New dataset is discovered in the catalog - Governance policy changes for a dataset - Access request is made for restricted data</p>"},{"location":"examples/event-triggers/#infrastructure-event","title":"Infrastructure Event","text":"<p>Example Trigger: <pre><code>on:\n  infrastructure:\n    - infrastructure_type: \"compute\"\n      event_type: \"scaling\"\n      resource_name: \"etl-worker-group\"\n      current_utilization: 92.0\n      threshold: 90.0\n      source: \"cloudwatch\"\n      scaling_action: \"scale_up\"\n</code></pre> Scenarios: - Compute resource utilization exceeds threshold - Auto-scaling event is triggered - Infrastructure failure or maintenance event - Cost optimization recommendation is issued</p>"},{"location":"examples/event-triggers/#compliance-event","title":"Compliance Event","text":"<p>Example Trigger: <pre><code>on:\n  compliance:\n    - compliance_type: \"audit\"\n      regulation_name: \"GDPR\"\n      audit_scope: \"data-retention\"\n      source: \"compliance-monitor\"\n      deadline: 30\n      severity: \"critical\"\n</code></pre> Scenarios: - Scheduled or ad-hoc audit is required - Regulatory change is detected (e.g., new law) - Data retention policy deadline is approaching - Privacy or security incident is reported</p>"},{"location":"examples/event-triggers/#business-event","title":"Business Event","text":"<p>Example Trigger: <pre><code>on:\n  business:\n    - business_event_type: \"business_hours\"\n      business_unit: \"finance\"\n      event_time: \"2024-07-01T09:00:00Z\"\n      source: \"business-calendar\"\n      priority: \"high\"\n      business_rules: [\"end_of_quarter\", \"reporting\"]\n</code></pre> Scenarios: - Start or end of business hours triggers workflow - Seasonal campaign or holiday event - Business rule or policy change - End-of-quarter or reporting period</p>"},{"location":"examples/pg/","title":"Extract Postgres","text":"<p>The call stage is the call Python function from any registry location.</p>"},{"location":"examples/pg/#getting-started","title":"Getting Started","text":"<p>First, you should start create your call.</p>"},{"location":"examples/pg/#examples","title":"Examples","text":""},{"location":"examples/pg/#call-extract-load","title":"Call (Extract &amp; Load)","text":"<pre><code>wf_el_pg_to_lake:\n  type: Workflow\n  params:\n    run-date: datetime\n    author-email: str\n  jobs:\n    extract-load:\n      stages:\n        - name: \"Extract Load from Postgres to Lake\"\n          id: extract-load\n          uses: tasks/postgres-to-delta@polars\n          with:\n            source:\n              conn: conn_postgres_url\n              query: |\n                select * from ${{ params.name }}\n                where update_date = '${{ params.datetime }}'\n            sink:\n              conn: conn_az_lake\n              endpoint: \"/${{ params.name }}\"\n</code></pre> <p>Implement call:</p> <pre><code>from ddeutil.workflow.caller import tag\n\n@tag('polars', alias='postgres-to-delta')\ndef postgres_to_delta(source, sink):\n    return {\n        \"source\": source, \"sink\": sink\n    }\n</code></pre>"},{"location":"examples/pg/#call-transform","title":"Call (Transform)","text":"<pre><code>wf_call_mssql_proc:\n  type: Workflow\n  params:\n    run_date: datetime\n    sp_name: str\n    source_name: str\n    target_name: str\n  jobs:\n    transform:\n      stages:\n        - name: \"Transform Data in MS SQL Server\"\n          id: transform\n          uses: tasks/mssql-proc@odbc\n          with:\n            exec: ${{ params.sp_name }}\n            params:\n              run_mode: \"T\"\n              run_date: ${{ params.run_date }}\n              source: ${{ params.source_name }}\n              target: ${{ params.target_name }}\n</code></pre> <p>Implement call:</p> <pre><code>from ddeutil.workflow.caller import tag\n\n@tag('odbc', alias='mssql-proc')\ndef odbc_mssql_procedure(_exec: str, params: dict):\n    return {\n        \"exec\": _exec, \"params\": params\n    }\n</code></pre>"},{"location":"examples/codes/quick_start_guide/","title":"Quick Start Guide: Optimized Tracing System","text":""},{"location":"examples/codes/quick_start_guide/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>The optimized tracing system provides 2-5x performance improvements over the traditional <code>FileTrace</code> model. Here's how to use it:</p>"},{"location":"examples/codes/quick_start_guide/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/codes/quick_start_guide/#1-simple-logging-recommended","title":"1. Simple Logging (Recommended)","text":"<pre><code>from ddeutil.workflow.traces import get_trace\n\n# This automatically uses OptimizedFileTrace now\ntrace = get_trace(\"workflow-123\", parent_run_id=\"parent-456\")\n\n# Standard logging interface\ntrace.info(\"Workflow started\")\ntrace.warning(\"High memory usage\")\ntrace.error(\"Database connection failed\")\n\n# Close when done (optional, but recommended)\ntrace.close()\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#2-with-context-information","title":"2. With Context Information","text":"<pre><code>trace = get_trace(\n    \"workflow-123\",\n    extras={\n        \"enable_write_log\": True,\n        \"workflow_name\": \"data-pipeline\",\n        \"stage_name\": \"extract\",\n        \"user_id\": \"data-engineer\",\n        \"environment\": \"production\"\n    }\n)\n\ntrace.info(\"Starting data extraction\")\ntrace.info(\"Processing 10,000 records\")\ntrace.warning(\"Found 50 records with missing values\")\ntrace.info(\"Extraction completed\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#3-error-handling","title":"3. Error Handling","text":"<pre><code>trace = get_trace(\"workflow-123\")\n\ntry:\n    trace.info(\"Starting risky operation\")\n    # ... your code here ...\n    raise ValueError(\"Something went wrong\")\nexcept Exception as e:\n    trace.error(f\"Operation failed: {e}\")\n    trace.exception(\"Full exception details:\")\nfinally:\n    trace.info(\"Cleanup completed\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#configuration","title":"Configuration","text":""},{"location":"examples/codes/quick_start_guide/#environment-variables","title":"Environment Variables","text":"<pre><code># Enable file logging\nexport WORKFLOW_LOG_TRACE_ENABLE_WRITE=true\n\n# Set log directory\nexport WORKFLOW_LOG_TRACE_URL=\"file:./logs\"\n\n# Custom log format\nexport WORKFLOW_LOG_FORMAT_FILE=\"[{datetime}] [{level}] {message}\"\n\n# Timezone\nexport WORKFLOW_LOG_TIMEZONE=\"UTC\"\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>import os\n\n# Set configuration programmatically\nos.environ[\"WORKFLOW_LOG_TRACE_ENABLE_WRITE\"] = \"true\"\nos.environ[\"WORKFLOW_LOG_TRACE_URL\"] = \"file:./my_logs\"\n\n# Then use get_trace()\ntrace = get_trace(\"workflow-123\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/codes/quick_start_guide/#1-direct-handler-usage","title":"1. Direct Handler Usage","text":"<pre><code>import logging\nfrom ddeutil.workflow.traces import WorkflowFileHandler\n\n# Create handler\nhandler = WorkflowFileHandler(\n    run_id=\"my-workflow\",\n    base_path=\"./logs\",\n    buffer_size=8192,  # 8KB buffer\n    extras={\"workflow_name\": \"my-app\"}\n)\n\n# Add to logger\nlogger = logging.getLogger(\"my_app\")\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Use standard Python logging\nlogger.info(\"Application started\")\nlogger.error(\"An error occurred\")\n\n# Cleanup\nhandler.close()\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#2-async-logging","title":"2. Async Logging","text":"<pre><code>import asyncio\nfrom ddeutil.workflow.traces import get_trace\n\nasync def async_workflow():\n    trace = get_trace(\"async-workflow-123\")\n\n    await trace.ainfo(\"Async workflow started\")\n    await trace.awarning(\"Processing batch 1\")\n    await trace.ainfo(\"Async workflow completed\")\n\n    trace.close()\n\n# Run async workflow\nasyncio.run(async_workflow())\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#3-concurrent-logging","title":"3. Concurrent Logging","text":"<pre><code>import threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom ddeutil.workflow.traces import get_trace\n\ndef worker(worker_id):\n    trace = get_trace(f\"worker-{worker_id}\")\n\n    for i in range(10):\n        trace.info(f\"Worker {worker_id}: Processing item {i}\")\n\n    trace.close()\n\n# Run multiple workers\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(worker, i) for i in range(4)]\n    for future in futures:\n        future.result()\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/codes/quick_start_guide/#1-batch-operations","title":"1. Batch Operations","text":"<pre><code>trace = get_trace(\"batch-example\")\n\n# Log batch start\ntrace.info(\"Starting batch processing: 1000 items\")\n\n# Process items (don't log every single item)\nfor i in range(0, 1000, 100):\n    # Process batch\n    # ...\n\n    # Log batch completion\n    trace.info(f\"Completed batch {i//100 + 1}/10\")\n\ntrace.info(\"Batch processing completed\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#2-context-management","title":"2. Context Management","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef workflow_trace(run_id, **extras):\n    trace = get_trace(run_id, extras=extras)\n    try:\n        yield trace\n    finally:\n        trace.close()\n\n# Usage\nwith workflow_trace(\"workflow-123\", workflow_name=\"data-pipeline\") as trace:\n    trace.info(\"Workflow started\")\n    # ... your code ...\n    trace.info(\"Workflow completed\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#3-custom-buffer-sizes","title":"3. Custom Buffer Sizes","text":"<pre><code>from ddeutil.workflow.traces import WorkflowFileHandler\n\n# For high-volume logging, use larger buffers\nhandler = WorkflowFileHandler(\n    run_id=\"high-volume\",\n    buffer_size=16384,  # 16KB buffer\n    flush_interval=0.5  # Flush every 500ms\n)\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#migration-from-filetrace","title":"Migration from FileTrace","text":""},{"location":"examples/codes/quick_start_guide/#automatic-migration","title":"Automatic Migration","text":"<p>If you're using <code>get_trace()</code>, you're already using the optimized version:</p> <pre><code># This now returns OptimizedFileTrace automatically\ntrace = get_trace(\"workflow-123\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#manual-migration","title":"Manual Migration","text":"<p>If you were using <code>FileTrace</code> directly:</p> <pre><code># Old way\nfrom ddeutil.workflow.traces import FileTrace\ntrace = FileTrace(url=\"file://./logs\", run_id=\"123\")\n\n# New way (recommended)\nfrom ddeutil.workflow.traces import OptimizedFileTrace\ntrace = OptimizedFileTrace(url=\"file://./logs\", run_id=\"123\")\n\n# Or use get_trace() (best)\nfrom ddeutil.workflow.traces import get_trace\ntrace = get_trace(\"123\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#file-structure","title":"File Structure","text":"<p>The optimized system creates the same file structure as the original:</p> <pre><code>./logs/\n\u2514\u2500\u2500 run_id=workflow-123/\n    \u251c\u2500\u2500 stdout.txt      # Standard output logs\n    \u251c\u2500\u2500 stderr.txt      # Error logs\n    \u2514\u2500\u2500 metadata.json   # Structured metadata\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"examples/codes/quick_start_guide/#check-buffer-status","title":"Check Buffer Status","text":"<pre><code>handler = WorkflowFileHandler(\"test\")\nprint(f\"Buffer sizes: stdout={len(handler.stdout_buffer)}, \"\n      f\"stderr={len(handler.stderr_buffer)}\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nstart_time = time.time()\ntrace = get_trace(\"performance-test\")\n\nfor i in range(10000):\n    trace.info(f\"Log message {i}\")\n\ntrace.close()\nduration = time.time() - start_time\nprint(f\"Logged 10,000 messages in {duration:.2f} seconds\")\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always close traces when done</li> <li>Use context managers for automatic cleanup</li> <li>Configure buffer sizes based on your logging volume</li> <li>Don't log every single operation - batch when possible</li> <li>Use appropriate log levels (DEBUG, INFO, WARNING, ERROR)</li> <li>Add context information for better debugging</li> <li>Monitor performance and adjust buffer sizes as needed</li> </ol>"},{"location":"examples/codes/quick_start_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/codes/quick_start_guide/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Make sure you're running from the project root</li> <li>Permission errors: Check write permissions to log directory</li> <li>Memory usage: Reduce buffer size if memory is constrained</li> <li>Performance: Increase buffer size for high-volume logging</li> </ol>"},{"location":"examples/codes/quick_start_guide/#debug-mode","title":"Debug Mode","text":"<pre><code>import logging\nlogging.getLogger(\"ddeutil.workflow.traces\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"examples/codes/quick_start_guide/#next-steps","title":"Next Steps","text":"<ul> <li>Read the full documentation</li> <li>Run the performance comparison</li> <li>Try the comprehensive examples</li> <li>Check the simple examples</li> </ul>"},{"location":"providers/aws-batch/","title":"AWS Batch Provider for ddeutil-workflow","text":"<p>This module provides AWS Batch integration for workflow job execution, enabling scalable and managed execution environments for complex workflow processing on AWS infrastructure.</p>"},{"location":"providers/aws-batch/#features","title":"Features","text":"<ul> <li>Automatic Job Definition Management: Creates and manages AWS Batch job definitions</li> <li>Job Submission: Submits workflow jobs to AWS Batch job queues</li> <li>File Management: Handles file upload/download via S3</li> <li>Result Collection: Retrieves and processes execution results</li> <li>Resource Cleanup: Automatic cleanup of AWS Batch resources</li> <li>Error Handling: Comprehensive error handling and status monitoring</li> </ul>"},{"location":"providers/aws-batch/#installation","title":"Installation","text":""},{"location":"providers/aws-batch/#prerequisites","title":"Prerequisites","text":"<ol> <li>AWS Account: You need an AWS account with Batch and S3 services</li> <li>AWS Batch Job Queue: Create an AWS Batch job queue in the AWS console</li> <li>S3 Bucket: Create an S3 bucket for file management</li> <li>IAM Roles: Configure appropriate IAM roles for Batch execution</li> </ol>"},{"location":"providers/aws-batch/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install with AWS dependencies\npip install ddeutil-workflow[aws]\n\n# Or install AWS dependencies separately\npip install boto3&gt;=1.34.0\n</code></pre>"},{"location":"providers/aws-batch/#configuration","title":"Configuration","text":""},{"location":"providers/aws-batch/#environment-variables","title":"Environment Variables","text":"<p>Set the following environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\nexport AWS_BATCH_JOB_QUEUE_ARN=\"arn:aws:batch:region:account:job-queue/queue-name\"\nexport AWS_S3_BUCKET=\"your-s3-bucket\"\n</code></pre>"},{"location":"providers/aws-batch/#aws-batch-setup","title":"AWS Batch Setup","text":"<ol> <li> <p>Create Job Queue:    <pre><code>aws batch create-job-queue \\\n  --job-queue-name my-job-queue \\\n  --state ENABLED \\\n  --priority 1 \\\n  --compute-environment-order order=1,computeEnvironment=arn:aws:batch:region:account:compute-environment/env-name\n</code></pre></p> </li> <li> <p>Create Compute Environment:    <pre><code>aws batch create-compute-environment \\\n  --compute-environment-name my-compute-env \\\n  --type MANAGED \\\n  --state ENABLED \\\n  --compute-resources type=EC2,minvCpus=0,maxvCpus=256,desiredvCpus=0,subnets=subnet-12345,securityGroupIds=sg-12345,instanceRole=arn:aws:iam::account:instance-profile/BatchInstanceRole\n</code></pre></p> </li> <li> <p>Create S3 Bucket:    <pre><code>aws s3 mb s3://my-workflow-bucket\n</code></pre></p> </li> </ol>"},{"location":"providers/aws-batch/#usage","title":"Usage","text":""},{"location":"providers/aws-batch/#basic-configuration","title":"Basic Configuration","text":"<pre><code># workflow.yml\nname: \"aws-batch-example\"\ndescription: \"Example workflow using AWS Batch\"\n\nparams:\n  data_source:\n    type: str\n    default: \"s3://my-bucket/data.csv\"\n\n  output_path:\n    type: str\n    default: \"s3://my-bucket/output\"\n\njobs:\n  data-processing:\n    id: \"data-processing\"\n    desc: \"Process data using AWS Batch\"\n\n    runs-on:\n      type: \"aws_batch\"\n      with:\n        job_queue_arn: \"${AWS_BATCH_JOB_QUEUE_ARN}\"\n        s3_bucket: \"${AWS_S3_BUCKET}\"\n        region_name: \"${AWS_DEFAULT_REGION}\"\n\n    stages:\n      - name: \"start\"\n        type: \"empty\"\n        echo: \"Starting AWS Batch job\"\n\n      - name: \"process-data\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          import boto3\n\n          # Download data from S3\n          s3_client = boto3.client('s3')\n          s3_client.download_file('my-bucket', 'data.csv', '/tmp/data.csv')\n\n          # Load and process data\n          data = pd.read_csv('/tmp/data.csv')\n          result = data.groupby('category').sum()\n\n          # Save results to S3\n          result.to_csv('/tmp/output.csv')\n          s3_client.upload_file('/tmp/output.csv', 'my-bucket', 'output/result.csv')\n\n          # Update context\n          result.context.update({\n              \"processed_rows\": len(data),\n              \"output_file\": \"s3://my-bucket/output/result.csv\"\n          })\n\n      - name: \"complete\"\n        type: \"empty\"\n        echo: \"AWS Batch job completed\"\n</code></pre>"},{"location":"providers/aws-batch/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/aws-batch/#custom-job-definition","title":"Custom Job Definition","text":"<pre><code>jobs:\n  custom-job:\n    runs-on:\n      type: \"aws_batch\"\n      with:\n        job_queue_arn: \"${AWS_BATCH_JOB_QUEUE_ARN}\"\n        s3_bucket: \"${AWS_S3_BUCKET}\"\n        region_name: \"us-west-2\"\n        aws_access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n        aws_secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n</code></pre>"},{"location":"providers/aws-batch/#resource-configuration","title":"Resource Configuration","text":"<pre><code>jobs:\n  resource-intensive:\n    runs-on:\n      type: \"aws_batch\"\n      with:\n        job_queue_arn: \"${AWS_BATCH_JOB_QUEUE_ARN}\"\n        s3_bucket: \"${AWS_S3_BUCKET}\"\n        region_name: \"us-east-1\"\n\n    stages:\n      - name: \"heavy-computation\"\n        type: \"py\"\n        run: |\n          # Resource-intensive processing\n          import numpy as np\n          import multiprocessing as mp\n\n          # Use all available cores\n          data = np.random.rand(10000, 10000)\n          result = np.linalg.eig(data)\n\n          result.context.update({\n              \"computation_completed\": True,\n              \"eigenvalues_count\": len(result[0])\n          })\n</code></pre>"},{"location":"providers/aws-batch/#architecture","title":"Architecture","text":""},{"location":"providers/aws-batch/#execution-flow","title":"Execution Flow","text":"<ol> <li>Job Definition Creation: Creates AWS Batch job definition if it doesn't exist</li> <li>File Upload: Uploads job configuration and parameters to S3</li> <li>Job Submission: Submits job to AWS Batch job queue</li> <li>Task Execution: AWS Batch executes the task on compute resources</li> <li>Result Collection: Downloads execution results from S3</li> <li>Cleanup: Removes temporary AWS Batch resources</li> </ol>"},{"location":"providers/aws-batch/#file-management","title":"File Management","text":"<p>The provider uses S3 for file management:</p> <ul> <li>Job Configuration: Serialized job configuration uploaded as JSON</li> <li>Parameters: Job parameters uploaded as JSON</li> <li>Task Script: Python script that executes the job using <code>local_execute</code></li> <li>Results: Execution results downloaded as JSON</li> </ul>"},{"location":"providers/aws-batch/#compute-resource-setup","title":"Compute Resource Setup","text":"<p>AWS Batch compute resources are automatically configured with:</p> <ul> <li>Python 3.11 slim container image</li> <li>ddeutil-workflow package installation</li> <li>S3 access for file upload/download</li> <li>Configurable CPU and memory limits</li> </ul>"},{"location":"providers/aws-batch/#monitoring","title":"Monitoring","text":""},{"location":"providers/aws-batch/#aws-console","title":"AWS Console","text":"<p>Monitor job execution through the AWS Console:</p> <ol> <li>Go to AWS Console &gt; Batch</li> <li>Navigate to Jobs &gt; Job queues</li> <li>Select your job queue and view jobs</li> </ol>"},{"location":"providers/aws-batch/#aws-cli","title":"AWS CLI","text":"<pre><code># List job queues\naws batch describe-job-queues\n\n# List jobs\naws batch list-jobs --job-queue my-job-queue\n\n# Get job details\naws batch describe-jobs --jobs job-arn\n\n# List job definitions\naws batch describe-job-definitions --job-definition-name workflow-job-def-*\n</code></pre>"},{"location":"providers/aws-batch/#cloudwatch-logs","title":"CloudWatch Logs","text":"<pre><code># Get job logs\naws logs describe-log-groups --log-group-name-prefix /aws/batch/job\n\n# Get specific job logs\naws logs filter-log-events --log-group-name /aws/batch/job --filter-pattern \"job-arn\"\n</code></pre>"},{"location":"providers/aws-batch/#best-practices","title":"Best Practices","text":""},{"location":"providers/aws-batch/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Use appropriate instance types for your workload</li> <li>Configure job queue priorities for different job types</li> <li>Set reasonable timeouts for job execution</li> <li>Monitor and adjust resource usage</li> </ul>"},{"location":"providers/aws-batch/#2-cost-optimization","title":"2. Cost Optimization","text":"<ul> <li>Use Spot instances for non-critical jobs</li> <li>Configure appropriate instance types</li> <li>Monitor and optimize resource usage</li> <li>Use lifecycle policies for S3 objects</li> </ul>"},{"location":"providers/aws-batch/#3-security","title":"3. Security","text":"<ul> <li>Store credentials securely using environment variables</li> <li>Use IAM roles with minimal required permissions</li> <li>Implement proper access controls</li> <li>Monitor access logs</li> </ul>"},{"location":"providers/aws-batch/#4-performance","title":"4. Performance","text":"<ul> <li>Optimize S3 upload/download operations</li> <li>Use appropriate storage tiers</li> <li>Configure parallel job execution</li> <li>Monitor and optimize resource usage</li> </ul>"},{"location":"providers/aws-batch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/aws-batch/#common-issues","title":"Common Issues","text":""},{"location":"providers/aws-batch/#1-authentication-errors","title":"1. Authentication Errors","text":"<p>Symptoms: <code>ClientError: An error occurred (UnauthorizedOperation)</code></p> <p>Solutions: - Verify AWS credentials - Check IAM permissions - Ensure correct region configuration</p> <pre><code># Verify credentials\naws sts get-caller-identity\n\n# Check Batch permissions\naws batch describe-job-queues\n</code></pre>"},{"location":"providers/aws-batch/#2-job-definition-failures","title":"2. Job Definition Failures","text":"<p>Symptoms: <code>ClientError: An error occurred (InvalidParameterValue)</code></p> <p>Solutions: - Verify job definition parameters - Check container image availability - Validate resource requirements</p> <pre><code># Check job definition\naws batch describe-job-definitions --job-definition-name workflow-job-def-*\n</code></pre>"},{"location":"providers/aws-batch/#3-job-execution-failures","title":"3. Job Execution Failures","text":"<p>Symptoms: Job status shows \"FAILED\"</p> <p>Solutions: - Check job logs in CloudWatch - Verify S3 bucket access - Review task script execution</p> <pre><code># Get job logs\naws logs filter-log-events --log-group-name /aws/batch/job --filter-pattern \"job-name\"\n</code></pre>"},{"location":"providers/aws-batch/#4-s3-access-issues","title":"4. S3 Access Issues","text":"<p>Symptoms: File upload/download failures</p> <p>Solutions: - Check S3 bucket permissions - Verify IAM roles have S3 access - Ensure bucket exists and is accessible</p> <pre><code># Test S3 access\naws s3 ls s3://your-bucket/\n</code></pre>"},{"location":"providers/aws-batch/#debug-information","title":"Debug Information","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger('ddeutil.workflow.plugins.providers.aws').setLevel(logging.DEBUG)\n</code></pre>"},{"location":"providers/aws-batch/#error-codes","title":"Error Codes","text":"Error Code Description Solution 401 Unauthorized Check credentials 403 Forbidden Check permissions 404 Not Found Check resource names 409 Conflict Resource already exists 429 Too Many Requests Implement retry logic"},{"location":"providers/aws-batch/#examples","title":"Examples","text":""},{"location":"providers/aws-batch/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>jobs:\n  data-pipeline:\n    strategy:\n      matrix:\n        dataset: [\"sales\", \"inventory\", \"users\"]\n      max_parallel: 3\n\n    runs-on:\n      type: \"aws_batch\"\n      with:\n        job_queue_arn: \"${AWS_BATCH_JOB_QUEUE_ARN}\"\n        s3_bucket: \"${AWS_S3_BUCKET}\"\n        region_name: \"${AWS_DEFAULT_REGION}\"\n\n    stages:\n      - name: \"process\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          import boto3\n          import os\n\n          dataset = os.environ.get('DATASET', 'unknown')\n          s3_client = boto3.client('s3')\n\n          # Download dataset\n          s3_client.download_file('my-bucket', f'data/{dataset}.csv', f'/tmp/{dataset}.csv')\n\n          # Process data\n          data = pd.read_csv(f'/tmp/{dataset}.csv')\n          processed = data.groupby('category').agg({\n              'value': ['sum', 'mean', 'count']\n          })\n\n          # Upload results\n          processed.to_csv(f'/tmp/{dataset}_processed.csv')\n          s3_client.upload_file(\n              f'/tmp/{dataset}_processed.csv',\n              'my-bucket',\n              f'output/{dataset}_processed.csv'\n          )\n\n          result.context.update({\n              \"dataset\": dataset,\n              \"rows_processed\": len(data),\n              \"output_file\": f\"s3://my-bucket/output/{dataset}_processed.csv\"\n          })\n</code></pre>"},{"location":"providers/aws-batch/#machine-learning-training","title":"Machine Learning Training","text":"<pre><code>jobs:\n  ml-training:\n    runs-on:\n      type: \"aws_batch\"\n      with:\n        job_queue_arn: \"${AWS_BATCH_JOB_QUEUE_ARN}\"\n        s3_bucket: \"${AWS_S3_BUCKET}\"\n        region_name: \"${AWS_DEFAULT_REGION}\"\n\n    stages:\n      - name: \"train\"\n        type: \"py\"\n        run: |\n          import numpy as np\n          import pickle\n          import boto3\n          from sklearn.ensemble import RandomForestClassifier\n          from sklearn.model_selection import train_test_split\n\n          # Download training data\n          s3_client = boto3.client('s3')\n          s3_client.download_file('my-bucket', 'data/training.csv', '/tmp/training.csv')\n\n          # Load and prepare data\n          data = np.loadtxt('/tmp/training.csv', delimiter=',')\n          X, y = data[:, :-1], data[:, -1]\n          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n          # Train model\n          model = RandomForestClassifier(n_estimators=100, random_state=42)\n          model.fit(X_train, y_train)\n\n          # Evaluate model\n          accuracy = model.score(X_test, y_test)\n\n          # Save model\n          with open('/tmp/model.pkl', 'wb') as f:\n              pickle.dump(model, f)\n\n          # Upload model\n          s3_client.upload_file('/tmp/model.pkl', 'my-bucket', 'models/trained_model.pkl')\n\n          result.context.update({\n              \"accuracy\": accuracy,\n              \"model_path\": \"s3://my-bucket/models/trained_model.pkl\"\n          })\n</code></pre>"},{"location":"providers/aws-batch/#api-reference","title":"API Reference","text":""},{"location":"providers/aws-batch/#awsbatchprovider","title":"AWSBatchProvider","text":"<p>Main provider class for AWS Batch operations.</p>"},{"location":"providers/aws-batch/#methods","title":"Methods","text":"<ul> <li><code>execute_job(job, params, run_id=None, event=None)</code>: Execute a job on AWS Batch</li> <li><code>cleanup(job_id=None)</code>: Clean up AWS Batch resources</li> <li><code>_create_job_definition_if_not_exists(job_def_name)</code>: Create job definition if it doesn't exist</li> <li><code>_create_job(job_name, job_def_arn, parameters)</code>: Create AWS Batch job</li> <li><code>_wait_for_job_completion(job_arn, timeout)</code>: Wait for job completion</li> </ul>"},{"location":"providers/aws-batch/#configuration-classes","title":"Configuration Classes","text":"<ul> <li><code>BatchComputeEnvironmentConfig</code>: Compute environment configuration</li> <li><code>BatchJobQueueConfig</code>: Job queue configuration</li> <li><code>BatchJobConfig</code>: Job configuration</li> <li><code>BatchTaskConfig</code>: Task configuration</li> </ul>"},{"location":"providers/aws-batch/#functions","title":"Functions","text":"<ul> <li><code>aws_batch_execute(job, params, run_id=None, event=None)</code>: Main execution function</li> </ul>"},{"location":"providers/aws-batch/#dependencies","title":"Dependencies","text":"<ul> <li><code>boto3&gt;=1.34.0</code>: AWS SDK for Python</li> <li><code>pydantic&gt;=2.11.7</code>: Data validation</li> <li><code>ddeutil-workflow</code>: Core workflow engine</li> </ul>"},{"location":"providers/aws-batch/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"providers/aws-batch/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"providers/azure-batch-provider/","title":"Azure Batch Provider","text":"<p>The Azure Batch provider enables workflow job execution on Azure Batch compute nodes, providing scalable and managed execution environments for complex workflow processing.</p>"},{"location":"providers/azure-batch-provider/#overview","title":"Overview","text":"<p>The Azure Batch provider handles the complete lifecycle of Azure Batch operations including:</p> <ul> <li>Pool Management: Automatic creation and management of Azure Batch pools</li> <li>Job Submission: Creating and submitting jobs to Azure Batch</li> <li>Task Execution: Running workflow tasks on compute nodes</li> <li>File Management: Uploading/downloading files via Azure Storage</li> <li>Result Retrieval: Collecting execution results and outputs</li> <li>Resource Cleanup: Automatic cleanup of Azure Batch resources</li> </ul>"},{"location":"providers/azure-batch-provider/#installation","title":"Installation","text":"<p>Install the Azure Batch provider with optional dependencies:</p> <pre><code>pip install ddeutil-workflow[azure]\n</code></pre> <p>This installs the required Azure SDK packages: - <code>azure-batch&gt;=13.0.0</code> - <code>azure-storage-blob&gt;=12.0.0</code></p>"},{"location":"providers/azure-batch-provider/#configuration","title":"Configuration","text":""},{"location":"providers/azure-batch-provider/#azure-batch-account-setup","title":"Azure Batch Account Setup","text":"<ol> <li>Create an Azure Batch account in the Azure portal</li> <li>Create an Azure Storage account for file management</li> <li>Note down the following credentials:</li> <li>Batch account name</li> <li>Batch account key</li> <li>Batch account URL</li> <li>Storage account name</li> <li>Storage account key</li> </ol>"},{"location":"providers/azure-batch-provider/#environment-variables","title":"Environment Variables","text":"<p>Set the following environment variables:</p> <pre><code>export AZURE_BATCH_ACCOUNT_NAME=\"your-batch-account\"\nexport AZURE_BATCH_ACCOUNT_KEY=\"your-batch-key\"\nexport AZURE_BATCH_ACCOUNT_URL=\"https://your-batch-account.region.batch.azure.com\"\nexport AZURE_STORAGE_ACCOUNT_NAME=\"your-storage-account\"\nexport AZURE_STORAGE_ACCOUNT_KEY=\"your-storage-key\"\n</code></pre>"},{"location":"providers/azure-batch-provider/#usage","title":"Usage","text":""},{"location":"providers/azure-batch-provider/#basic-configuration","title":"Basic Configuration","text":"<p>Configure a job to run on Azure Batch:</p> <pre><code>jobs:\n  my-job:\n    id: \"my-job\"\n    desc: \"Job running on Azure Batch\"\n\n    runs-on:\n      type: \"azure_batch\"\n      with:\n        batch_account_name: \"${AZURE_BATCH_ACCOUNT_NAME}\"\n        batch_account_key: \"${AZURE_BATCH_ACCOUNT_KEY}\"\n        batch_account_url: \"${AZURE_BATCH_ACCOUNT_URL}\"\n        storage_account_name: \"${AZURE_STORAGE_ACCOUNT_NAME}\"\n        storage_account_key: \"${AZURE_STORAGE_ACCOUNT_KEY}\"\n\n    stages:\n      - name: \"start\"\n        type: \"empty\"\n        echo: \"Starting Azure Batch job\"\n\n      - name: \"process\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n\n          # Your processing logic here\n          data = pd.read_csv('/tmp/input.csv')\n          result = data.groupby('category').sum()\n\n          # Save results\n          result.to_csv('/tmp/output.csv')\n\n          # Update context\n          result.context.update({\n              \"processed_rows\": len(data),\n              \"output_file\": \"/tmp/output.csv\"\n          })\n\n      - name: \"complete\"\n        type: \"empty\"\n        echo: \"Azure Batch job completed\"\n</code></pre>"},{"location":"providers/azure-batch-provider/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/azure-batch-provider/#pool-configuration","title":"Pool Configuration","text":"<p>Customize the Azure Batch pool settings:</p> <pre><code>from ddeutil.workflow.plugins.providers.az import BatchPoolConfig\n\npool_config = BatchPoolConfig(\n    pool_id=\"my-custom-pool\",\n    vm_size=\"Standard_D4s_v3\",\n    node_count=2,\n    max_tasks_per_node=8,\n    enable_auto_scale=True,\n    auto_scale_formula=\"$TargetDedicatedNodes=min(10, $PendingTasks)\"\n)\n</code></pre>"},{"location":"providers/azure-batch-provider/#job-configuration","title":"Job Configuration","text":"<p>Configure job-specific settings:</p> <pre><code>from ddeutil.workflow.plugins.providers.az import BatchJobConfig\n\njob_config = BatchJobConfig(\n    job_id=\"my-custom-job\",\n    pool_id=\"my-custom-pool\",\n    display_name=\"My Custom Job\",\n    priority=100,\n    uses_task_dependencies=True\n)\n</code></pre>"},{"location":"providers/azure-batch-provider/#task-configuration","title":"Task Configuration","text":"<p>Customize task execution settings:</p> <pre><code>from ddeutil.workflow.plugins.providers.az import BatchTaskConfig\n\ntask_config = BatchTaskConfig(\n    task_id=\"my-custom-task\",\n    command_line=\"python3 my_script.py\",\n    max_wall_clock_time=\"PT2H\",  # 2 hours\n    retention_time=\"PT1H\"        # 1 hour\n)\n</code></pre>"},{"location":"providers/azure-batch-provider/#architecture","title":"Architecture","text":""},{"location":"providers/azure-batch-provider/#execution-flow","title":"Execution Flow","text":"<ol> <li>Pool Creation: Creates Azure Batch pool if it doesn't exist</li> <li>Job Creation: Creates a new Azure Batch job</li> <li>File Upload: Uploads job configuration and parameters to Azure Storage</li> <li>Task Creation: Creates a task that executes the workflow job</li> <li>Task Execution: The task runs on Azure Batch compute nodes</li> <li>Result Collection: Downloads execution results from Azure Storage</li> <li>Cleanup: Removes temporary Azure Batch resources</li> </ol>"},{"location":"providers/azure-batch-provider/#file-management","title":"File Management","text":"<p>The provider uses Azure Storage for file management:</p> <ul> <li>Job Configuration: Serialized job configuration uploaded as JSON</li> <li>Parameters: Job parameters uploaded as JSON</li> <li>Task Script: Python script that executes the job using <code>local_execute</code></li> <li>Results: Execution results downloaded as JSON</li> </ul>"},{"location":"providers/azure-batch-provider/#compute-node-setup","title":"Compute Node Setup","text":"<p>Azure Batch compute nodes are automatically configured with:</p> <ul> <li>Ubuntu 20.04 LTS</li> <li>Python 3.x</li> <li>ddeutil-workflow package</li> <li>Required system packages</li> </ul>"},{"location":"providers/azure-batch-provider/#error-handling","title":"Error Handling","text":"<p>The provider includes comprehensive error handling:</p> <ul> <li>Connection Errors: Handles Azure service connection issues</li> <li>Authentication Errors: Validates Azure credentials</li> <li>Resource Errors: Manages pool and job creation failures</li> <li>Execution Errors: Captures and reports task execution failures</li> <li>Timeout Handling: Configurable timeouts for long-running tasks</li> </ul>"},{"location":"providers/azure-batch-provider/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"providers/azure-batch-provider/#azure-batch-monitoring","title":"Azure Batch Monitoring","text":"<p>Monitor job execution through:</p> <ul> <li>Azure Portal Batch service</li> <li>Azure CLI Batch commands</li> <li>Azure Batch REST API</li> </ul>"},{"location":"providers/azure-batch-provider/#logging","title":"Logging","text":"<p>The provider integrates with the workflow logging system:</p> <pre><code>from ddeutil.workflow.traces import get_trace\n\ntrace = get_trace(run_id)\ntrace.info(\"[AZURE_BATCH]: Starting job execution\")\ntrace.debug(\"[AZURE_BATCH]: Pool status: active\")\ntrace.error(\"[AZURE_BATCH]: Task failed: timeout\")\n</code></pre>"},{"location":"providers/azure-batch-provider/#best-practices","title":"Best Practices","text":""},{"location":"providers/azure-batch-provider/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Use appropriate VM sizes for your workload</li> <li>Configure auto-scaling for variable workloads</li> <li>Set reasonable timeouts for task execution</li> <li>Clean up resources after job completion</li> </ul>"},{"location":"providers/azure-batch-provider/#2-cost-optimization","title":"2. Cost Optimization","text":"<ul> <li>Use low-priority VMs for non-critical jobs</li> <li>Configure appropriate node counts</li> <li>Monitor and adjust resource usage</li> <li>Use spot instances when possible</li> </ul>"},{"location":"providers/azure-batch-provider/#3-security","title":"3. Security","text":"<ul> <li>Store credentials securely using environment variables</li> <li>Use Azure Key Vault for sensitive configuration</li> <li>Implement proper access controls</li> <li>Monitor access logs</li> </ul>"},{"location":"providers/azure-batch-provider/#4-performance","title":"4. Performance","text":"<ul> <li>Optimize file upload/download operations</li> <li>Use appropriate storage tiers</li> <li>Configure parallel task execution</li> <li>Monitor and optimize resource usage</li> </ul>"},{"location":"providers/azure-batch-provider/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/azure-batch-provider/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify Azure credentials</li> <li>Check account permissions</li> <li> <p>Ensure correct account URLs</p> </li> <li> <p>Pool Creation Failures</p> </li> <li>Verify VM size availability</li> <li>Check subscription quotas</li> <li> <p>Validate image references</p> </li> <li> <p>Task Execution Failures</p> </li> <li>Check task command line</li> <li>Verify file uploads</li> <li> <p>Review task logs</p> </li> <li> <p>Timeout Issues</p> </li> <li>Increase task timeout</li> <li>Optimize task execution</li> <li>Check resource availability</li> </ol>"},{"location":"providers/azure-batch-provider/#debug-information","title":"Debug Information","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger('ddeutil.workflow.plugins.providers.az').setLevel(logging.DEBUG)\n</code></pre>"},{"location":"providers/azure-batch-provider/#azure-cli-commands","title":"Azure CLI Commands","text":"<p>Use Azure CLI for troubleshooting:</p> <pre><code># List pools\naz batch pool list\n\n# List jobs\naz batch job list\n\n# List tasks\naz batch task list --job-id &lt;job-id&gt;\n\n# Get task details\naz batch task show --job-id &lt;job-id&gt; --task-id &lt;task-id&gt;\n</code></pre>"},{"location":"providers/azure-batch-provider/#examples","title":"Examples","text":""},{"location":"providers/azure-batch-provider/#data-processing-workflow","title":"Data Processing Workflow","text":"<p>See <code>docs/examples/azure-batch-example.yml</code> for a complete example of a data processing workflow using Azure Batch.</p>"},{"location":"providers/azure-batch-provider/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<pre><code>jobs:\n  ml-training:\n    runs-on:\n      type: \"azure_batch\"\n      with:\n        batch_account_name: \"${AZURE_BATCH_ACCOUNT_NAME}\"\n        batch_account_key: \"${AZURE_BATCH_ACCOUNT_KEY}\"\n        batch_account_url: \"${AZURE_BATCH_ACCOUNT_URL}\"\n        storage_account_name: \"${AZURE_STORAGE_ACCOUNT_NAME}\"\n        storage_account_key: \"${AZURE_STORAGE_ACCOUNT_KEY}\"\n\n    stages:\n      - name: \"prepare-data\"\n        type: \"py\"\n        run: |\n          # Data preparation logic\n          pass\n\n      - name: \"train-model\"\n        type: \"py\"\n        run: |\n          # Model training logic\n          pass\n\n      - name: \"evaluate-model\"\n        type: \"py\"\n        run: |\n          # Model evaluation logic\n          pass\n</code></pre>"},{"location":"providers/azure-batch-provider/#api-reference","title":"API Reference","text":""},{"location":"providers/azure-batch-provider/#azurebatchprovider","title":"AzureBatchProvider","text":"<p>Main provider class for Azure Batch operations.</p>"},{"location":"providers/azure-batch-provider/#methods","title":"Methods","text":"<ul> <li><code>execute_job(job, params, run_id=None, event=None)</code>: Execute a job on Azure Batch</li> <li><code>cleanup(job_id=None)</code>: Clean up Azure Batch resources</li> <li><code>_create_pool_if_not_exists(pool_id)</code>: Create pool if it doesn't exist</li> <li><code>_create_job(job_id, pool_id)</code>: Create Azure Batch job</li> <li><code>_create_task(job_id, task_id, command_line, ...)</code>: Create Azure Batch task</li> </ul>"},{"location":"providers/azure-batch-provider/#configuration-classes","title":"Configuration Classes","text":"<ul> <li><code>BatchPoolConfig</code>: Pool configuration</li> <li><code>BatchJobConfig</code>: Job configuration</li> <li><code>BatchTaskConfig</code>: Task configuration</li> </ul>"},{"location":"providers/azure-batch-provider/#functions","title":"Functions","text":"<ul> <li><code>azure_batch_execute(job, params, run_id=None, event=None)</code>: Main execution function</li> </ul>"},{"location":"providers/azure-batch-provider/#dependencies","title":"Dependencies","text":"<ul> <li><code>azure-batch&gt;=13.0.0</code>: Azure Batch SDK</li> <li><code>azure-storage-blob&gt;=12.0.0</code>: Azure Storage SDK</li> <li><code>pydantic&gt;=2.11.7</code>: Data validation</li> <li><code>ddeutil-workflow</code>: Core workflow engine</li> </ul>"},{"location":"providers/azure-batch/","title":"Azure Batch Provider for ddeutil-workflow","text":"<p>This module provides Azure Batch integration for workflow job execution, enabling scalable and managed execution environments for complex workflow processing.</p>"},{"location":"providers/azure-batch/#features","title":"Features","text":"<ul> <li>Automatic Pool Management: Creates and manages Azure Batch pools</li> <li>Job Submission: Submits workflow jobs to Azure Batch compute nodes</li> <li>File Management: Handles file upload/download via Azure Storage</li> <li>Result Collection: Retrieves and processes execution results</li> <li>Resource Cleanup: Automatic cleanup of Azure Batch resources</li> <li>Error Handling: Comprehensive error handling and status monitoring</li> </ul>"},{"location":"providers/azure-batch/#installation","title":"Installation","text":""},{"location":"providers/azure-batch/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure Account: You need an Azure account with Batch and Storage services</li> <li>Azure Batch Account: Create an Azure Batch account in the Azure portal</li> <li>Azure Storage Account: Create an Azure Storage account for file management</li> </ol>"},{"location":"providers/azure-batch/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install with Azure dependencies\npip install ddeutil-workflow[azure]\n\n# Or install Azure dependencies separately\npip install azure-batch&gt;=13.0.0 azure-storage-blob&gt;=12.0.0\n</code></pre>"},{"location":"providers/azure-batch/#configuration","title":"Configuration","text":""},{"location":"providers/azure-batch/#environment-variables","title":"Environment Variables","text":"<p>Set the following environment variables:</p> <pre><code>export AZURE_BATCH_ACCOUNT_NAME=\"your-batch-account\"\nexport AZURE_BATCH_ACCOUNT_KEY=\"your-batch-key\"\nexport AZURE_BATCH_ACCOUNT_URL=\"https://your-batch-account.region.batch.azure.com\"\nexport AZURE_STORAGE_ACCOUNT_NAME=\"your-storage-account\"\nexport AZURE_STORAGE_ACCOUNT_KEY=\"your-storage-key\"\n</code></pre>"},{"location":"providers/azure-batch/#azure-batch-account-setup","title":"Azure Batch Account Setup","text":"<ol> <li> <p>Create Batch Account:    <pre><code>az batch account create \\\n  --name your-batch-account \\\n  --resource-group your-resource-group \\\n  --location eastus\n</code></pre></p> </li> <li> <p>Get Account Credentials:    <pre><code>az batch account keys list \\\n  --name your-batch-account \\\n  --resource-group your-resource-group\n</code></pre></p> </li> <li> <p>Create Storage Account:    <pre><code>az storage account create \\\n  --name your-storage-account \\\n  --resource-group your-resource-group \\\n  --location eastus \\\n  --sku Standard_LRS\n</code></pre></p> </li> </ol>"},{"location":"providers/azure-batch/#usage","title":"Usage","text":""},{"location":"providers/azure-batch/#basic-configuration","title":"Basic Configuration","text":"<pre><code># workflow.yml\nname: \"azure-batch-example\"\ndescription: \"Example workflow using Azure Batch\"\n\nparams:\n  data_source:\n    type: str\n    default: \"https://example.com/data.csv\"\n\n  output_path:\n    type: str\n    default: \"/tmp/output\"\n\njobs:\n  data-processing:\n    id: \"data-processing\"\n    desc: \"Process data using Azure Batch\"\n\n    runs-on:\n      type: \"azure_batch\"\n      with:\n        batch_account_name: \"${AZURE_BATCH_ACCOUNT_NAME}\"\n        batch_account_key: \"${AZURE_BATCH_ACCOUNT_KEY}\"\n        batch_account_url: \"${AZURE_BATCH_ACCOUNT_URL}\"\n        storage_account_name: \"${AZURE_STORAGE_ACCOUNT_NAME}\"\n        storage_account_key: \"${AZURE_STORAGE_ACCOUNT_KEY}\"\n\n    stages:\n      - name: \"start\"\n        type: \"empty\"\n        echo: \"Starting Azure Batch job\"\n\n      - name: \"process-data\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n\n          # Load and process data\n          data = pd.read_csv('/tmp/input.csv')\n          result = data.groupby('category').sum()\n\n          # Save results\n          result.to_csv('/tmp/output.csv')\n\n          # Update context\n          result.context.update({\n              \"processed_rows\": len(data),\n              \"output_file\": \"/tmp/output.csv\"\n          })\n\n      - name: \"complete\"\n        type: \"empty\"\n        echo: \"Azure Batch job completed\"\n</code></pre>"},{"location":"providers/azure-batch/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/azure-batch/#custom-pool-configuration","title":"Custom Pool Configuration","text":"<pre><code>from ddeutil.workflow.plugins.providers.az import BatchPoolConfig\n\npool_config = BatchPoolConfig(\n    pool_id=\"my-custom-pool\",\n    vm_size=\"Standard_D4s_v3\",\n    node_count=2,\n    max_tasks_per_node=8,\n    enable_auto_scale=True,\n    auto_scale_formula=\"$TargetDedicatedNodes=min(10, $PendingTasks)\"\n)\n</code></pre>"},{"location":"providers/azure-batch/#custom-job-configuration","title":"Custom Job Configuration","text":"<pre><code>from ddeutil.workflow.plugins.providers.az import BatchJobConfig\n\njob_config = BatchJobConfig(\n    job_id=\"my-custom-job\",\n    pool_id=\"my-custom-pool\",\n    display_name=\"My Custom Job\",\n    priority=100,\n    uses_task_dependencies=True\n)\n</code></pre>"},{"location":"providers/azure-batch/#custom-task-configuration","title":"Custom Task Configuration","text":"<pre><code>from ddeutil.workflow.plugins.providers.az import BatchTaskConfig\n\ntask_config = BatchTaskConfig(\n    task_id=\"my-custom-task\",\n    command_line=\"python3 my_script.py\",\n    max_wall_clock_time=\"PT2H\",  # 2 hours\n    retention_time=\"PT1H\"        # 1 hour\n)\n</code></pre>"},{"location":"providers/azure-batch/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from ddeutil.workflow.plugins.providers.az import AzureBatchProvider\nfrom ddeutil.workflow.job import Job\n\n# Create provider\nprovider = AzureBatchProvider(\n    batch_account_name=\"mybatchaccount\",\n    batch_account_key=\"mykey\",\n    batch_account_url=\"https://mybatchaccount.region.batch.azure.com\",\n    storage_account_name=\"mystorageaccount\",\n    storage_account_key=\"mystoragekey\"\n)\n\n# Execute job\njob = Job(...)  # Your job configuration\nparams = {\"param1\": \"value1\"}\nresult = provider.execute_job(job, params, run_id=\"job-123\")\n\n# Clean up\nprovider.cleanup(\"workflow-job-job-123\")\n</code></pre>"},{"location":"providers/azure-batch/#architecture","title":"Architecture","text":""},{"location":"providers/azure-batch/#execution-flow","title":"Execution Flow","text":"<ol> <li>Pool Creation: Creates Azure Batch pool if it doesn't exist</li> <li>Job Creation: Creates a new Azure Batch job</li> <li>File Upload: Uploads job configuration and parameters to Azure Storage</li> <li>Task Creation: Creates a task that executes the workflow job</li> <li>Task Execution: The task runs on Azure Batch compute nodes</li> <li>Result Collection: Downloads execution results from Azure Storage</li> <li>Cleanup: Removes temporary Azure Batch resources</li> </ol>"},{"location":"providers/azure-batch/#file-management","title":"File Management","text":"<p>The provider uses Azure Storage for file management:</p> <ul> <li>Job Configuration: Serialized job configuration uploaded as JSON</li> <li>Parameters: Job parameters uploaded as JSON</li> <li>Task Script: Python script that executes the job using <code>local_execute</code></li> <li>Results: Execution results downloaded as JSON</li> </ul>"},{"location":"providers/azure-batch/#compute-node-setup","title":"Compute Node Setup","text":"<p>Azure Batch compute nodes are automatically configured with:</p> <ul> <li>Ubuntu 20.04 LTS</li> <li>Python 3.x</li> <li>ddeutil-workflow package</li> <li>Required system packages</li> </ul>"},{"location":"providers/azure-batch/#monitoring","title":"Monitoring","text":""},{"location":"providers/azure-batch/#azure-portal","title":"Azure Portal","text":"<p>Monitor job execution through the Azure Portal:</p> <ol> <li>Go to Azure Portal &gt; Batch accounts</li> <li>Select your batch account</li> <li>Navigate to Jobs &gt; Pools &gt; Tasks</li> </ol>"},{"location":"providers/azure-batch/#azure-cli","title":"Azure CLI","text":"<pre><code># List pools\naz batch pool list\n\n# List jobs\naz batch job list\n\n# List tasks\naz batch task list --job-id &lt;job-id&gt;\n\n# Get task details\naz batch task show --job-id &lt;job-id&gt; --task-id &lt;task-id&gt;\n\n# Get task files\naz batch file list --job-id &lt;job-id&gt; --task-id &lt;task-id&gt;\n</code></pre>"},{"location":"providers/azure-batch/#logging","title":"Logging","text":"<p>The provider integrates with the workflow logging system:</p> <pre><code>from ddeutil.workflow.traces import get_trace\n\ntrace = get_trace(run_id)\ntrace.info(\"[AZURE_BATCH]: Starting job execution\")\ntrace.debug(\"[AZURE_BATCH]: Pool status: active\")\ntrace.error(\"[AZURE_BATCH]: Task failed: timeout\")\n</code></pre>"},{"location":"providers/azure-batch/#best-practices","title":"Best Practices","text":""},{"location":"providers/azure-batch/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Use appropriate VM sizes for your workload</li> <li>Configure auto-scaling for variable workloads</li> <li>Set reasonable timeouts for task execution</li> <li>Clean up resources after job completion</li> </ul>"},{"location":"providers/azure-batch/#2-cost-optimization","title":"2. Cost Optimization","text":"<ul> <li>Use low-priority VMs for non-critical jobs</li> <li>Configure appropriate node counts</li> <li>Monitor and adjust resource usage</li> <li>Use spot instances when possible</li> </ul>"},{"location":"providers/azure-batch/#3-security","title":"3. Security","text":"<ul> <li>Store credentials securely using environment variables</li> <li>Use Azure Key Vault for sensitive configuration</li> <li>Implement proper access controls</li> <li>Monitor access logs</li> </ul>"},{"location":"providers/azure-batch/#4-performance","title":"4. Performance","text":"<ul> <li>Optimize file upload/download operations</li> <li>Use appropriate storage tiers</li> <li>Configure parallel task execution</li> <li>Monitor and optimize resource usage</li> </ul>"},{"location":"providers/azure-batch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/azure-batch/#common-issues","title":"Common Issues","text":""},{"location":"providers/azure-batch/#1-authentication-errors","title":"1. Authentication Errors","text":"<p>Symptoms: <code>BatchErrorException: 401 Unauthorized</code></p> <p>Solutions: - Verify Azure credentials - Check account permissions - Ensure correct account URLs</p> <pre><code># Verify credentials\naz batch account keys list --name your-batch-account\n</code></pre>"},{"location":"providers/azure-batch/#2-pool-creation-failures","title":"2. Pool Creation Failures","text":"<p>Symptoms: <code>BatchErrorException: 400 Bad Request</code></p> <p>Solutions: - Verify VM size availability - Check subscription quotas - Validate image references</p> <pre><code># Check VM sizes\naz vm list-sizes --location eastus --output table\n</code></pre>"},{"location":"providers/azure-batch/#3-task-execution-failures","title":"3. Task Execution Failures","text":"<p>Symptoms: Task status shows \"failed\"</p> <p>Solutions: - Check task command line - Verify file uploads - Review task logs</p> <pre><code># Get task logs\naz batch file download --job-id &lt;job-id&gt; --task-id &lt;task-id&gt; --file-path stdout.txt\n</code></pre>"},{"location":"providers/azure-batch/#4-timeout-issues","title":"4. Timeout Issues","text":"<p>Symptoms: Task status shows \"timeout\"</p> <p>Solutions: - Increase task timeout - Optimize task execution - Check resource availability</p>"},{"location":"providers/azure-batch/#debug-information","title":"Debug Information","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger('ddeutil.workflow.plugins.providers.az').setLevel(logging.DEBUG)\n</code></pre>"},{"location":"providers/azure-batch/#error-codes","title":"Error Codes","text":"Error Code Description Solution 401 Unauthorized Check credentials 403 Forbidden Check permissions 404 Not Found Check resource names 409 Conflict Resource already exists 429 Too Many Requests Implement retry logic"},{"location":"providers/azure-batch/#examples","title":"Examples","text":""},{"location":"providers/azure-batch/#data-processing-workflow","title":"Data Processing Workflow","text":"<p>See <code>docs/examples/azure-batch-example.yml</code> for a complete example.</p>"},{"location":"providers/azure-batch/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<pre><code>jobs:\n  ml-training:\n    runs-on:\n      type: \"azure_batch\"\n      with:\n        batch_account_name: \"${AZURE_BATCH_ACCOUNT_NAME}\"\n        batch_account_key: \"${AZURE_BATCH_ACCOUNT_KEY}\"\n        batch_account_url: \"${AZURE_BATCH_ACCOUNT_URL}\"\n        storage_account_name: \"${AZURE_STORAGE_ACCOUNT_NAME}\"\n        storage_account_key: \"${AZURE_STORAGE_ACCOUNT_KEY}\"\n\n    stages:\n      - name: \"prepare-data\"\n        type: \"py\"\n        run: |\n          # Data preparation logic\n          pass\n\n      - name: \"train-model\"\n        type: \"py\"\n        run: |\n          # Model training logic\n          pass\n\n      - name: \"evaluate-model\"\n        type: \"py\"\n        run: |\n          # Model evaluation logic\n          pass\n</code></pre>"},{"location":"providers/azure-batch/#parallel-processing","title":"Parallel Processing","text":"<pre><code>jobs:\n  parallel-processing:\n    strategy:\n      matrix:\n        worker_id: [1, 2, 3, 4]\n      max_parallel: 2\n\n    runs-on:\n      type: \"azure_batch\"\n      with:\n        batch_account_name: \"${AZURE_BATCH_ACCOUNT_NAME}\"\n        batch_account_key: \"${AZURE_BATCH_ACCOUNT_KEY}\"\n        batch_account_url: \"${AZURE_BATCH_ACCOUNT_URL}\"\n        storage_account_name: \"${AZURE_STORAGE_ACCOUNT_NAME}\"\n        storage_account_key: \"${AZURE_STORAGE_ACCOUNT_KEY}\"\n\n    stages:\n      - name: \"process-chunk\"\n        type: \"py\"\n        run: |\n          # Process data chunk based on worker_id\n          chunk_id = params['matrix']['worker_id']\n          # Processing logic here\n</code></pre>"},{"location":"providers/azure-batch/#api-reference","title":"API Reference","text":""},{"location":"providers/azure-batch/#azurebatchprovider","title":"AzureBatchProvider","text":"<p>Main provider class for Azure Batch operations.</p>"},{"location":"providers/azure-batch/#methods","title":"Methods","text":"<ul> <li><code>execute_job(job, params, run_id=None, event=None)</code>: Execute a job on Azure Batch</li> <li><code>cleanup(job_id=None)</code>: Clean up Azure Batch resources</li> <li><code>_create_pool_if_not_exists(pool_id)</code>: Create pool if it doesn't exist</li> <li><code>_create_job(job_id, pool_id)</code>: Create Azure Batch job</li> <li><code>_create_task(job_id, task_id, command_line, ...)</code>: Create Azure Batch task</li> </ul>"},{"location":"providers/azure-batch/#configuration-classes","title":"Configuration Classes","text":"<ul> <li><code>BatchPoolConfig</code>: Pool configuration</li> <li><code>BatchJobConfig</code>: Job configuration</li> <li><code>BatchTaskConfig</code>: Task configuration</li> </ul>"},{"location":"providers/azure-batch/#functions","title":"Functions","text":"<ul> <li><code>azure_batch_execute(job, params, run_id=None, event=None)</code>: Main execution function</li> </ul>"},{"location":"providers/azure-batch/#dependencies","title":"Dependencies","text":"<ul> <li><code>azure-batch&gt;=13.0.0</code>: Azure Batch SDK</li> <li><code>azure-storage-blob&gt;=12.0.0</code>: Azure Storage SDK</li> <li><code>pydantic&gt;=2.11.7</code>: Data validation</li> <li><code>ddeutil-workflow</code>: Core workflow engine</li> </ul>"},{"location":"providers/azure-batch/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"providers/azure-batch/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"providers/container/","title":"Container Provider for ddeutil-workflow","text":"<p>This module provides container-based execution for workflow jobs, enabling workflow execution inside Docker containers on any self-hosted server.</p>"},{"location":"providers/container/#features","title":"Features","text":"<ul> <li>Multi-OS Support: Run jobs on Ubuntu, Windows, Linux, and other container images</li> <li>Self-Hosted Compatibility: Works on any server with Docker installed</li> <li>Isolated Execution: Each job runs in its own container environment</li> <li>Volume Mounting: Mount host directories and volumes for data sharing</li> <li>Resource Management: Configure CPU, memory, and other resource limits</li> <li>Network Configuration: Customize container networking</li> <li>Result Collection: Automatic result retrieval and error handling</li> <li>Resource Cleanup: Automatic cleanup of containers and volumes</li> </ul>"},{"location":"providers/container/#installation","title":"Installation","text":""},{"location":"providers/container/#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker: Docker must be installed and running on the host system</li> <li>Python: Python 3.9+ with pip</li> <li>Network Access: Internet access for pulling container images</li> </ol>"},{"location":"providers/container/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install with container dependencies\npip install ddeutil-workflow[docker]\n\n# Or install Docker dependencies separately\npip install docker&gt;=7.1.0\n</code></pre>"},{"location":"providers/container/#configuration","title":"Configuration","text":""},{"location":"providers/container/#basic-configuration","title":"Basic Configuration","text":"<pre><code># workflow.yml\nname: \"container-example\"\ndescription: \"Example workflow using container execution\"\n\nparams:\n  data_path:\n    type: str\n    default: \"/host/data\"\n\n  output_path:\n    type: str\n    default: \"/container/output\"\n\njobs:\n  data-processing:\n    id: \"data-processing\"\n    desc: \"Process data using container execution\"\n\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"ubuntu:20.04\"\n        container_name: \"workflow-{run_id}\"\n        volumes:\n          - source: \"/host/data\"\n            target: \"/container/data\"\n            mode: \"rw\"\n        environment:\n          PYTHONPATH: \"/app\"\n        resources:\n          memory: \"2g\"\n          cpu: \"2\"\n\n    stages:\n      - name: \"start\"\n        type: \"empty\"\n        echo: \"Starting container job\"\n\n      - name: \"process-data\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          import os\n\n          # Load and process data\n          data = pd.read_csv('/container/data/input.csv')\n          result = data.groupby('category').sum()\n\n          # Save results\n          os.makedirs('/container/output', exist_ok=True)\n          result.to_csv('/container/output/result.csv')\n\n          # Update context\n          result.context.update({\n              \"processed_rows\": len(data),\n              \"output_file\": \"/container/output/result.csv\"\n          })\n\n      - name: \"complete\"\n        type: \"empty\"\n        echo: \"Container job completed\"\n</code></pre>"},{"location":"providers/container/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/container/#custom-volume-mounting","title":"Custom Volume Mounting","text":"<pre><code>jobs:\n  advanced-container:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"python:3.11-slim\"\n        volumes:\n          - source: \"/host/data\"\n            target: \"/container/data\"\n            mode: \"ro\"  # Read-only\n          - source: \"/host/output\"\n            target: \"/container/output\"\n            mode: \"rw\"  # Read-write\n          - source: \"/host/config\"\n            target: \"/container/config\"\n            mode: \"rw\"\n</code></pre>"},{"location":"providers/container/#resource-limits","title":"Resource Limits","text":"<pre><code>jobs:\n  resource-limited:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"ubuntu:20.04\"\n        resources:\n          memory: \"4g\"\n          cpu: \"2.5\"\n          cpuset_cpus: \"0,1\"\n          memswap_limit: \"8g\"\n</code></pre>"},{"location":"providers/container/#network-configuration","title":"Network Configuration","text":"<pre><code>jobs:\n  networked:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"ubuntu:20.04\"\n        network:\n          network_mode: \"host\"\n          ports:\n            \"8080\": \"8080\"\n            \"9000\": \"9000\"\n</code></pre>"},{"location":"providers/container/#custom-environment","title":"Custom Environment","text":"<pre><code>jobs:\n  custom-env:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"python:3.11-slim\"\n        environment:\n          PYTHONPATH: \"/app:/workflow\"\n          DATABASE_URL: \"postgresql://user:pass@host:5432/db\"\n          API_KEY: \"${API_KEY}\"\n          DEBUG: \"true\"\n        working_dir: \"/app\"\n        user: \"1000:1000\"\n</code></pre>"},{"location":"providers/container/#usage-examples","title":"Usage Examples","text":""},{"location":"providers/container/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>jobs:\n  data-pipeline:\n    strategy:\n      matrix:\n        dataset: [\"sales\", \"inventory\", \"users\"]\n      max_parallel: 2\n\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"python:3.11-slim\"\n        volumes:\n          - source: \"/data/{{ matrix.dataset }}\"\n            target: \"/container/data\"\n            mode: \"ro\"\n          - source: \"/output/{{ matrix.dataset }}\"\n            target: \"/container/output\"\n            mode: \"rw\"\n        environment:\n          DATASET: \"{{ matrix.dataset }}\"\n\n    stages:\n      - name: \"process\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          import os\n\n          dataset = os.environ['DATASET']\n          data = pd.read_csv(f'/container/data/{dataset}.csv')\n\n          # Processing logic here\n          processed = data.groupby('category').agg({\n              'value': ['sum', 'mean', 'count']\n          })\n\n          # Save results\n          output_dir = f'/container/output/{dataset}'\n          os.makedirs(output_dir, exist_ok=True)\n          processed.to_csv(f'{output_dir}/processed.csv')\n\n          result.context.update({\n              \"dataset\": dataset,\n              \"rows_processed\": len(data),\n              \"output_file\": f'{output_dir}/processed.csv'\n          })\n</code></pre>"},{"location":"providers/container/#machine-learning-training","title":"Machine Learning Training","text":"<pre><code>jobs:\n  ml-training:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"tensorflow/tensorflow:2.13.0-gpu\"\n        volumes:\n          - source: \"/ml/data\"\n            target: \"/container/data\"\n            mode: \"ro\"\n          - source: \"/ml/models\"\n            target: \"/container/models\"\n            mode: \"rw\"\n        resources:\n          memory: \"8g\"\n          cpu: \"4\"\n        environment:\n          CUDA_VISIBLE_DEVICES: \"0\"\n\n    stages:\n      - name: \"train\"\n        type: \"py\"\n        run: |\n          import tensorflow as tf\n          import numpy as np\n\n          # Load data\n          data = np.load('/container/data/training.npy')\n          labels = np.load('/container/data/labels.npy')\n\n          # Create model\n          model = tf.keras.Sequential([\n              tf.keras.layers.Dense(128, activation='relu'),\n              tf.keras.layers.Dropout(0.2),\n              tf.keras.layers.Dense(64, activation='relu'),\n              tf.keras.layers.Dense(10, activation='softmax')\n          ])\n\n          model.compile(\n              optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy']\n          )\n\n          # Train model\n          history = model.fit(\n              data, labels,\n              epochs=10,\n              validation_split=0.2,\n              batch_size=32\n          )\n\n          # Save model\n          model.save('/container/models/trained_model.h5')\n\n          result.context.update({\n              \"final_accuracy\": history.history['accuracy'][-1],\n              \"model_path\": \"/container/models/trained_model.h5\"\n          })\n</code></pre>"},{"location":"providers/container/#web-application-testing","title":"Web Application Testing","text":"<pre><code>jobs:\n  web-testing:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"node:18-alpine\"\n        volumes:\n          - source: \"/app\"\n            target: \"/container/app\"\n            mode: \"rw\"\n        network:\n          network_mode: \"bridge\"\n          ports:\n            \"3000\": \"3000\"\n        environment:\n          NODE_ENV: \"test\"\n\n    stages:\n      - name: \"install\"\n        type: \"bash\"\n        run: |\n          cd /container/app\n          npm install\n\n      - name: \"test\"\n        type: \"bash\"\n        run: |\n          cd /container/app\n          npm test\n\n      - name: \"build\"\n        type: \"bash\"\n        run: |\n          cd /container/app\n          npm run build\n</code></pre>"},{"location":"providers/container/#architecture","title":"Architecture","text":""},{"location":"providers/container/#execution-flow","title":"Execution Flow","text":"<ol> <li>Container Creation: Creates a Docker container with the specified image</li> <li>Volume Setup: Mounts configured volumes and creates workflow volume</li> <li>File Upload: Uploads job configuration and parameters to container</li> <li>Environment Setup: Configures environment variables and working directory</li> <li>Job Execution: Runs the workflow job inside the container</li> <li>Result Collection: Retrieves execution results and logs</li> <li>Cleanup: Removes container and volumes (if configured)</li> </ol>"},{"location":"providers/container/#volume-management","title":"Volume Management","text":"<p>The provider uses Docker volumes for file management:</p> <ul> <li>Workflow Volume: Temporary volume for job configuration and results</li> <li>Host Volumes: Mounted host directories for data sharing</li> <li>Named Volumes: Persistent volumes for data storage</li> </ul>"},{"location":"providers/container/#container-lifecycle","title":"Container Lifecycle","text":"<ol> <li>Preparation: Create volumes and upload files</li> <li>Execution: Start container and run job</li> <li>Monitoring: Wait for completion and collect results</li> <li>Cleanup: Remove resources and clean up</li> </ol>"},{"location":"providers/container/#best-practices","title":"Best Practices","text":""},{"location":"providers/container/#1-image-selection","title":"1. Image Selection","text":"<ul> <li>Use official base images (ubuntu, python, node, etc.)</li> <li>Choose minimal images for faster startup</li> <li>Consider multi-stage builds for custom images</li> <li>Use specific version tags for reproducibility</li> </ul>"},{"location":"providers/container/#2-resource-management","title":"2. Resource Management","text":"<ul> <li>Set appropriate memory and CPU limits</li> <li>Monitor resource usage during execution</li> <li>Use resource limits to prevent system overload</li> <li>Consider using cgroups for fine-grained control</li> </ul>"},{"location":"providers/container/#3-volume-configuration","title":"3. Volume Configuration","text":"<ul> <li>Use read-only mounts for input data</li> <li>Use read-write mounts for output data</li> <li>Consider using named volumes for persistence</li> <li>Avoid mounting sensitive host directories</li> </ul>"},{"location":"providers/container/#4-security","title":"4. Security","text":"<ul> <li>Run containers as non-root users when possible</li> <li>Use minimal base images</li> <li>Avoid mounting sensitive host directories</li> <li>Consider using Docker security options</li> </ul>"},{"location":"providers/container/#5-performance","title":"5. Performance","text":"<ul> <li>Use appropriate base images for your workload</li> <li>Configure resource limits based on requirements</li> <li>Use volume mounts for data sharing</li> <li>Consider using multi-stage builds</li> </ul>"},{"location":"providers/container/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/container/#common-issues","title":"Common Issues","text":""},{"location":"providers/container/#1-container-startup-failures","title":"1. Container Startup Failures","text":"<p>Symptoms: Container fails to start or exits immediately</p> <p>Solutions: - Check Docker daemon is running - Verify image exists and is accessible - Check resource limits are appropriate - Review container logs for errors</p> <pre><code># Check Docker daemon\ndocker info\n\n# Check image exists\ndocker images\n\n# Check container logs\ndocker logs &lt;container_name&gt;\n</code></pre>"},{"location":"providers/container/#2-volume-mount-issues","title":"2. Volume Mount Issues","text":"<p>Symptoms: Files not accessible in container</p> <p>Solutions: - Verify host paths exist and are accessible - Check volume mount permissions - Ensure correct mount modes (ro/rw)</p> <pre><code># Check volume mounts\ndocker inspect &lt;container_name&gt; | grep -A 10 \"Mounts\"\n\n# Test volume access\ndocker run --rm -v /host/path:/container/path ubuntu ls /container/path\n</code></pre>"},{"location":"providers/container/#3-resource-limit-issues","title":"3. Resource Limit Issues","text":"<p>Symptoms: Container killed or performance issues</p> <p>Solutions: - Increase memory limits - Adjust CPU limits - Monitor resource usage - Check system resources</p> <pre><code># Check container resource usage\ndocker stats &lt;container_name&gt;\n\n# Check system resources\nfree -h\nnproc\n</code></pre>"},{"location":"providers/container/#4-network-issues","title":"4. Network Issues","text":"<p>Symptoms: Container cannot access network resources</p> <p>Solutions: - Check network mode configuration - Verify port mappings - Check firewall settings - Test network connectivity</p> <pre><code># Check container network\ndocker inspect &lt;container_name&gt; | grep -A 10 \"NetworkSettings\"\n\n# Test network connectivity\ndocker run --rm ubuntu ping google.com\n</code></pre>"},{"location":"providers/container/#debug-information","title":"Debug Information","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger('ddeutil.workflow.plugins.providers.container').setLevel(logging.DEBUG)\n</code></pre>"},{"location":"providers/container/#error-codes","title":"Error Codes","text":"Error Code Description Solution 125 Container not found Check container exists 126 Container not running Check container status 127 Command not found Check command exists 128 Permission denied Check permissions 139 Container killed Check resource limits"},{"location":"providers/container/#examples","title":"Examples","text":""},{"location":"providers/container/#multi-platform-testing","title":"Multi-Platform Testing","text":"<pre><code>jobs:\n  cross-platform-test:\n    strategy:\n      matrix:\n        platform: [\"ubuntu:20.04\", \"ubuntu:22.04\", \"python:3.9\", \"python:3.11\"]\n      max_parallel: 4\n\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"{{ matrix.platform }}\"\n        volumes:\n          - source: \"/test/code\"\n            target: \"/container/code\"\n            mode: \"ro\"\n        environment:\n          PLATFORM: \"{{ matrix.platform }}\"\n\n    stages:\n      - name: \"test\"\n        type: \"py\"\n        run: |\n          import sys\n          import platform\n\n          print(f\"Testing on {platform.platform()}\")\n          print(f\"Python version: {sys.version}\")\n\n          # Run tests\n          import subprocess\n          result = subprocess.run(['python', '-m', 'pytest', '/container/code'])\n\n          result.context.update({\n              \"platform\": platform.platform(),\n              \"python_version\": sys.version,\n              \"test_exit_code\": result.returncode\n          })\n</code></pre>"},{"location":"providers/container/#database-operations","title":"Database Operations","text":"<pre><code>jobs:\n  database-migration:\n    runs-on:\n      type: \"container\"\n      with:\n        image: \"postgres:15\"\n        volumes:\n          - source: \"/db/migrations\"\n            target: \"/container/migrations\"\n            mode: \"ro\"\n        network:\n          network_mode: \"host\"\n        environment:\n          POSTGRES_PASSWORD: \"${DB_PASSWORD}\"\n          POSTGRES_DB: \"myapp\"\n\n    stages:\n      - name: \"migrate\"\n        type: \"bash\"\n        run: |\n          # Wait for database to be ready\n          until pg_isready -h localhost -p 5432; do\n            sleep 1\n          done\n\n          # Run migrations\n          psql -h localhost -U postgres -d myapp -f /container/migrations/001_initial.sql\n          psql -h localhost -U postgres -d myapp -f /container/migrations/002_add_users.sql\n</code></pre>"},{"location":"providers/container/#api-reference","title":"API Reference","text":""},{"location":"providers/container/#containerprovider","title":"ContainerProvider","text":"<p>Main provider class for container operations.</p>"},{"location":"providers/container/#methods","title":"Methods","text":"<ul> <li><code>execute_job(job, params, run_id=None, event=None)</code>: Execute a job in container</li> <li><code>cleanup(run_id=None)</code>: Clean up container resources</li> <li><code>_create_workflow_volume(run_id)</code>: Create temporary volume for workflow files</li> <li><code>_prepare_container_volumes(run_id)</code>: Prepare container volume mounts</li> <li><code>_prepare_environment(run_id, job, params)</code>: Prepare container environment</li> </ul>"},{"location":"providers/container/#configuration-classes","title":"Configuration Classes","text":"<ul> <li><code>ContainerConfig</code>: Container execution configuration</li> <li><code>VolumeConfig</code>: Volume mount configuration</li> <li><code>NetworkConfig</code>: Network configuration</li> <li><code>ResourceConfig</code>: Resource limits configuration</li> </ul>"},{"location":"providers/container/#functions","title":"Functions","text":"<ul> <li><code>container_execute(job, params, run_id=None, event=None)</code>: Main execution function</li> </ul>"},{"location":"providers/container/#dependencies","title":"Dependencies","text":"<ul> <li><code>docker&gt;=7.1.0</code>: Docker Python SDK</li> <li><code>pydantic&gt;=2.11.7</code>: Data validation</li> <li><code>ddeutil-workflow</code>: Core workflow engine</li> </ul>"},{"location":"providers/container/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"providers/container/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"providers/gcp-batch/","title":"Google Cloud Batch Provider for ddeutil-workflow","text":"<p>This module provides Google Cloud Batch integration for workflow job execution, enabling scalable and managed execution environments for complex workflow processing on Google Cloud infrastructure.</p>"},{"location":"providers/gcp-batch/#features","title":"Features","text":"<ul> <li>Automatic Job Creation: Creates and manages Google Cloud Batch jobs</li> <li>Task Execution: Executes workflow tasks on Google Cloud compute resources</li> <li>File Management: Handles file upload/download via Google Cloud Storage</li> <li>Result Collection: Retrieves and processes execution results</li> <li>Resource Cleanup: Automatic cleanup of Google Cloud Batch resources</li> <li>Error Handling: Comprehensive error handling and status monitoring</li> </ul>"},{"location":"providers/gcp-batch/#installation","title":"Installation","text":""},{"location":"providers/gcp-batch/#prerequisites","title":"Prerequisites","text":"<ol> <li>Google Cloud Project: You need a Google Cloud project with Batch API enabled</li> <li>Service Account: Create a service account with appropriate permissions</li> <li>Google Cloud Storage Bucket: Create a GCS bucket for file management</li> <li>Batch API: Enable the Batch API in your Google Cloud project</li> </ol>"},{"location":"providers/gcp-batch/#install-dependencies","title":"Install Dependencies","text":"<pre><code># Install with Google Cloud dependencies\npip install ddeutil-workflow[gcp]\n\n# Or install Google Cloud dependencies separately\npip install google-cloud-batch&gt;=0.10.0 google-cloud-storage&gt;=2.10.0\n</code></pre>"},{"location":"providers/gcp-batch/#configuration","title":"Configuration","text":""},{"location":"providers/gcp-batch/#environment-variables","title":"Environment Variables","text":"<p>Set the following environment variables:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account.json\"\nexport GOOGLE_CLOUD_PROJECT=\"your-project-id\"\nexport GOOGLE_CLOUD_REGION=\"us-central1\"\nexport GCS_BUCKET=\"your-gcs-bucket\"\n</code></pre>"},{"location":"providers/gcp-batch/#google-cloud-setup","title":"Google Cloud Setup","text":"<ol> <li> <p>Enable APIs:    <pre><code>gcloud services enable batch.googleapis.com\ngcloud services enable storage.googleapis.com\n</code></pre></p> </li> <li> <p>Create Service Account:    <pre><code>gcloud iam service-accounts create workflow-batch \\\n  --display-name=\"Workflow Batch Service Account\"\n\ngcloud projects add-iam-policy-binding your-project-id \\\n  --member=\"serviceAccount:workflow-batch@your-project-id.iam.gserviceaccount.com\" \\\n  --role=\"roles/batch.worker\"\n\ngcloud projects add-iam-policy-binding your-project-id \\\n  --member=\"serviceAccount:workflow-batch@your-project-id.iam.gserviceaccount.com\" \\\n  --role=\"roles/storage.objectViewer\"\n\ngcloud projects add-iam-policy-binding your-project-id \\\n  --member=\"serviceAccount:workflow-batch@your-project-id.iam.gserviceaccount.com\" \\\n  --role=\"roles/storage.objectCreator\"\n</code></pre></p> </li> <li> <p>Create Service Account Key:    <pre><code>gcloud iam service-accounts keys create service-account.json \\\n  --iam-account=workflow-batch@your-project-id.iam.gserviceaccount.com\n</code></pre></p> </li> <li> <p>Create GCS Bucket:    <pre><code>gsutil mb gs://my-workflow-bucket\n</code></pre></p> </li> </ol>"},{"location":"providers/gcp-batch/#usage","title":"Usage","text":""},{"location":"providers/gcp-batch/#basic-configuration","title":"Basic Configuration","text":"<pre><code># workflow.yml\nname: \"gcp-batch-example\"\ndescription: \"Example workflow using Google Cloud Batch\"\n\nparams:\n  data_source:\n    type: str\n    default: \"gs://my-bucket/data.csv\"\n\n  output_path:\n    type: str\n    default: \"gs://my-bucket/output\"\n\njobs:\n  data-processing:\n    id: \"data-processing\"\n    desc: \"Process data using Google Cloud Batch\"\n\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n\n    stages:\n      - name: \"start\"\n        type: \"empty\"\n        echo: \"Starting Google Cloud Batch job\"\n\n      - name: \"process-data\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          from google.cloud import storage\n\n          # Download data from GCS\n          storage_client = storage.Client()\n          bucket = storage_client.bucket('my-bucket')\n          blob = bucket.blob('data.csv')\n          blob.download_to_filename('/tmp/data.csv')\n\n          # Load and process data\n          data = pd.read_csv('/tmp/data.csv')\n          result = data.groupby('category').sum()\n\n          # Save results to GCS\n          result.to_csv('/tmp/output.csv')\n          blob = bucket.blob('output/result.csv')\n          blob.upload_from_filename('/tmp/output.csv')\n\n          # Update context\n          result.context.update({\n              \"processed_rows\": len(data),\n              \"output_file\": \"gs://my-bucket/output/result.csv\"\n          })\n\n      - name: \"complete\"\n        type: \"empty\"\n        echo: \"Google Cloud Batch job completed\"\n</code></pre>"},{"location":"providers/gcp-batch/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"providers/gcp-batch/#custom-resource-configuration","title":"Custom Resource Configuration","text":"<pre><code>jobs:\n  resource-intensive:\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n        machine_type: \"n1-standard-8\"\n        max_parallel_tasks: 4\n</code></pre>"},{"location":"providers/gcp-batch/#custom-credentials","title":"Custom Credentials","text":"<pre><code>jobs:\n  custom-auth:\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n        credentials_path: \"/path/to/custom-service-account.json\"\n</code></pre>"},{"location":"providers/gcp-batch/#architecture","title":"Architecture","text":""},{"location":"providers/gcp-batch/#execution-flow","title":"Execution Flow","text":"<ol> <li>Job Creation: Creates Google Cloud Batch job with task specifications</li> <li>File Upload: Uploads job configuration and parameters to GCS</li> <li>Task Execution: Google Cloud Batch executes the task on compute resources</li> <li>Result Collection: Downloads execution results from GCS</li> <li>Cleanup: Removes temporary Google Cloud Batch resources</li> </ol>"},{"location":"providers/gcp-batch/#file-management","title":"File Management","text":"<p>The provider uses Google Cloud Storage for file management:</p> <ul> <li>Job Configuration: Serialized job configuration uploaded as JSON</li> <li>Parameters: Job parameters uploaded as JSON</li> <li>Task Script: Python script that executes the job using <code>local_execute</code></li> <li>Results: Execution results downloaded as JSON</li> </ul>"},{"location":"providers/gcp-batch/#compute-resource-setup","title":"Compute Resource Setup","text":"<p>Google Cloud Batch compute resources are automatically configured with:</p> <ul> <li>Python 3.11 slim container image</li> <li>ddeutil-workflow package installation</li> <li>GCS access for file upload/download</li> <li>Configurable CPU and memory limits</li> </ul>"},{"location":"providers/gcp-batch/#monitoring","title":"Monitoring","text":""},{"location":"providers/gcp-batch/#google-cloud-console","title":"Google Cloud Console","text":"<p>Monitor job execution through the Google Cloud Console:</p> <ol> <li>Go to Google Cloud Console &gt; Batch</li> <li>Navigate to Jobs</li> <li>Select your job and view details</li> </ol>"},{"location":"providers/gcp-batch/#google-cloud-cli","title":"Google Cloud CLI","text":"<pre><code># List jobs\ngcloud batch jobs list\n\n# Get job details\ngcloud batch jobs describe job-name\n\n# List tasks\ngcloud batch tasks list --job job-name\n\n# Get task details\ngcloud batch tasks describe task-name --job job-name\n</code></pre>"},{"location":"providers/gcp-batch/#cloud-logging","title":"Cloud Logging","text":"<pre><code># Get job logs\ngcloud logging read \"resource.type=batch_job\" --limit=50\n\n# Get specific job logs\ngcloud logging read \"resource.type=batch_job AND resource.labels.job_name=job-name\" --limit=50\n</code></pre>"},{"location":"providers/gcp-batch/#best-practices","title":"Best Practices","text":""},{"location":"providers/gcp-batch/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Use appropriate machine types for your workload</li> <li>Configure parallel task execution for better performance</li> <li>Set reasonable timeouts for job execution</li> <li>Monitor and adjust resource usage</li> </ul>"},{"location":"providers/gcp-batch/#2-cost-optimization","title":"2. Cost Optimization","text":"<ul> <li>Use appropriate machine types</li> <li>Configure parallel task execution efficiently</li> <li>Monitor and optimize resource usage</li> <li>Use lifecycle policies for GCS objects</li> </ul>"},{"location":"providers/gcp-batch/#3-security","title":"3. Security","text":"<ul> <li>Store credentials securely using service account keys</li> <li>Use service accounts with minimal required permissions</li> <li>Implement proper access controls</li> <li>Monitor access logs</li> </ul>"},{"location":"providers/gcp-batch/#4-performance","title":"4. Performance","text":"<ul> <li>Optimize GCS upload/download operations</li> <li>Use appropriate storage classes</li> <li>Configure parallel job execution</li> <li>Monitor and optimize resource usage</li> </ul>"},{"location":"providers/gcp-batch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/gcp-batch/#common-issues","title":"Common Issues","text":""},{"location":"providers/gcp-batch/#1-authentication-errors","title":"1. Authentication Errors","text":"<p>Symptoms: <code>google.auth.exceptions.DefaultCredentialsError</code></p> <p>Solutions: - Verify service account credentials - Check service account permissions - Ensure correct project configuration</p> <pre><code># Verify credentials\ngcloud auth application-default print-access-token\n\n# Check service account\ngcloud iam service-accounts list\n</code></pre>"},{"location":"providers/gcp-batch/#2-job-creation-failures","title":"2. Job Creation Failures","text":"<p>Symptoms: <code>google.api_core.exceptions.InvalidArgument</code></p> <p>Solutions: - Verify job parameters - Check machine type availability - Validate resource requirements</p> <pre><code># Check available machine types\ngcloud compute machine-types list --filter=\"zone:us-central1-a\"\n</code></pre>"},{"location":"providers/gcp-batch/#3-job-execution-failures","title":"3. Job Execution Failures","text":"<p>Symptoms: Job status shows \"FAILED\"</p> <p>Solutions: - Check job logs in Cloud Logging - Verify GCS bucket access - Review task script execution</p> <pre><code># Get job logs\ngcloud logging read \"resource.type=batch_job AND resource.labels.job_name=job-name\" --limit=50\n</code></pre>"},{"location":"providers/gcp-batch/#4-gcs-access-issues","title":"4. GCS Access Issues","text":"<p>Symptoms: File upload/download failures</p> <p>Solutions: - Check GCS bucket permissions - Verify service account has GCS access - Ensure bucket exists and is accessible</p> <pre><code># Test GCS access\ngsutil ls gs://your-bucket/\n</code></pre>"},{"location":"providers/gcp-batch/#debug-information","title":"Debug Information","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.getLogger('ddeutil.workflow.plugins.providers.gcs').setLevel(logging.DEBUG)\n</code></pre>"},{"location":"providers/gcp-batch/#error-codes","title":"Error Codes","text":"Error Code Description Solution 401 Unauthorized Check credentials 403 Forbidden Check permissions 404 Not Found Check resource names 409 Conflict Resource already exists 429 Too Many Requests Implement retry logic"},{"location":"providers/gcp-batch/#examples","title":"Examples","text":""},{"location":"providers/gcp-batch/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>jobs:\n  data-pipeline:\n    strategy:\n      matrix:\n        dataset: [\"sales\", \"inventory\", \"users\"]\n      max_parallel: 3\n\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n\n    stages:\n      - name: \"process\"\n        type: \"py\"\n        run: |\n          import pandas as pd\n          from google.cloud import storage\n          import os\n\n          dataset = os.environ.get('DATASET', 'unknown')\n          storage_client = storage.Client()\n          bucket = storage_client.bucket('my-bucket')\n\n          # Download dataset\n          blob = bucket.blob(f'data/{dataset}.csv')\n          blob.download_to_filename(f'/tmp/{dataset}.csv')\n\n          # Process data\n          data = pd.read_csv(f'/tmp/{dataset}.csv')\n          processed = data.groupby('category').agg({\n              'value': ['sum', 'mean', 'count']\n          })\n\n          # Upload results\n          processed.to_csv(f'/tmp/{dataset}_processed.csv')\n          blob = bucket.blob(f'output/{dataset}_processed.csv')\n          blob.upload_from_filename(f'/tmp/{dataset}_processed.csv')\n\n          result.context.update({\n              \"dataset\": dataset,\n              \"rows_processed\": len(data),\n              \"output_file\": f\"gs://my-bucket/output/{dataset}_processed.csv\"\n          })\n</code></pre>"},{"location":"providers/gcp-batch/#machine-learning-training","title":"Machine Learning Training","text":"<pre><code>jobs:\n  ml-training:\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n        machine_type: \"n1-standard-4\"\n\n    stages:\n      - name: \"train\"\n        type: \"py\"\n        run: |\n          import numpy as np\n          import pickle\n          from google.cloud import storage\n          from sklearn.ensemble import RandomForestClassifier\n          from sklearn.model_selection import train_test_split\n\n          # Download training data\n          storage_client = storage.Client()\n          bucket = storage_client.bucket('my-bucket')\n          blob = bucket.blob('data/training.csv')\n          blob.download_to_filename('/tmp/training.csv')\n\n          # Load and prepare data\n          data = np.loadtxt('/tmp/training.csv', delimiter=',')\n          X, y = data[:, :-1], data[:, -1]\n          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n          # Train model\n          model = RandomForestClassifier(n_estimators=100, random_state=42)\n          model.fit(X_train, y_train)\n\n          # Evaluate model\n          accuracy = model.score(X_test, y_test)\n\n          # Save model\n          with open('/tmp/model.pkl', 'wb') as f:\n              pickle.dump(model, f)\n\n          # Upload model\n          blob = bucket.blob('models/trained_model.pkl')\n          blob.upload_from_filename('/tmp/model.pkl')\n\n          result.context.update({\n              \"accuracy\": accuracy,\n              \"model_path\": \"gs://my-bucket/models/trained_model.pkl\"\n          })\n</code></pre>"},{"location":"providers/gcp-batch/#bigquery-data-processing","title":"BigQuery Data Processing","text":"<pre><code>jobs:\n  bigquery-processing:\n    runs-on:\n      type: \"gcp_batch\"\n      with:\n        project_id: \"${GOOGLE_CLOUD_PROJECT}\"\n        region: \"${GOOGLE_CLOUD_REGION}\"\n        gcs_bucket: \"${GCS_BUCKET}\"\n\n    stages:\n      - name: \"process-bigquery\"\n        type: \"py\"\n        run: |\n          from google.cloud import bigquery\n          from google.cloud import storage\n          import pandas as pd\n\n          # Initialize BigQuery client\n          bq_client = bigquery.Client()\n\n          # Execute query\n          query = \"\"\"\n          SELECT\n            category,\n            COUNT(*) as count,\n            AVG(value) as avg_value\n          FROM `my-project.my-dataset.my-table`\n          GROUP BY category\n          \"\"\"\n\n          df = bq_client.query(query).to_dataframe()\n\n          # Save results to GCS\n          df.to_csv('/tmp/bigquery_results.csv', index=False)\n\n          storage_client = storage.Client()\n          bucket = storage_client.bucket('my-bucket')\n          blob = bucket.blob('output/bigquery_results.csv')\n          blob.upload_from_filename('/tmp/bigquery_results.csv')\n\n          result.context.update({\n              \"rows_processed\": len(df),\n              \"output_file\": \"gs://my-bucket/output/bigquery_results.csv\"\n          })\n</code></pre>"},{"location":"providers/gcp-batch/#api-reference","title":"API Reference","text":""},{"location":"providers/gcp-batch/#googlecloudbatchprovider","title":"GoogleCloudBatchProvider","text":"<p>Main provider class for Google Cloud Batch operations.</p>"},{"location":"providers/gcp-batch/#methods","title":"Methods","text":"<ul> <li><code>execute_job(job, params, run_id=None, event=None)</code>: Execute a job on Google Cloud Batch</li> <li><code>cleanup(job_id=None)</code>: Clean up Google Cloud Batch resources</li> <li><code>_create_job(job_name, task_script_gcs_url, job_config_gcs_url, params_gcs_url)</code>: Create Google Cloud Batch job</li> <li><code>_wait_for_job_completion(job_name, timeout)</code>: Wait for job completion</li> </ul>"},{"location":"providers/gcp-batch/#configuration-classes","title":"Configuration Classes","text":"<ul> <li><code>BatchResourceConfig</code>: Compute resource configuration</li> <li><code>BatchJobConfig</code>: Job configuration</li> <li><code>BatchTaskConfig</code>: Task configuration</li> </ul>"},{"location":"providers/gcp-batch/#functions","title":"Functions","text":"<ul> <li><code>gcp_batch_execute(job, params, run_id=None, event=None)</code>: Main execution function</li> </ul>"},{"location":"providers/gcp-batch/#dependencies","title":"Dependencies","text":"<ul> <li><code>google-cloud-batch&gt;=0.10.0</code>: Google Cloud Batch client library</li> <li><code>google-cloud-storage&gt;=2.10.0</code>: Google Cloud Storage client library</li> <li><code>pydantic&gt;=2.11.7</code>: Data validation</li> <li><code>ddeutil-workflow</code>: Core workflow engine</li> </ul>"},{"location":"providers/gcp-batch/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"providers/gcp-batch/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"}]}