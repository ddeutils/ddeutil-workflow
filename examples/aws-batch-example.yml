name: "aws-batch-example"
description: "Example workflow using AWS Batch execution"

params:
  data_source:
    type: str
    default: "s3://my-bucket/data.csv"

  output_path:
    type: str
    default: "s3://my-bucket/output"

  region:
    type: str
    default: "us-east-1"

jobs:
  data-processing:
    id: "data-processing"
    desc: "Process data using AWS Batch"

    runs-on:
      type: "aws_batch"
      with:
        job_queue_arn: "${AWS_BATCH_JOB_QUEUE_ARN}"
        s3_bucket: "${AWS_S3_BUCKET}"
        region_name: "${AWS_DEFAULT_REGION}"

    stages:
      - name: "start"
        type: "empty"
        echo: "Starting AWS Batch data processing job"

      - name: "download-data"
        type: "py"
        run: |
          import boto3
          import os

          # Initialize S3 client
          s3_client = boto3.client('s3')

          # Download data from S3
          data_source = os.environ.get('DATA_SOURCE', 's3://my-bucket/data.csv')
          bucket_name = data_source.split('/')[2]
          key = '/'.join(data_source.split('/')[3:])

          s3_client.download_file(bucket_name, key, '/tmp/input_data.csv')

          result.context.update({
              "data_downloaded": True,
              "input_file": data_source
          })

      - name: "process-data"
        type: "py"
        run: |
          import pandas as pd
          import numpy as np
          import boto3
          import os

          # Load data
          data = pd.read_csv('/tmp/input_data.csv')

          # Process data
          processed_data = data.groupby('category').agg({
              'value': ['sum', 'mean', 'std', 'count']
          }).round(2)

          # Add some computed columns
          processed_data['total_value'] = processed_data[('value', 'sum')]
          processed_data['avg_value'] = processed_data[('value', 'mean')]
          processed_data['std_value'] = processed_data[('value', 'std')]
          processed_data['count'] = processed_data[('value', 'count')]

          # Save processed data
          processed_data.to_csv('/tmp/processed_data.csv')

          # Upload to S3
          s3_client = boto3.client('s3')
          output_path = os.environ.get('OUTPUT_PATH', 's3://my-bucket/output')
          bucket_name = output_path.split('/')[2]
          key = '/'.join(output_path.split('/')[3:]) + '/processed_data.csv'

          s3_client.upload_file('/tmp/processed_data.csv', bucket_name, key)

          result.context.update({
              "rows_processed": len(data),
              "categories_processed": len(processed_data),
              "output_file": f"{output_path}/processed_data.csv",
              "processing_completed": True
          })

      - name: "generate-report"
        type: "py"
        run: |
          import pandas as pd
          import boto3
          import os
          from datetime import datetime

          # Load processed data
          data = pd.read_csv('/tmp/processed_data.csv')

          # Generate report
          report = f"""
          Data Processing Report
          Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

          Summary:
          - Total categories processed: {len(data)}
          - Total value: {data['total_value'].sum():,.2f}
          - Average value per category: {data['avg_value'].mean():,.2f}

          Top 5 categories by total value:
          {data.nlargest(5, 'total_value')[['category', 'total_value', 'avg_value']].to_string(index=False)}
          """

          # Save report
          with open('/tmp/processing_report.txt', 'w') as f:
              f.write(report)

          # Upload report to S3
          s3_client = boto3.client('s3')
          output_path = os.environ.get('OUTPUT_PATH', 's3://my-bucket/output')
          bucket_name = output_path.split('/')[2]
          key = '/'.join(output_path.split('/')[3:]) + '/processing_report.txt'

          s3_client.upload_file('/tmp/processing_report.txt', bucket_name, key)

          result.context.update({
              "report_generated": True,
              "report_file": f"{output_path}/processing_report.txt"
          })

      - name: "complete"
        type: "empty"
        echo: "AWS Batch data processing completed successfully"

  ml-training:
    id: "ml-training"
    desc: "Train machine learning model using AWS Batch"

    runs-on:
      type: "aws_batch"
      with:
        job_queue_arn: "${AWS_BATCH_JOB_QUEUE_ARN}"
        s3_bucket: "${AWS_S3_BUCKET}"
        region_name: "${AWS_DEFAULT_REGION}"

    stages:
      - name: "prepare-data"
        type: "py"
        run: |
          import pandas as pd
          import numpy as np
          import boto3
          from sklearn.model_selection import train_test_split

          # Generate synthetic data for training
          np.random.seed(42)
          n_samples = 10000

          # Create features
          X = np.random.randn(n_samples, 10)

          # Create target (simple linear combination with noise)
          y = np.dot(X, np.random.randn(10)) + np.random.randn(n_samples) * 0.1

          # Split data
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          # Save training data
          train_data = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(10)])
          train_data['target'] = y_train
          train_data.to_csv('/tmp/train_data.csv', index=False)

          # Save test data
          test_data = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(10)])
          test_data['target'] = y_test
          test_data.to_csv('/tmp/test_data.csv', index=False)

          result.context.update({
              "training_samples": len(X_train),
              "test_samples": len(X_test),
              "features": 10
          })

      - name: "train-model"
        type: "py"
        run: |
          import pandas as pd
          import numpy as np
          import pickle
          import boto3
          from sklearn.ensemble import RandomForestRegressor
          from sklearn.metrics import mean_squared_error, r2_score

          # Load training data
          train_data = pd.read_csv('/tmp/train_data.csv')
          test_data = pd.read_csv('/tmp/test_data.csv')

          X_train = train_data.drop('target', axis=1)
          y_train = train_data['target']
          X_test = test_data.drop('target', axis=1)
          y_test = test_data['target']

          # Train model
          model = RandomForestRegressor(
              n_estimators=100,
              max_depth=10,
              random_state=42,
              n_jobs=-1
          )

          model.fit(X_train, y_train)

          # Make predictions
          y_pred = model.predict(X_test)

          # Calculate metrics
          mse = mean_squared_error(y_test, y_pred)
          r2 = r2_score(y_test, y_pred)

          # Save model
          with open('/tmp/trained_model.pkl', 'wb') as f:
              pickle.dump(model, f)

          # Save metrics
          metrics = {
              'mse': mse,
              'rmse': np.sqrt(mse),
              'r2': r2,
              'feature_importance': dict(zip(X_train.columns, model.feature_importances_))
          }

          import json
          with open('/tmp/model_metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          # Upload model and metrics to S3
          s3_client = boto3.client('s3')
          s3_client.upload_file('/tmp/trained_model.pkl', 'my-bucket', 'models/trained_model.pkl')
          s3_client.upload_file('/tmp/model_metrics.json', 'my-bucket', 'models/model_metrics.json')

          result.context.update({
              "model_trained": True,
              "mse": mse,
              "rmse": np.sqrt(mse),
              "r2": r2,
              "model_path": "s3://my-bucket/models/trained_model.pkl",
              "metrics_path": "s3://my-bucket/models/model_metrics.json"
          })

      - name: "complete"
        type: "empty"
        echo: "ML model training completed successfully"

  parallel-processing:
    id: "parallel-processing"
    desc: "Parallel data processing using AWS Batch"

    strategy:
      matrix:
        dataset: ["sales", "inventory", "users", "transactions"]
      max_parallel: 4

    runs-on:
      type: "aws_batch"
      with:
        job_queue_arn: "${AWS_BATCH_JOB_QUEUE_ARN}"
        s3_bucket: "${AWS_S3_BUCKET}"
        region_name: "${AWS_DEFAULT_REGION}"

    stages:
      - name: "process-dataset"
        type: "py"
        run: |
          import pandas as pd
          import numpy as np
          import boto3
          import os
          from datetime import datetime

          # Get dataset name from matrix strategy
          dataset = os.environ.get('DATASET', 'unknown')

          # Generate synthetic data for the dataset
          np.random.seed(hash(dataset) % 2**32)
          n_samples = np.random.randint(1000, 10000)

          # Create dataset-specific data
          if dataset == "sales":
              data = pd.DataFrame({
                  'date': pd.date_range('2023-01-01', periods=n_samples, freq='D'),
                  'product_id': np.random.randint(1, 100, n_samples),
                  'quantity': np.random.randint(1, 50, n_samples),
                  'price': np.random.uniform(10, 1000, n_samples),
                  'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)
              })
              data['total_sales'] = data['quantity'] * data['price']

          elif dataset == "inventory":
              data = pd.DataFrame({
                  'product_id': np.random.randint(1, 100, n_samples),
                  'warehouse': np.random.choice(['A', 'B', 'C'], n_samples),
                  'quantity': np.random.randint(0, 1000, n_samples),
                  'reorder_level': np.random.randint(10, 100, n_samples),
                  'last_updated': pd.date_range('2023-01-01', periods=n_samples, freq='H')
              })

          elif dataset == "users":
              data = pd.DataFrame({
                  'user_id': range(1, n_samples + 1),
                  'age': np.random.randint(18, 80, n_samples),
                  'gender': np.random.choice(['M', 'F'], n_samples),
                  'country': np.random.choice(['US', 'CA', 'UK', 'DE', 'FR'], n_samples),
                  'registration_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),
                  'subscription_type': np.random.choice(['basic', 'premium', 'enterprise'], n_samples)
              })

          else:  # transactions
              data = pd.DataFrame({
                  'transaction_id': range(1, n_samples + 1),
                  'user_id': np.random.randint(1, 1000, n_samples),
                  'amount': np.random.uniform(1, 10000, n_samples),
                  'payment_method': np.random.choice(['credit_card', 'debit_card', 'paypal'], n_samples),
                  'status': np.random.choice(['completed', 'pending', 'failed'], n_samples, p=[0.9, 0.08, 0.02]),
                  'timestamp': pd.date_range('2023-01-01', periods=n_samples, freq='min')
              })

          # Process data
          if dataset == "sales":
              processed = data.groupby(['region', 'product_id']).agg({
                  'total_sales': ['sum', 'mean', 'count'],
                  'quantity': 'sum'
              }).round(2)

          elif dataset == "inventory":
              processed = data.groupby('warehouse').agg({
                  'quantity': ['sum', 'mean'],
                  'product_id': 'nunique'
              }).round(2)

          elif dataset == "users":
              processed = data.groupby(['country', 'subscription_type']).agg({
                  'user_id': 'count',
                  'age': ['mean', 'std']
              }).round(2)

          else:  # transactions
              processed = data.groupby(['payment_method', 'status']).agg({
                  'amount': ['sum', 'mean', 'count'],
                  'user_id': 'nunique'
              }).round(2)

          # Save processed data
          processed.to_csv(f'/tmp/{dataset}_processed.csv')

          # Upload to S3
          s3_client = boto3.client('s3')
          s3_client.upload_file(
              f'/tmp/{dataset}_processed.csv',
              'my-bucket',
              f'output/parallel/{dataset}_processed.csv'
          )

          result.context.update({
              "dataset": dataset,
              "rows_processed": len(data),
              "output_file": f"s3://my-bucket/output/parallel/{dataset}_processed.csv",
              "processing_timestamp": datetime.now().isoformat()
          })

      - name: "complete"
        type: "empty"
        echo: "Parallel processing completed for dataset: ${DATASET}"
